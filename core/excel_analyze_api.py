"""
Excelæ™ºèƒ½åˆ†æAPI
æ”¯æŒï¼š
1. ä¸Šä¼ Excelæ–‡ä»¶è‡ªåŠ¨å¤„ç†å¤šçº§è¡¨å¤´
2. ä¼šè¯ç®¡ç†ï¼ˆæ”¯æŒä¼šè¯å¤ç”¨ï¼‰
3. è‡ªåŠ¨æ•°æ®åˆ†æ
"""

import json
import os
import sys
import time
import uuid
import random
import shutil
import logging
import queue
import threading
from pathlib import Path
from typing import List, Optional, Dict, Any, Generator

import openai


# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

from .config import (
    DEFAULT_TEMPERATURE, STOP_TOKEN_IDS, MAX_NEW_TOKENS,
    EXCEL_VALID_EXTENSIONS, EXCEL_MAX_FILE_SIZE_MB,
    EXCEL_LLM_API_KEY, EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL,
    DEFAULT_EXCEL_ANALYSIS_PROMPT,
    ANALYZER_TYPE,  # åˆ†æå™¨ç±»å‹é…ç½®
    CLEANUP_TIMEOUT_HOURS,  # æ¸…ç†è¶…æ—¶é…ç½®
)
# Import ProcessedFileInfo as it's still used in the code
from .models import ProcessedFileInfo
# Other models are no longer used as Pydantic models, but kept for type reference if needed
# from .models import ExcelAnalyzeResponse, HeaderAnalysisResponse, ExcelSheetsResponse
from .storage import storage
from .utils import (
    get_thread_workspace, build_file_path, WorkspaceTracker,
    render_file_block, generate_report_from_messages, extract_code_from_segment,
    execute_code_safe, collect_file_info
)
from .excel_processor import (
    process_excel_file, get_sheet_names, generate_analysis_prompt,
    ExcelProcessResult
)

# matplotlibä¸­æ–‡æ”¯æŒä»£ç  - è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ä¸­æ–‡å­—ä½“
Chinese_matplot_str = """
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import warnings

# å°è¯•çš„ä¸­æ–‡å­—ä½“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
chinese_fonts = [
    'SimHei',           # Windows é»‘ä½“
    'Microsoft YaHei',  # Windows å¾®è½¯é›…é»‘
    'WenQuanYi Micro Hei',  # Linux æ–‡æ³‰é©¿å¾®ç±³é»‘
    'WenQuanYi Zen Hei',    # Linux æ–‡æ³‰é©¿æ­£é»‘
    'Noto Sans CJK SC',      # Google Noto å­—ä½“
    'Source Han Sans CN',    # æ€æºé»‘ä½“
    'STHeiti',          # macOS é»‘ä½“
    'Arial Unicode MS', # é€šç”¨ Unicode å­—ä½“
]

# è·å–æ‰€æœ‰å¯ç”¨å­—ä½“
available_fonts = [f.name for f in fm.fontManager.ttflist]

# æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
chinese_font = None
for font in chinese_fonts:
    if font in available_fonts:
        chinese_font = font
        break

# å¦‚æœæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å­—ä½“å¹¶å¿½ç•¥è­¦å‘Š
if chinese_font:
    plt.rcParams['font.sans-serif'] = [chinese_font] + plt.rcParams['font.sans-serif']
else:
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“å¹¶å¿½ç•¥å­—ä½“è­¦å‘Š
    warnings.filterwarnings('ignore', category=UserWarning, message='.*Glyph.*missing.*')
    # å°è¯•ä½¿ç”¨ DejaVu Sans ä½œä¸ºåå¤‡ï¼ˆè™½ç„¶ä¸æ”¯æŒä¸­æ–‡ï¼Œä½†è‡³å°‘ä¸ä¼šæŠ¥é”™ï¼‰
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans'] + plt.rcParams['font.sans-serif']

plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
"""

# Helper function to extract base URL from full API URL
def extract_api_base(api_url: str) -> str:
    """ä»å®Œæ•´çš„API URLä¸­æå–base URL"""
    if api_url.endswith("/chat/completions"):
        return api_url.rsplit("/chat/completions", 1)[0]
    elif "/v1" in api_url:
        return api_url.rsplit("/v1", 1)[0] + "/v1"
    else:
        return api_url


def validate_excel_file(filename: str, file_size: int, max_file_size_mb: Optional[int] = None) -> None:
    """éªŒè¯Excelæ–‡ä»¶
    
    å‚æ•°:
        filename: æ–‡ä»¶å
        file_size: æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    """
    # æ£€æŸ¥æ‰©å±•å
    ext = Path(filename).suffix.lower()
    if ext not in EXCEL_VALID_EXTENSIONS:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(EXCEL_VALID_EXTENSIONS)}"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    max_size_mb = max_file_size_mb if max_file_size_mb is not None else EXCEL_MAX_FILE_SIZE_MB
    max_size_bytes = max_size_mb * 1024 * 1024
    if file_size > max_size_bytes:
        raise ValueError(
            f"æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.2f}MBï¼Œæœ€å¤§æ”¯æŒ: {max_size_mb}MB"
        )


def get_or_create_thread(thread_id: Optional[str]) -> tuple:
    """è·å–æˆ–åˆ›å»ºä¼šè¯
    
    å¦‚æœæä¾›äº†thread_idä½†ä¼šè¯ä¸å­˜åœ¨ï¼Œä¼šåˆ›å»ºæ–°ä¼šè¯å¹¶ä½¿ç”¨è¯¥thread_id
    
    åŒæ—¶ä¼šè¿›è¡Œè½»é‡çº§çš„è¿‡æœŸä¼šè¯æ¸…ç†ï¼ˆ10%æ¦‚ç‡æ‰§è¡Œï¼Œé¿å…é¢‘ç¹æ£€æŸ¥ï¼‰
    """
    # è½»é‡çº§æ¸…ç†ï¼š10%æ¦‚ç‡æ‰§è¡Œæ¸…ç†ï¼Œé¿å…é¢‘ç¹æ£€æŸ¥å½±å“æ€§èƒ½
    if random.random() < 0.1:
        try:
            cleaned_count = storage.cleanup_expired_threads(CLEANUP_TIMEOUT_HOURS)
            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸä¼šè¯åŠå…¶å·¥ä½œç©ºé—´")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è¿‡æœŸä¼šè¯æ—¶å‡ºé”™: {e}")
    
    if thread_id:
        # å°è¯•ä½¿ç”¨å·²æœ‰ä¼šè¯
        thread = storage.get_thread(thread_id)
        if thread:
            # ä¼šè¯å­˜åœ¨ï¼Œä½¿ç”¨å®ƒ
            workspace_dir = get_thread_workspace(thread_id)
            return thread_id, workspace_dir, False  # Falseè¡¨ç¤ºéæ–°å»º
        else:
            # ä¼šè¯ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¼šè¯å¹¶ä½¿ç”¨ä¼ å…¥çš„thread_id
            logger.info(f"ä¼šè¯ {thread_id} ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¼šè¯å¹¶ä½¿ç”¨è¯¥ID")
            
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹æ³•åˆ›å»ºæŒ‡å®šIDçš„ä¼šè¯
            thread = storage.create_thread_with_id(
                thread_id=thread_id,
                metadata={"type": "excel_analysis", "dify_conversation_id": thread_id}
            )
            workspace_dir = get_thread_workspace(thread_id)
            
            return thread_id, workspace_dir, True  # Trueè¡¨ç¤ºæ–°å»º
    else:
        # åˆ›å»ºæ–°ä¼šè¯
        thread = storage.create_thread(metadata={"type": "excel_analysis"})
        workspace_dir = get_thread_workspace(thread.id)
        return thread.id, workspace_dir, True  # Trueè¡¨ç¤ºæ–°å»º


async def run_data_analysis(
    workspace_dir: str,
    thread_id: str,
    process_result: ExcelProcessResult,
    analysis_prompt: str,
    model: str,
    temperature: float,
    analysis_api_url: str,
    analysis_api_key: Optional[str] = None,
    stream: bool = False
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ•°æ®åˆ†ææµç¨‹
    """
    generated_dir = os.path.join(workspace_dir, "generated")
    os.makedirs(generated_dir, exist_ok=True)
    
    # æ„å»ºåˆ†ææç¤ºè¯
    full_prompt = generate_analysis_prompt(process_result, analysis_prompt)
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "user", "content": full_prompt}]
    
    # å‡†å¤‡vLLMæ¶ˆæ¯æ ¼å¼
    workspace_file_info = collect_file_info(workspace_dir)
    vllm_messages = [{
        "role": "user",
        "content": f"# Instruction\n{full_prompt}\n\n# Data\n{workspace_file_info}"
    }]
    
    # è·Ÿè¸ªç”Ÿæˆçš„æ–‡ä»¶
    generated_files = []
    tracker = WorkspaceTracker(workspace_dir, generated_dir)
    
    assistant_reply = ""
    finished = False
    
    # éªŒè¯ API URL æ ¼å¼
    if not analysis_api_url:
        raise ValueError("analysis_api_url ä¸èƒ½ä¸ºç©º")
    
    if not (analysis_api_url.startswith("http://") or analysis_api_url.startswith("https://")):
        raise ValueError(f"analysis_api_url æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´: {analysis_api_url}")
    
    # åˆ›å»ºåˆ†æ API å®¢æˆ·ç«¯
    try:
        api_base = extract_api_base(analysis_api_url)
        api_key = analysis_api_key or "dummy"
        analysis_client_async = openai.AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=60.0)
    except Exception as e:
        raise ValueError(f"åˆ›å»ºåˆ†æ API å®¢æˆ·ç«¯å¤±è´¥: {str(e)}ã€‚è¯·æ£€æŸ¥ analysis_api_url é…ç½®: {analysis_api_url}")
    
    while not finished:
        # è°ƒç”¨åˆ†æ API
        logger.info("=" * 60)
        logger.info("ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹ API è¿›è¡Œæ•°æ®åˆ†æ")
        logger.info(f"ğŸ“Œ æ¨¡å‹: {model}")
        logger.info(f"ğŸŒ¡ï¸  æ¸©åº¦: {temperature}")
        logger.info(f"ğŸ“ æ¶ˆæ¯æ•°é‡: {len(vllm_messages)}")
        logger.info(f"ğŸ”— API åœ°å€: {analysis_api_url}")
        
        # è®°å½•æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆå®Œæ•´å†…å®¹ï¼‰
        if vllm_messages:
            last_message = vllm_messages[-1]
            if isinstance(last_message, dict) and "content" in last_message:
                content_full = str(last_message["content"])
                logger.info("ğŸ“„ æœ€åä¸€æ¡æ¶ˆæ¯å®Œæ•´å†…å®¹:")
                logger.info("=" * 60)
                logger.info(content_full)
                logger.info("=" * 60)
        
        try:
            logger.info("ğŸ“¡ å‘é€ API è¯·æ±‚...")
            response = await analysis_client_async.chat.completions.create(
                model=model,
                messages=vllm_messages,
                temperature=temperature,
                stream=True,
                extra_body={
                    "add_generation_prompt": False,
                    "stop_token_ids": STOP_TOKEN_IDS,
                    "max_new_tokens": MAX_NEW_TOKENS,
                },
            )
            logger.info("âœ… API è¯·æ±‚æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æµå¼å“åº”...")
        except openai.APIConnectionError as e:
            error_msg = (
                f"âŒ **è¿æ¥åˆ†æ API å¤±è´¥**\n\n"
                f"**é”™è¯¯è¯¦æƒ…ï¼š** {str(e)}\n\n"
                f"**å¯èƒ½çš„åŸå› ï¼š**\n"
                f"1. åˆ†æ API æœåŠ¡æœªå¯åŠ¨æˆ–æ— æ³•è®¿é—®\n"
                f"2. API åœ°å€é…ç½®é”™è¯¯: `{analysis_api_url}`\n"
                f"3. ç½‘ç»œè¿æ¥é—®é¢˜ï¼ˆé˜²ç«å¢™ã€ä»£ç†ç­‰ï¼‰\n"
                f"4. API æœåŠ¡åœ°å€ä¸æ­£ç¡®æˆ–ç«¯å£æœªå¼€æ”¾\n\n"
                f"**è§£å†³æ–¹æ³•ï¼š**\n"
                f"1. ç¡®è®¤åˆ†æ API æœåŠ¡æ­£åœ¨è¿è¡Œ\n"
                f"2. æ£€æŸ¥ API åœ°å€æ˜¯å¦æ­£ç¡®: `{analysis_api_url}`\n"
                f"3. å°è¯•åœ¨æµè§ˆå™¨æˆ–å‘½ä»¤è¡Œä¸­è®¿é—®è¯¥åœ°å€\n"
                f"4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®\n"
                f"5. å¦‚æœä½¿ç”¨ localhostï¼Œç¡®ä¿æœåŠ¡åœ¨æ­£ç¡®çš„ç«¯å£ä¸Šè¿è¡Œ\n"
            )
            raise ConnectionError(error_msg) from e
        except openai.APIError as e:
            error_msg = (
                f"âŒ **åˆ†æ API è°ƒç”¨å¤±è´¥**\n\n"
                f"**é”™è¯¯è¯¦æƒ…ï¼š** {str(e)}\n\n"
                f"**API åœ°å€ï¼š** {analysis_api_url}\n"
                f"**æ¨¡å‹ï¼š** {model}\n\n"
                f"**å¯èƒ½çš„åŸå› ï¼š**\n"
                f"1. API å¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ\n"
                f"2. æ¨¡å‹åç§°ä¸æ­£ç¡®\n"
                f"3. API æœåŠ¡è¿”å›é”™è¯¯\n"
                f"4. è¯·æ±‚å‚æ•°ä¸åˆæ³•\n"
            )
            raise ValueError(error_msg) from e
        except Exception as e:
            error_msg = (
                f"âŒ **è°ƒç”¨åˆ†æ API æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯**\n\n"
                f"**é”™è¯¯ç±»å‹ï¼š** {type(e).__name__}\n"
                f"**é”™è¯¯è¯¦æƒ…ï¼š** {str(e)}\n\n"
                f"**API åœ°å€ï¼š** {analysis_api_url}\n"
                f"**æ¨¡å‹ï¼š** {model}\n"
            )
            raise RuntimeError(error_msg) from e
        
        cur_res = ""
        last_finish_reason = None
        chunk_count = 0
        
        logger.info("ğŸ“¥ å¼€å§‹æ¥æ”¶æµå¼å“åº”...")
        async for chunk in response:
            chunk_count += 1
            if chunk.choices and chunk.choices[0].delta.content is not None:
                delta = chunk.choices[0].delta.content
                cur_res += delta
                assistant_reply += delta
            
            # è®°å½• finish_reason
            if chunk.choices and chunk.choices[0].finish_reason:
                last_finish_reason = chunk.choices[0].finish_reason
                logger.debug(f"ğŸ“Š Chunk {chunk_count}: finish_reason = {last_finish_reason}")
            
            # æ¯ 50 ä¸ª chunk è®°å½•ä¸€æ¬¡è¿›åº¦
            if chunk_count % 50 == 0:
                logger.debug(f"ğŸ“Š å·²æ¥æ”¶ {chunk_count} ä¸ª chunksï¼Œå½“å‰å“åº”é•¿åº¦: {len(cur_res)} å­—ç¬¦")
            
            if "</Answer>" in cur_res:
                finished = True
                logger.info(f"âœ… æ£€æµ‹åˆ° </Answer> æ ‡ç­¾ï¼Œå®Œæˆå“åº”æ¥æ”¶")
                break
        
        logger.info(f"ğŸ“Š å“åº”ç»Ÿè®¡:")
        logger.info(f"   - æ¥æ”¶ chunks æ•°é‡: {chunk_count}")
        logger.info(f"   - å“åº”æ€»é•¿åº¦: {len(cur_res)} å­—ç¬¦")
        logger.info(f"   - å®ŒæˆåŸå› : {last_finish_reason}")
        
        # è®°å½•å®Œæ•´çš„å“åº”å†…å®¹
        logger.info("=" * 60)
        logger.info("ğŸ“ å¤§æ¨¡å‹å®Œæ•´å“åº”å†…å®¹:")
        logger.info("=" * 60)
        logger.info(cur_res)
        logger.info("=" * 60)
        
        has_code_segment = "<Code>" in cur_res
        has_closed_code = "</Code>" in cur_res
        
        if last_finish_reason == "stop" and not finished:
            if has_code_segment and not has_closed_code:
                cur_res += "</Code>"
                assistant_reply += "</Code>"
                has_closed_code = True
            elif not has_code_segment:
                finished = True
        
        if "</Answer>" in cur_res:
            finished = True
        
        # æ‰§è¡Œä»£ç 
        if has_code_segment and has_closed_code and not finished:
            logger.info("")
            logger.info("ğŸ” æ£€æµ‹åˆ°ä»£ç æ®µï¼Œå‡†å¤‡æ‰§è¡Œ...")
            vllm_messages.append({"role": "assistant", "content": cur_res})
            code_str = extract_code_from_segment(cur_res)
            if code_str:
                logger.info("ğŸ“ æå–çš„ä»£ç :")
                logger.info("=" * 60)
                logger.info(code_str)
                logger.info("=" * 60)
                code_str = Chinese_matplot_str + "\n" + code_str
                logger.info("â–¶ï¸  å¼€å§‹æ‰§è¡Œä»£ç ...")
                exe_output = execute_code_safe(code_str, workspace_dir)
                logger.info("âœ… ä»£ç æ‰§è¡Œå®Œæˆ")
                logger.info("ğŸ“Š æ‰§è¡Œè¾“å‡º:")
                logger.info("=" * 60)
                logger.info(exe_output)
                logger.info("=" * 60)
                artifacts = tracker.diff_and_collect()
                if artifacts:
                    logger.info(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡: {len(artifacts)}")
                    for artifact in artifacts:
                        logger.info(f"   - {artifact}")
                exe_str = f"\n<Execute>\n```\n{exe_output}\n```\n</Execute>\n"
                render_file_block(artifacts, workspace_dir, thread_id, generated_files)
                assistant_reply += exe_str
                vllm_messages.append({"role": "execute", "content": exe_output})
            else:
                logger.warning("âš ï¸ æ— æ³•æå–ä»£ç ï¼Œç»“æŸå¯¹è¯")
                finished = True
    
    # ä¸å†ç”Ÿæˆåˆ†ææŠ¥å‘Š
    logger.info("")
    logger.info("=" * 60)
    logger.info("ğŸ‰ æ•°æ®åˆ†æå®Œæˆ")
    logger.info(f"ğŸ“Š æœ€ç»ˆå“åº”é•¿åº¦: {len(assistant_reply)} å­—ç¬¦")
    logger.info(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(generated_files)}")
    logger.info("=" * 60)
    
    return {
        "reasoning": assistant_reply,
        "generated_files": generated_files,
        "report": ""  # ä¸å†ç”ŸæˆæŠ¥å‘Š
    }


async def analyze_excel(
    file_content: bytes,
    filename: str,
    analysis_api_url: str,
    analysis_model: str,
    thread_id: Optional[str] = None,
    use_llm_validate: bool = False,
    sheet_name: Optional[str] = None,
    auto_analysis: bool = True,
    analysis_prompt: Optional[str] = None,
    stream: bool = True,  # é»˜è®¤å¯ç”¨æµå¼è¾“å‡º
    temperature: float = DEFAULT_TEMPERATURE,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    analysis_api_key: Optional[str] = None
) -> Dict[str, Any]:
    """
    Excelæ™ºèƒ½åˆ†æå‡½æ•°
    
    åŠŸèƒ½ï¼š
    1. å¤„ç†Excelæ–‡ä»¶
    2. ä½¿ç”¨è§„åˆ™åˆ†æå¤„ç†å¤šçº§è¡¨å¤´ï¼ˆé»˜è®¤ï¼‰
    3. å¯é€‰ä½¿ç”¨LLMéªŒè¯è§„åˆ™åˆ†æç»“æœ
    4. å¯é€‰è‡ªåŠ¨æ•°æ®åˆ†æ
    5. æ”¯æŒä¼šè¯å¤ç”¨
    
    å‚æ•°ï¼š
    - file_content: Excelæ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰
    - filename: æ–‡ä»¶å
    - thread_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™åˆ›å»ºæ–°ä¼šè¯ï¼‰
    - use_llm_validate: æ˜¯å¦ä½¿ç”¨LLMéªŒè¯è§„åˆ™åˆ†æç»“æœï¼ˆå¯é€‰ï¼Œé»˜è®¤Falseï¼‰
    - llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    - llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
    - llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
    - sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ç¬¬ä¸€ä¸ªï¼‰
    - auto_analysis: æ˜¯å¦è‡ªåŠ¨åˆ†æï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼‰
    - analysis_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰
    - stream: æ˜¯å¦æµå¼è¿”å›ï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼Œå¯ç”¨æµå¼è¾“å‡ºï¼‰
    - analysis_api_url: æ•°æ®åˆ†æAPIåœ°å€ï¼ˆå¿…å¡«ï¼‰
    - analysis_model: æ•°æ®åˆ†ææ¨¡å‹åç§°ï¼ˆå¿…å¡«ï¼‰
    - analysis_api_key: æ•°æ®åˆ†æAPIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    - temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤0.4ï¼‰
    
    è¿”å›ï¼š
    - DictåŒ…å«: thread_id, status, header_analysis, processed_file, analysis_resultç­‰
    """
    file_size = len(file_content)
    
    # éªŒè¯æ–‡ä»¶
    validate_excel_file(filename, file_size)
    
    # è·å–æˆ–åˆ›å»ºä¼šè¯
    current_thread_id, workspace_dir, is_new = get_or_create_thread(thread_id)
    generated_dir = os.path.join(workspace_dir, "generated")
    os.makedirs(generated_dir, exist_ok=True)
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°å·¥ä½œç©ºé—´
        excel_path = os.path.join(workspace_dir, filename)
        with open(excel_path, "wb") as f:
            f.write(file_content)
        
        # è·å–å¯ç”¨å·¥ä½œè¡¨
        available_sheets = get_sheet_names(excel_path)
        
        # æ£€æŸ¥LLMé…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        if use_llm_validate and not api_key:
            use_llm_validate = False  # æ²¡æœ‰API keyåˆ™ä¸è¿›è¡ŒLLMéªŒè¯
        
        # å¤„ç†Excelæ–‡ä»¶
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ max_file_size_mb å‚æ•°ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»åœ¨ validate_excel_file ä¸­éªŒè¯è¿‡å¤§å°
        process_result = process_excel_file(
            filepath=excel_path,
            output_dir=workspace_dir,
            sheet_name=sheet_name,
            use_llm_validate=use_llm_validate,
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            max_file_size_mb=None  # ä½¿ç”¨é»˜è®¤å€¼ï¼Œå› ä¸ºæ–‡ä»¶å¤§å°å·²åœ¨ validate_excel_file ä¸­éªŒè¯
        )
        
        if not process_result.success:
            return {
                "thread_id": current_thread_id,
                "status": "error",
                "error_message": process_result.error_message,
                "available_sheets": available_sheets
            }
        
        # æ„å»ºå¤„ç†åçš„æ–‡ä»¶ä¿¡æ¯
        processed_file_info = None
        metadata_file_info = None
        
        if process_result.processed_file_path:
            csv_filename = os.path.basename(process_result.processed_file_path)
            processed_file_info = ProcessedFileInfo(
                filename=csv_filename,
                url=build_file_path(current_thread_id, csv_filename),
                size_bytes=os.path.getsize(process_result.processed_file_path)
            )
        
        if process_result.metadata_file_path:
            meta_filename = os.path.basename(process_result.metadata_file_path)
            metadata_file_info = ProcessedFileInfo(
                filename=meta_filename,
                url=build_file_path(current_thread_id, meta_filename),
                size_bytes=os.path.getsize(process_result.metadata_file_path) if os.path.exists(process_result.metadata_file_path) else None
            )
        
        # æ„å»ºè¡¨å¤´åˆ†æå“åº”
        header_analysis_response = None
        if process_result.header_analysis:
            ha = process_result.header_analysis
            header_analysis_response = {
                "skip_rows": ha.skip_rows,
                "header_rows": ha.header_rows,
                "header_type": ha.header_type,
                "data_start_row": ha.data_start_row,
                "confidence": ha.confidence,
                "reason": ha.reason
            }
        
        # æ•°æ®æ‘˜è¦
        data_summary = {
            "row_count": process_result.row_count,
            "column_count": len(process_result.column_names),
            "column_names": process_result.column_names
        }
        
        # æ³¨æ„ï¼šæµå¼è¿”å›åœ¨å½“å‰å®ç°ä¸­ä¸æ”¯æŒï¼Œstream å‚æ•°å°†è¢«å¿½ç•¥
        # å¦‚æœéœ€è¦æµå¼åŠŸèƒ½ï¼Œå¯ä»¥åœ¨è°ƒç”¨æ–¹å®ç°
        
        # éæµå¼å¤„ç†
        analysis_result = None
        if auto_analysis:
            prompt = analysis_prompt or DEFAULT_EXCEL_ANALYSIS_PROMPT
            analysis_result = await run_data_analysis(
                workspace_dir=workspace_dir,
                thread_id=current_thread_id,
                process_result=process_result,
                analysis_prompt=prompt,
                model=analysis_model,
                temperature=temperature,
                analysis_api_url=analysis_api_url,
                analysis_api_key=analysis_api_key,
                stream=False
            )
        
        # æ›´æ–°ä¼šè¯å…ƒæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        excel_file_info = {
            "original_name": filename,
            "processed_name": os.path.basename(process_result.processed_file_path) if process_result.processed_file_path else None,
            "sheet_name": sheet_name,
            "timestamp": int(time.time())
        }
        storage.append_thread_metadata_list(current_thread_id, "excel_files", excel_file_info)
        
        return {
            "thread_id": current_thread_id,
            "status": "success",
            "header_analysis": header_analysis_response,
            "processed_file": processed_file_info,
            "metadata_file": metadata_file_info,
            "data_summary": data_summary,
            "column_metadata": process_result.column_metadata,
            "analysis_result": analysis_result,
            "available_sheets": available_sheets
        }
        
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return {
            "thread_id": current_thread_id if 'current_thread_id' in locals() else "",
            "status": "error",
            "error_message": error_msg
        }


async def get_excel_sheets(file_id: str) -> Dict[str, Any]:
    """
    è·å–Excelæ–‡ä»¶çš„å·¥ä½œè¡¨åˆ—è¡¨
    
    å‚æ•°ï¼š
    - file_id: å·²ä¸Šä¼ çš„æ–‡ä»¶ID
    
    è¿”å›ï¼š
    - DictåŒ…å«: filename, sheets, default_sheet
    """
    file_obj = storage.get_file(file_id)
    if not file_obj:
        raise ValueError(f"æ–‡ä»¶ {file_id} ä¸å­˜åœ¨")
    
    # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹æ³•è·å–æ–‡ä»¶è·¯å¾„
    filepath = storage.get_file_path(file_id)
    if not filepath or not os.path.exists(filepath):
        raise ValueError("æ–‡ä»¶ä¸å­˜åœ¨")
    
    sheets = get_sheet_names(filepath)
    if not sheets:
        raise ValueError("æ— æ³•è¯»å–å·¥ä½œè¡¨åˆ—è¡¨")
    
    return {
        "filename": file_obj.filename,
        "sheets": sheets,
        "default_sheet": sheets[0]
    }


async def process_excel_only(
    file_content: bytes,
    filename: str,
    thread_id: Optional[str] = None,
    use_llm_validate: bool = False,
    sheet_name: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None
) -> Dict[str, Any]:
    """
    ä»…å¤„ç†Excelæ–‡ä»¶ï¼ˆä¸è¿›è¡Œæ•°æ®åˆ†æï¼‰
    
    ç”¨äºåªéœ€è¦å¤„ç†è¡¨å¤´ã€è½¬æ¢æ ¼å¼çš„åœºæ™¯
    é»˜è®¤ä½¿ç”¨è§„åˆ™åˆ†æï¼Œå¯é€‰ä½¿ç”¨LLMéªŒè¯ç»“æœï¼ˆLLMé…ç½®ä».envè¯»å–ï¼‰
    """
    file_size = len(file_content)
    
    # éªŒè¯æ–‡ä»¶
    validate_excel_file(filename, file_size)
    
    # è·å–æˆ–åˆ›å»ºä¼šè¯
    current_thread_id, workspace_dir, is_new = get_or_create_thread(thread_id)
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        excel_path = os.path.join(workspace_dir, filename)
        with open(excel_path, "wb") as f:
            f.write(file_content)
        
        # è·å–å¯ç”¨å·¥ä½œè¡¨
        available_sheets = get_sheet_names(excel_path)
        
        # æ£€æŸ¥LLMé…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        if use_llm_validate and not api_key:
            use_llm_validate = False
        
        # å¤„ç†Excelæ–‡ä»¶
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ max_file_size_mb å‚æ•°ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»åœ¨ validate_excel_file ä¸­éªŒè¯è¿‡å¤§å°
        process_result = process_excel_file(
            filepath=excel_path,
            output_dir=workspace_dir,
            sheet_name=sheet_name,
            use_llm_validate=use_llm_validate,
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            max_file_size_mb=None  # ä½¿ç”¨é»˜è®¤å€¼ï¼Œå› ä¸ºæ–‡ä»¶å¤§å°å·²åœ¨ validate_excel_file ä¸­éªŒè¯
        )
        
        if not process_result.success:
            return {
                "thread_id": current_thread_id,
                "status": "error",
                "error_message": process_result.error_message,
                "available_sheets": available_sheets
            }
        
        # æ„å»ºå“åº”
        processed_file_info = None
        metadata_file_info = None
        
        if process_result.processed_file_path:
            csv_filename = os.path.basename(process_result.processed_file_path)
            processed_file_info = {
                "filename": csv_filename,
                "file_path": build_file_path(current_thread_id, csv_filename),
                "size_bytes": os.path.getsize(process_result.processed_file_path)
            }
        else:
            processed_file_info = None
        
        if process_result.metadata_file_path:
            meta_filename = os.path.basename(process_result.metadata_file_path)
            metadata_file_info = {
                "filename": meta_filename,
                "file_path": build_file_path(current_thread_id, meta_filename)
            }
        else:
            metadata_file_info = None
        
        header_analysis_response = None
        if process_result.header_analysis:
            ha = process_result.header_analysis
            header_analysis_response = {
                "skip_rows": ha.skip_rows,
                "header_rows": ha.header_rows,
                "header_type": ha.header_type,
                "data_start_row": ha.data_start_row,
                "confidence": ha.confidence,
                "reason": ha.reason
            }
        
        return {
            "thread_id": current_thread_id,
            "status": "success",
            "header_analysis": header_analysis_response,
            "processed_file": processed_file_info,
            "metadata_file": metadata_file_info,
            "data_summary": {
                "row_count": process_result.row_count,
                "column_count": len(process_result.column_names),
                "column_names": process_result.column_names
            },
            "column_metadata": process_result.column_metadata,
            "available_sheets": available_sheets
        }
        
    except Exception as e:
        import traceback
        return {
            "thread_id": current_thread_id if 'current_thread_id' in locals() else "",
            "status": "error",
            "error_message": f"{str(e)}\n{traceback.format_exc()}"
        }


async def continue_analysis(
    thread_id: str,
    prompt: str,
    analysis_api_url: str,
    analysis_model: str,
    temperature: float = DEFAULT_TEMPERATURE,
    stream: bool = False,
    analysis_api_key: Optional[str] = None
) -> Dict[str, Any]:
    """
    åœ¨å·²æœ‰ä¼šè¯ä¸­ç»§ç»­åˆ†æ
    
    ç”¨äºå¯¹å·²å¤„ç†çš„æ•°æ®è¿›è¡Œåç»­åˆ†æ
    
    å‚æ•°:
    - thread_id: ä¼šè¯IDï¼ˆå¿…å¡«ï¼‰
    - prompt: åˆ†ææç¤ºè¯ï¼ˆå¿…å¡«ï¼‰
    - analysis_api_url: æ•°æ®åˆ†æAPIåœ°å€ï¼ˆå¿…å¡«ï¼‰
    - analysis_model: æ•°æ®åˆ†ææ¨¡å‹åç§°ï¼ˆå¿…å¡«ï¼‰
    - temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤0.4ï¼‰
    - stream: æ˜¯å¦æµå¼è¿”å›ï¼ˆå½“å‰ä¸æ”¯æŒï¼Œå°†è¢«å¿½ç•¥ï¼‰
    - analysis_api_key: æ•°æ®åˆ†æAPIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    
    æ³¨æ„ï¼šstream å‚æ•°å½“å‰ä¸æ”¯æŒï¼Œå°†è¢«å¿½ç•¥
    """
    # éªŒè¯ä¼šè¯
    thread = storage.get_thread(thread_id)
    if not thread:
        raise ValueError(f"ä¼šè¯ {thread_id} ä¸å­˜åœ¨")
    
    workspace_dir = get_thread_workspace(thread_id)
    generated_dir = os.path.join(workspace_dir, "generated")
    os.makedirs(generated_dir, exist_ok=True)
    
    # æ„å»ºæ¶ˆæ¯
    workspace_file_info = collect_file_info(workspace_dir)
    vllm_messages = [{
        "role": "user",
        "content": f"# Instruction\n{prompt}\n\n# Data\n{workspace_file_info}"
    }]
    
    # æ³¨æ„ï¼šæµå¼è¿”å›åœ¨å½“å‰å®ç°ä¸­ä¸æ”¯æŒï¼Œstream å‚æ•°å°†è¢«å¿½ç•¥
    # å¦‚æœéœ€è¦æµå¼åŠŸèƒ½ï¼Œå¯ä»¥åœ¨è°ƒç”¨æ–¹å®ç°
    
    # éæµå¼å¤„ç†
    generated_files = []
    tracker = WorkspaceTracker(workspace_dir, generated_dir)
    assistant_reply = ""
    finished = False
    
    # åˆ›å»ºåˆ†æ API å®¢æˆ·ç«¯
    api_base = extract_api_base(analysis_api_url)
    api_key = analysis_api_key or "dummy"
    analysis_client_async = openai.AsyncOpenAI(base_url=api_base, api_key=api_key)
    
    while not finished:
        response = await analysis_client_async.chat.completions.create(
            model=analysis_model,
            messages=vllm_messages,
            temperature=temperature,
            stream=True,
            extra_body={
                "add_generation_prompt": False,
                "stop_token_ids": STOP_TOKEN_IDS,
                "max_new_tokens": MAX_NEW_TOKENS,
            },
        )
        
        cur_res = ""
        last_finish_reason = None
        
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content is not None:
                delta = chunk.choices[0].delta.content
                cur_res += delta
                assistant_reply += delta
            last_finish_reason = chunk.choices[0].finish_reason
            if "</Answer>" in cur_res:
                finished = True
                break
        
        has_code_segment = "<Code>" in cur_res
        has_closed_code = "</Code>" in cur_res
        
        if last_finish_reason == "stop" and not finished:
            if has_code_segment and not has_closed_code:
                cur_res += "</Code>"
                assistant_reply += "</Code>"
                has_closed_code = True
            elif not has_code_segment:
                finished = True
        
        if has_code_segment and has_closed_code and not finished:
            vllm_messages.append({"role": "assistant", "content": cur_res})
            code_str = extract_code_from_segment(cur_res)
            if code_str:
                code_str = Chinese_matplot_str + "\n" + code_str
                exe_output = execute_code_safe(code_str, workspace_dir)
                artifacts = tracker.diff_and_collect()
                exe_str = f"\n<Execute>\n```\n{exe_output}\n```\n</Execute>\n"
                render_file_block(artifacts, workspace_dir, thread_id, generated_files)
                assistant_reply += exe_str
                vllm_messages.append({"role": "execute", "content": exe_output})
            else:
                finished = True
    
    # ç”ŸæˆæŠ¥å‘Š
    messages = [{"role": "user", "content": prompt}]
    generate_report_from_messages(
        messages, assistant_reply, workspace_dir, thread_id, generated_files
    )
    
    return {
        "thread_id": thread_id,
        "status": "success",
        "reasoning": assistant_reply,
        "generated_files": generated_files
    }


# ============================================================================
# æµå¼è¾“å‡ºç‰ˆæœ¬çš„å‡½æ•°
# ============================================================================

def run_data_analysis_stream(
    workspace_dir: str,
    thread_id: str,
    process_result: ExcelProcessResult,
    analysis_prompt: str,
    model: str,
    temperature: float,
    analysis_api_url: str,
    analysis_api_key: Optional[str] = None,
    debug_print_execution_output: bool = False  # æ˜¯å¦åœ¨æµå¼è¾“å‡ºä¸­æ‰“å°ä»£ç æ‰§è¡Œç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
) -> Generator[str, None, None]:
    """
    æ‰§è¡Œæ•°æ®åˆ†ææµç¨‹ - æµå¼ç‰ˆæœ¬
    
    é€æ­¥ yield å¤„ç†è¿›åº¦å’Œ LLM å“åº”
    
    å‚æ•°:
        workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
        thread_id: ä¼šè¯ID
        process_result: Excelå¤„ç†ç»“æœ
        analysis_prompt: åˆ†ææç¤ºè¯
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        analysis_api_url: åˆ†æAPIåœ°å€
        analysis_api_key: åˆ†æAPIå¯†é’¥
    
    Yields:
        str: æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
    """
    generated_dir = os.path.join(workspace_dir, "generated")
    os.makedirs(generated_dir, exist_ok=True)
    
    # æ„å»ºåˆ†ææç¤ºè¯
    full_prompt = generate_analysis_prompt(process_result, analysis_prompt)
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "user", "content": full_prompt}]
    
    # å‡†å¤‡vLLMæ¶ˆæ¯æ ¼å¼
    workspace_file_info = collect_file_info(workspace_dir)
    vllm_messages = [{
        "role": "user",
        "content": f"# Instruction\n{full_prompt}\n\n# Data\n{workspace_file_info}"
    }]
    
    # è·Ÿè¸ªç”Ÿæˆçš„æ–‡ä»¶
    generated_files = []
    tracker = WorkspaceTracker(workspace_dir, generated_dir)
    
    assistant_reply = ""
    finished = False
    
    # éªŒè¯ API URL æ ¼å¼
    if not analysis_api_url:
        yield "âŒ **é”™è¯¯**: analysis_api_url ä¸èƒ½ä¸ºç©º\n"
        return
    
    if not (analysis_api_url.startswith("http://") or analysis_api_url.startswith("https://")):
        yield f"âŒ **é”™è¯¯**: analysis_api_url æ ¼å¼ä¸æ­£ç¡®: {analysis_api_url}\n"
        return
    
    # åˆ›å»ºåˆ†æ API å®¢æˆ·ç«¯
    try:
        api_base = extract_api_base(analysis_api_url)
        api_key = analysis_api_key or "dummy"
        analysis_client = openai.OpenAI(base_url=api_base, api_key=api_key, timeout=60.0)
    except Exception as e:
        yield f"âŒ **é”™è¯¯**: åˆ›å»ºåˆ†æ API å®¢æˆ·ç«¯å¤±è´¥: {str(e)}\n"
        return
    
    round_num = 1
    while not finished:
        yield f"\n{'='*50}\n"
        yield f"ğŸ“Š **åˆ†æè½®æ¬¡ {round_num}**\n"
        yield f"{'='*50}\n\n"
        
        # è°ƒç”¨åˆ†æ API
        logger.info(f"ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹ API - è½®æ¬¡ {round_num}")
        
        try:
            response = analysis_client.chat.completions.create(
                model=model,
                messages=vllm_messages,
                temperature=temperature,
                stream=True,
                extra_body={
                    "add_generation_prompt": False,
                    "stop_token_ids": STOP_TOKEN_IDS,
                    "max_new_tokens": MAX_NEW_TOKENS,
                },
            )
        except openai.APIConnectionError as e:
            yield f"âŒ **è¿æ¥åˆ†æ API å¤±è´¥**: {str(e)}\n"
            yield f"è¯·æ£€æŸ¥ API åœ°å€: {analysis_api_url}\n"
            return
        except openai.APIError as e:
            yield f"âŒ **API è°ƒç”¨å¤±è´¥**: {str(e)}\n"
            return
        except Exception as e:
            yield f"âŒ **æœªçŸ¥é”™è¯¯**: {str(e)}\n"
            return
        
        cur_res = ""
        last_finish_reason = None
        
        # æµå¼è¾“å‡º LLM å“åº”
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content is not None:
                delta = chunk.choices[0].delta.content
                cur_res += delta
                assistant_reply += delta
                yield delta  # å®æ—¶è¾“å‡ºæ¯ä¸ª token
            
            if chunk.choices and chunk.choices[0].finish_reason:
                last_finish_reason = chunk.choices[0].finish_reason
            
            if "</Answer>" in cur_res:
                finished = True
                break
        
        has_code_segment = "<Code>" in cur_res
        has_closed_code = "</Code>" in cur_res
        
        if last_finish_reason == "stop" and not finished:
            if has_code_segment and not has_closed_code:
                cur_res += "</Code>"
                assistant_reply += "</Code>"
                yield "</Code>"
                has_closed_code = True
            elif not has_code_segment:
                finished = True
        
        if "</Answer>" in cur_res:
            finished = True
        
        # æ‰§è¡Œä»£ç 
        if has_code_segment and has_closed_code and not finished:
            yield "\n\n"
            yield "â–¶ï¸ **æ£€æµ‹åˆ°ä»£ç æ®µï¼Œå¼€å§‹æ‰§è¡Œ...**\n\n"
            
            vllm_messages.append({"role": "assistant", "content": cur_res})
            code_str = extract_code_from_segment(cur_res)
            
            if code_str:
                code_str = Chinese_matplot_str + "\n" + code_str
                
                yield "â³ æ­£åœ¨æ‰§è¡Œä»£ç ...\n"
                exe_output = execute_code_safe(code_str, workspace_dir)
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¾“å‡ºæ‰§è¡Œç»“æœ
                if debug_print_execution_output:
                    yield "\nğŸ“Š **æ‰§è¡Œç»“æœ:**\n"
                    yield f"```\n{exe_output}\n```\n"
                
                artifacts = tracker.diff_and_collect()
                if artifacts:
                    yield f"\nğŸ“ **ç”Ÿæˆçš„æ–‡ä»¶** ({len(artifacts)}ä¸ª):\n"
                    for artifact in artifacts:
                        yield f"   - {artifact.name}\n"
                
                exe_str = f"\n<Execute>\n```\n{exe_output}\n```\n</Execute>\n"
                render_file_block(artifacts, workspace_dir, thread_id, generated_files)
                assistant_reply += exe_str
                vllm_messages.append({"role": "execute", "content": exe_output})
            else:
                yield "âš ï¸ æ— æ³•æå–ä»£ç ï¼Œç»“æŸåˆ†æ\n"
                finished = True
        
        round_num += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if round_num > 10:
            yield "\nâš ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡é™åˆ¶ï¼Œç»“æŸåˆ†æ\n"
            finished = True
    
    # ä¸å†ç”Ÿæˆåˆ†ææŠ¥å‘Š
    # è¿”å›æœ€ç»ˆç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼ˆä»…ä»£ç æ‰§è¡Œç”Ÿæˆçš„æ–‡ä»¶ï¼‰
    if generated_files:
        yield f"\nğŸ“ **æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶:**\n"
        for file_info in generated_files:
            yield f"   - {file_info.get('name', 'N/A')}\n"


def analyze_excel_stream(
    file_content: bytes,
    filename: str,
    analysis_api_url: str,
    analysis_model: str,
    thread_id: Optional[str] = None,
    use_llm_validate: bool = False,
    sheet_name: Optional[str] = None,
    auto_analysis: bool = True,
    analysis_prompt: Optional[str] = None,
    temperature: float = DEFAULT_TEMPERATURE,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    analysis_api_key: Optional[str] = None,
    analyzer_type: Optional[str] = None,  # æ–°å¢ï¼šåˆ†æå™¨ç±»å‹å‚æ•°
    preprocessing_timeout: Optional[int] = None,  # é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    analysis_timeout: Optional[int] = None,  # åˆ†æè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    debug_print_execution_output: bool = False,  # æ˜¯å¦åœ¨æµå¼è¾“å‡ºä¸­æ‰“å°ä»£ç æ‰§è¡Œç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
    debug_print_header_analysis: bool = False,  # æ˜¯å¦åœ¨æµå¼è¾“å‡ºä¸­æ‰“å°è¡¨å¤´åˆ†æLLMå“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    max_file_size_mb: Optional[int] = None,  # æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    excel_processing_timeout: Optional[int] = None,  # Excelå¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œåœ¨LLMåˆ†æä¹‹å‰
    max_rows: Optional[int] = None,  # æœ€å¤§è¡Œæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼10000
) -> Generator[str, None, None]:
    """
    Excelæ™ºèƒ½åˆ†æå‡½æ•° - æµå¼ç‰ˆæœ¬
    
    ä½¿ç”¨ async generator é€æ­¥ yield å¤„ç†è¿›åº¦å’Œç»“æœ
    
    å‚æ•°ï¼š
    - file_content: Excelæ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰
    - filename: æ–‡ä»¶å
    - analysis_api_url: æ•°æ®åˆ†æAPIåœ°å€ï¼ˆå¿…å¡«ï¼‰
    - analysis_model: æ•°æ®åˆ†ææ¨¡å‹åç§°ï¼ˆå¿…å¡«ï¼‰
    - thread_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™åˆ›å»ºæ–°ä¼šè¯ï¼‰
    - use_llm_validate: æ˜¯å¦ä½¿ç”¨LLMéªŒè¯è§„åˆ™åˆ†æç»“æœï¼ˆå¯é€‰ï¼Œé»˜è®¤Falseï¼‰
    - sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ç¬¬ä¸€ä¸ªï¼‰
    - auto_analysis: æ˜¯å¦è‡ªåŠ¨åˆ†æï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼‰
    - analysis_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰
    - temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤0.4ï¼‰
    - llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    - llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
    - llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
    - analysis_api_key: æ•°æ®åˆ†æAPIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    - analyzer_type: åˆ†æå™¨ç±»å‹ï¼ˆå¯é€‰ï¼Œ"langgraph" æˆ– "legacy"ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
    
    Yields:
        str: æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
    """
    # ç¡®å®šä½¿ç”¨å“ªç§åˆ†æå™¨
    use_analyzer = analyzer_type or ANALYZER_TYPE
    
    # å¦‚æœä½¿ç”¨ LangGraph åˆ†æå™¨ï¼Œå§”æ‰˜ç»™æ–°çš„å®ç°
    if use_analyzer == "langgraph":
        logger.info("ğŸ”„ ä½¿ç”¨ LangGraph åˆ†æå™¨")
        from .analyzer import analyze_excel_with_langgraph
        
        yield from analyze_excel_with_langgraph(
            file_content=file_content,
            filename=filename,
            analysis_api_url=analysis_api_url,
            analysis_model=analysis_model,
            thread_id=thread_id,
            use_llm_validate=use_llm_validate,
            sheet_name=sheet_name,
            analysis_prompt=analysis_prompt,
            temperature=temperature,
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            analysis_api_key=analysis_api_key,
            preprocessing_timeout=preprocessing_timeout,
            analysis_timeout=analysis_timeout,
            debug_print_execution_output=debug_print_execution_output,
            debug_print_header_analysis=debug_print_header_analysis,
            max_file_size_mb=max_file_size_mb,
            excel_processing_timeout=excel_processing_timeout,
        )
        return
    
    # ä»¥ä¸‹æ˜¯åŸæœ‰çš„ legacy åˆ†æå™¨å®ç°
    logger.info("ğŸ”„ ä½¿ç”¨ Legacyï¼ˆDeepAnalyzeï¼‰åˆ†æå™¨")
    
    file_size = len(file_content)
    
    # === é™é»˜å¤„ç†ï¼šæ–‡ä»¶éªŒè¯ ===
    try:
        validate_excel_file(filename, file_size, max_file_size_mb=max_file_size_mb)
    except ValueError as e:
        yield f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}\n"
        return
    
    # === é™é»˜å¤„ç†ï¼šåˆ›å»ºä¼šè¯ ===
    try:
        current_thread_id, workspace_dir, is_new = get_or_create_thread(thread_id)
        generated_dir = os.path.join(workspace_dir, "generated")
        os.makedirs(generated_dir, exist_ok=True)
    except Exception as e:
        yield f"âŒ åˆ›å»ºä¼šè¯å¤±è´¥: {str(e)}\n"
        return
    
    # === é™é»˜å¤„ç†ï¼šä¿å­˜æ–‡ä»¶ ===
    try:
        excel_path = os.path.join(workspace_dir, filename)
        logger.info(f"ğŸ“ [DEBUG] å¼€å§‹ä¿å­˜æ–‡ä»¶åˆ°: {excel_path}")
        with open(excel_path, "wb") as f:
            f.write(file_content)
        logger.info(f"âœ… [DEBUG] æ–‡ä»¶ä¿å­˜å®Œæˆ: {excel_path}")
        
        # æ‰“å°æœ€åˆä¼ å…¥çš„ExcelåŸå§‹æ•°æ®
        logger.info(f"ğŸ“Š [DEBUG] å‡†å¤‡æ‰“å°ExcelåŸå§‹æ•°æ®: {excel_path}")
        from ..excel_processor import print_excel_raw_data
        logger.info(f"ğŸ”„ [DEBUG] è°ƒç”¨ print_excel_raw_data å‡½æ•°...")
        print("ğŸ” [DEBUG] è°ƒç”¨ print_excel_raw_data å‰ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        sys.stdout.flush()
        try:
            print_excel_raw_data(excel_path, sheet_name=sheet_name)
            print("ğŸ” [DEBUG] print_excel_raw_data å‡½æ•°å·²è¿”å›ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
            sys.stdout.flush()
        except Exception as e:
            print(f"âŒ [DEBUG] print_excel_raw_data è°ƒç”¨å¼‚å¸¸: {e}ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
            sys.stdout.flush()
            raise
        logger.info(f"âœ… [DEBUG] print_excel_raw_data å‡½æ•°å·²è¿”å›")
        logger.info(f"âœ… [DEBUG] ExcelåŸå§‹æ•°æ®æ‰“å°å®Œæˆï¼Œå‡†å¤‡ç»§ç»­æ‰§è¡Œåç»­ä»£ç ")
    except Exception as e:
        logger.error(f"âŒ [DEBUG] æ–‡ä»¶ä¿å­˜æˆ–æ‰“å°å¤±è´¥: {str(e)}", exc_info=True)
        yield f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}\n"
        return
    
    logger.info(f"ğŸš€ [DEBUG] æ–‡ä»¶ä¿å­˜å’Œæ‰“å°å®Œæˆï¼Œå‡†å¤‡è¿›å…¥é˜¶æ®µ0: LLMè¡¨å¤´åˆ†æ")
    # === é˜¶æ®µ0: LLMè¡¨å¤´åˆ†æ ===
    logger.info(f"ğŸ“ [DEBUG] å‡†å¤‡yieldé˜¶æ®µ0æ ‡é¢˜")
    yield "ğŸ¤– **é˜¶æ®µ0: LLMæ™ºèƒ½åˆ†æè¡¨æ ¼ç»“æ„**\n\n"
    logger.info(f"âœ… [DEBUG] é˜¶æ®µ0æ ‡é¢˜å·²yield")
    logger.info(f"ğŸ“ [DEBUG] å‡†å¤‡yieldæ–‡ä»¶å¤§å°ä¿¡æ¯")
    yield f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.1f} MB\n"
    logger.info(f"âœ… [DEBUG] æ–‡ä»¶å¤§å°ä¿¡æ¯å·²yield")
    logger.info(f"ğŸ“ [DEBUG] å‡†å¤‡yieldç­‰å¾…æç¤º")
    yield "â³ æ­£åœ¨åŠ è½½Excelæ–‡ä»¶å¹¶åˆ†æè¡¨å¤´ç»“æ„ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n\n"
    logger.info(f"âœ… [DEBUG] ç­‰å¾…æç¤ºå·²yield")
    
    logger.info(f"ğŸ”‘ [DEBUG] å¼€å§‹æ£€æŸ¥LLMé…ç½®...")
    api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
    actual_use_llm_validate = use_llm_validate and bool(api_key)
    logger.info(f"ğŸ”‘ [DEBUG] LLMé…ç½®æ£€æŸ¥å®Œæˆ - use_llm_validate: {actual_use_llm_validate}, api_keyå­˜åœ¨: {bool(api_key)}")
    
    try:
        # å¤„ç†Excelæ–‡ä»¶
        process_result = process_excel_file(
            filepath=excel_path,
            output_dir=workspace_dir,
            sheet_name=sheet_name,
            use_llm_validate=actual_use_llm_validate,
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            excel_processing_timeout=excel_processing_timeout,
            debug_print_header_analysis=debug_print_header_analysis,
            thinking_callback=None,  # ä¸è¾“å‡º thinking å†…å®¹
            max_file_size_mb=max_file_size_mb,  # ä¼ é€’æ–‡ä»¶å¤§å°é™åˆ¶
            max_rows=max_rows  # ä¼ é€’æœ€å¤§è¡Œæ•°é™åˆ¶
        )
        
        if not process_result.success:
            yield f"âŒ Excelå¤„ç†å¤±è´¥: {process_result.error_message}\n"
            return
        
        # è¡¨å¤´åˆ†æå®Œæˆä¿¡æ¯å·²ç§»é™¤ï¼Œä¸å†è¾“å‡º
        
        # æ ¹æ®è°ƒè¯•å¼€å…³å†³å®šæ˜¯å¦è¾“å‡ºLLMåŸå§‹å“åº”
        if debug_print_header_analysis and process_result.llm_analysis_response:
            yield "\nğŸ“‹ **LLMè¡¨å¤´åˆ†æåŸå§‹å“åº”ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰ï¼š**\n\n"
            yield "```json\n"
            yield process_result.llm_analysis_response
            yield "\n```\n\n"
        
    except Exception as e:
        yield f"âŒ è¡¨å¤´åˆ†æå¤±è´¥: {str(e)}\n"
        import traceback
        yield f"{traceback.format_exc()}\n"
        return
    
    # === é˜¶æ®µ1: è¯»å–å·¥ä½œè¡¨ä¿¡æ¯ ===
    yield "ğŸ“‹ **é˜¶æ®µ1: è¯»å–å·¥ä½œè¡¨ä¿¡æ¯**\n"
    
    available_sheets = get_sheet_names(excel_path)
    if available_sheets:
        yield f"   å¯ç”¨å·¥ä½œè¡¨: {', '.join(available_sheets)}\n"
        if sheet_name:
            yield f"   ä½¿ç”¨æŒ‡å®šå·¥ä½œè¡¨: {sheet_name}\n"
        else:
            yield f"   ä½¿ç”¨é»˜è®¤å·¥ä½œè¡¨: {available_sheets[0]}\n"
    yield "\n"
    
    # === é˜¶æ®µ2: AIæ•°æ®åˆ†æ ===
    if auto_analysis:
        yield "ğŸ§  **é˜¶æ®µ2: AIæ•°æ®åˆ†æ**\n\n"
        
        prompt = analysis_prompt or DEFAULT_EXCEL_ANALYSIS_PROMPT
        
        # è°ƒç”¨æµå¼æ•°æ®åˆ†æ
        consumer_disconnected = False
        for chunk in run_data_analysis_stream(
            workspace_dir=workspace_dir,
            thread_id=current_thread_id,
            process_result=process_result,
            analysis_prompt=prompt,
            model=analysis_model,
            temperature=temperature,
            analysis_api_url=analysis_api_url,
            analysis_api_key=analysis_api_key,
            debug_print_execution_output=debug_print_execution_output
        ):
            try:
                yield chunk
            except Exception as e:
                # æ•è· yield å¼‚å¸¸ï¼ˆé€šå¸¸æ˜¯è¿æ¥æ–­å¼€ï¼‰
                logger.warning(f"âš ï¸ [DEBUG] yield æ—¶è¿æ¥æ–­å¼€: {e}")
                break
    else:
        yield "â„¹ï¸ å·²è·³è¿‡è‡ªåŠ¨åˆ†æï¼ˆauto_analysis=Falseï¼‰\n"
    
    # æ›´æ–°ä¼šè¯å…ƒæ•°æ®ï¼ˆé™é»˜å¤„ç†ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰
    try:
        excel_file_info = {
            "original_name": filename,
            "processed_name": os.path.basename(process_result.processed_file_path) if process_result.processed_file_path else None,
            "sheet_name": sheet_name,
            "timestamp": int(time.time())
        }
        storage.append_thread_metadata_list(current_thread_id, "excel_files", excel_file_info)
    except Exception:
        pass  # å¿½ç•¥å…ƒæ•°æ®æ›´æ–°é”™è¯¯

