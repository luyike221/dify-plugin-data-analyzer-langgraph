"""
Excelæ™ºèƒ½å¤„ç†æ¨¡å—
æ”¯æŒï¼š
1. è‡ªåŠ¨è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ³¨é‡Šã€æ ‡é¢˜ç­‰ï¼‰
2. å•è¡¨å¤´/å¤šè¡¨å¤´è‡ªåŠ¨è¯†åˆ«
3. å¯é€‰è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
4. åˆå¹¶å•å…ƒæ ¼å¤„ç†
5. åˆ—ç»“æ„å…ƒæ•°æ®ç”Ÿæˆ
"""

import pandas as pd
import json
import re
import os
import requests
import logging
import tempfile
import shutil
import time
from openpyxl import load_workbook
from typing import Tuple, List, Dict, Optional, Any
from collections import defaultdict
from dataclasses import dataclass, asdict, field
from pathlib import Path

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥é…ç½®ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨å»¶è¿Ÿå¯¼å…¥ï¼‰

from .config import EXCEL_LLM_API_KEY, EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL



@dataclass
class HeaderAnalysis:
    """è¡¨å¤´åˆ†æç»“æœ"""
    skip_rows: int          # éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œæ•°
    header_rows: int        # è¡¨å¤´å ç”¨çš„è¡Œæ•°
    header_type: str        # 'single' æˆ– 'multi'
    data_start_row: int     # æ•°æ®å¼€å§‹è¡Œï¼ˆ1-indexedï¼‰
    confidence: str         # ç½®ä¿¡åº¦: high/medium/low
    reason: str             # åˆ†æåŸå› è¯´æ˜
    start_col: int = 1      # æ•°æ®èµ·å§‹åˆ—ï¼ˆ1-indexedï¼‰ï¼Œç¬¬ä¸€ä¸ªè¡¨å¤´è¡Œä¸­ç¬¬ä¸€ä¸ªéç©ºè¡¨å¤´å¼€å§‹çš„åˆ—
    valid_cols: Optional[List[int]] = None  # æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        if result.get('valid_cols') is None:
            result['valid_cols'] = None
        return result


@dataclass
class ExcelProcessResult:
    """Excelå¤„ç†ç»“æœ"""
    success: bool
    header_analysis: Optional[HeaderAnalysis]
    processed_file_path: Optional[str]      # å¤„ç†åçš„CSVæ–‡ä»¶è·¯å¾„
    metadata_file_path: Optional[str]       # å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„
    column_names: List[str]                 # åˆ—ååˆ—è¡¨
    column_metadata: Dict[str, Dict]        # åˆ—ç»“æ„å…ƒæ•°æ®
    row_count: int                          # æ•°æ®è¡Œæ•°
    error_message: Optional[str]            # é”™è¯¯ä¿¡æ¯
    llm_analysis_response: Optional[str] = None  # LLMåˆ†æåŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "success": self.success,
            "header_analysis": self.header_analysis.to_dict() if self.header_analysis else None,
            "processed_file_path": self.processed_file_path,
            "metadata_file_path": self.metadata_file_path,
            "column_names": self.column_names,
            "column_metadata": self.column_metadata,
            "row_count": self.row_count,
            "error_message": self.error_message
        }


class SmartHeaderProcessor:
    """æ™ºèƒ½è¡¨å¤´å¤„ç†å™¨"""
    
    def __init__(self, filepath: str, sheet_name: str = None):
        self.filepath = filepath
        self.sheet_name = sheet_name
        self.file_ext = Path(filepath).suffix.lower()
        self._temp_xlsx_path = None  # ç”¨äºå­˜å‚¨ä¸´æ—¶è½¬æ¢çš„ .xlsx æ–‡ä»¶è·¯å¾„
        
        # å¦‚æœæ˜¯ .xls æ ¼å¼ï¼Œå…ˆè½¬æ¢ä¸º .xlsx
        if self.file_ext == '.xls':
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ° .xls æ ¼å¼æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸º .xlsx...")
            self._temp_xlsx_path = self._convert_xls_to_xlsx(filepath)
            actual_filepath = self._temp_xlsx_path
            logger.info(f"âœ… è½¬æ¢å®Œæˆ: {self._temp_xlsx_path}")
        else:
            actual_filepath = filepath
        
        # ç»Ÿä¸€ä½¿ç”¨ openpyxl è¯»å–
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ read_only æ¨¡å¼ï¼Œå› ä¸ºéœ€è¦è®¿é—® merged_cells å±æ€§æ¥å¤„ç†åˆå¹¶å•å…ƒæ ¼
        self.wb = load_workbook(actual_filepath, data_only=True)
        # ä¿®å¤ï¼šæ˜ç¡®ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼Œè€Œä¸æ˜¯ä¾èµ– wb.activeï¼ˆactiveå¯èƒ½æ˜¯ç”¨æˆ·æœ€åæŸ¥çœ‹çš„å·¥ä½œè¡¨ï¼‰
        if sheet_name:
            self.ws = self.wb[sheet_name]
        else:
            # æ˜ç¡®ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼ˆç´¢å¼•0ï¼‰ï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´
            if not self.wb.sheetnames:
                raise ValueError("Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨")
            self.ws = self.wb[self.wb.sheetnames[0]]
        self.merged_cells_map = self._build_merged_cells_map()
    
    def _convert_xls_to_xlsx(self, xls_path: str) -> str:
        """
        å°† .xls æ–‡ä»¶è½¬æ¢ä¸º .xlsx æ ¼å¼
        
        å‚æ•°:
            xls_path: .xls æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
            ä¸´æ—¶ .xlsx æ–‡ä»¶è·¯å¾„
        """
        try:
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(xls_path, engine='xlrd')
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_dir = os.path.dirname(xls_path)
            temp_xlsx_path = os.path.join(
                temp_dir, 
                f"{Path(xls_path).stem}_converted_{os.getpid()}.xlsx"
            )
            
            # ä½¿ç”¨ ExcelWriter å†™å…¥æ‰€æœ‰å·¥ä½œè¡¨
            with pd.ExcelWriter(temp_xlsx_path, engine='openpyxl') as writer:
                for sheet_name in excel_file.sheet_names:
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='xlrd')
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            logger.info(f"âœ… .xls æ–‡ä»¶å·²è½¬æ¢ä¸º .xlsx: {temp_xlsx_path}")
            return temp_xlsx_path
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢ .xls æ–‡ä»¶å¤±è´¥: {e}")
            raise ValueError(
                f"æ— æ³•è½¬æ¢ .xls æ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²å®‰è£… xlrd åº“: pip install xlrdã€‚é”™è¯¯: {str(e)}"
            )
    
    def _build_merged_cells_map(self) -> Dict[Tuple[int, int], str]:
        """æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„"""
        merged_map = {}
        try:
            for merged_range in self.ws.merged_cells.ranges:
                min_row, min_col = merged_range.min_row, merged_range.min_col
                value = self.ws.cell(min_row, min_col).value
                for row in range(merged_range.min_row, merged_range.max_row + 1):
                    for col in range(merged_range.min_col, merged_range.max_col + 1):
                        merged_map[(row, col)] = value
        except Exception as e:
            # å¦‚æœæ— æ³•è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›ç©ºå­—å…¸
            logger.warning(f"âš ï¸ æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„æ—¶å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨ç©ºæ˜ å°„")
        
        return merged_map
    
    def get_cell_value(self, row: int, col: int) -> Any:
        """è·å–å•å…ƒæ ¼å€¼ï¼Œå¤„ç†åˆå¹¶å•å…ƒæ ¼"""
        if (row, col) in self.merged_cells_map:
            return self.merged_cells_map[(row, col)]
        return self.ws.cell(row, col).value
    
    def get_preview_data(self, max_rows: int = 30, max_cols: int = 20) -> List[List[Any]]:
        """
        è·å–é¢„è§ˆæ•°æ®ç”¨äºåˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ”¹è¿›ï¼š
        1. å¢åŠ é¢„è§ˆè¡Œæ•°å’Œåˆ—æ•°ï¼Œæ›´å¥½åœ°å±•ç¤ºå¤æ‚è¡¨å¤´
        2. æ ‡è®°åˆå¹¶å•å…ƒæ ¼ï¼Œä¾¿äºè¯†åˆ«å±‚çº§ç»“æ„
        3. åŒºåˆ†æ–‡æœ¬å’Œæ•°å€¼ï¼Œä¾¿äºè¯†åˆ«æ•°æ®èµ·å§‹è¡Œ
        """
        actual_max_col = min(self.ws.max_column, max_cols)
        actual_max_row = min(self.ws.max_row, max_rows)
        
        data = []
        for row in range(1, actual_max_row + 1):
            row_data = []
            for col in range(1, actual_max_col + 1):
                value = self.get_cell_value(row, col)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶å•å…ƒæ ¼çš„å·¦ä¸Šè§’
                is_merged_top_left = False
                merge_marker = ""
                try:
                    for merged_range in self.ws.merged_cells.ranges:
                        if merged_range.min_row == row and merged_range.min_col == col:
                            is_merged_top_left = True
                            # æ ‡è®°åˆå¹¶èŒƒå›´
                            row_span = merged_range.max_row - merged_range.min_row + 1
                            col_span = merged_range.max_col - merged_range.min_col + 1
                            if row_span > 1 or col_span > 1:
                                merge_marker = f"[åˆå¹¶:è¡Œ{row_span}åˆ—{col_span}]"
                            break
                except Exception:
                    # å¦‚æœæ— æ³•è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼Œè·³è¿‡æ ‡è®°
                    pass
                
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä¾¿äºåˆ†æ
                if value is None:
                    # æ£€æŸ¥æ˜¯å¦åœ¨åˆå¹¶å•å…ƒæ ¼å†…ï¼ˆä½†ä¸æ˜¯å·¦ä¸Šè§’ï¼‰
                    is_in_merged = (row, col) in self.merged_cells_map
                    if is_in_merged and not is_merged_top_left:
                        row_data.append("[åˆå¹¶å†…]")
                    else:
                        row_data.append("")
                elif isinstance(value, (int, float)):
                    row_data.append(f"[æ•°å€¼:{value}]")
                else:
                    value_str = str(value)[:40]  # æˆªæ–­è¿‡é•¿å†…å®¹
                    if is_merged_top_left and merge_marker:
                        row_data.append(f"{value_str}{merge_marker}")
                    else:
                        row_data.append(value_str)
            data.append(row_data)
        return data
    
    def get_merged_info(self) -> List[Dict]:
        """
        è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ”¹è¿›ï¼š
        1. å¢åŠ æ›´å¤šè¡Œæ•°çš„åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼ˆå‰30è¡Œï¼‰
        2. åŒºåˆ†è¡Œåˆå¹¶å’Œåˆ—åˆå¹¶
        3. æä¾›æ›´è¯¦ç»†çš„ç»“æ„ä¿¡æ¯
        """
        merged_info = []
        try:
            for merged_range in self.ws.merged_cells.ranges:
                if merged_range.min_row <= 30:  # å…³æ³¨å‰30è¡Œï¼Œè¦†ç›–å¤æ‚è¡¨å¤´
                    row_span = merged_range.max_row - merged_range.min_row + 1
                    col_span = merged_range.max_col - merged_range.min_col + 1
                    value = self.ws.cell(merged_range.min_row, merged_range.min_col).value
                    value_str = str(value)[:50] if value else ""
                    
                    merged_info.append({
                        'range': str(merged_range),
                        'rows': f"{merged_range.min_row}-{merged_range.max_row}",
                        'cols': f"{merged_range.min_col}-{merged_range.max_col}",
                        'row_span': row_span,
                        'col_span': col_span,
                        'value': value_str,
                        'is_row_merge': row_span > 1,  # æ˜¯å¦è·¨è¡Œåˆå¹¶
                        'is_col_merge': col_span > 1,  # æ˜¯å¦è·¨åˆ—åˆå¹¶
                    })
            
            # æŒ‰è¡Œå·æ’åºï¼Œä¾¿äºåˆ†æ
            merged_info.sort(key=lambda x: int(x['rows'].split('-')[0]))
        except Exception as e:
            # å¦‚æœæ— æ³•è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›ç©ºåˆ—è¡¨
            logger.warning(f"âš ï¸ è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        return merged_info
    
    def analyze_with_llm(self, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         timeout: Optional[int] = None) -> Tuple[HeaderAnalysis, str]:
        """
        ä½¿ç”¨LLMç›´æ¥åˆ†æExcelè¡¨æ ¼çš„è¡Œå’Œåˆ—ç»“æ„
        
        å‚æ•°:
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        
        è¿”å›:
            (åˆ†æç»“æœ, LLMåŸå§‹å“åº”)ï¼ˆå¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼‰
        """
        # å¢åŠ é¢„è§ˆè¡Œæ•°å’Œåˆ—æ•°ï¼Œæ›´å¥½åœ°å±•ç¤ºå¤æ‚å¤šçº§è¡¨å¤´
        preview_data = self.get_preview_data(max_rows=30, max_cols=20)
        merged_info = self.get_merged_info()
        max_col = self.ws.max_column
        
        # æ„å»ºåˆ†ææç¤ºè¯
        prompt = self._build_llm_analysis_prompt(preview_data, merged_info, max_col)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model, timeout=timeout)
        
        if not result:
            raise ValueError("LLMåˆ†æå¤±è´¥ï¼šæ— æ³•è·å–LLMå“åº”ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        
        # è§£æLLMåˆ†æç»“æœ
        analysis = self._parse_llm_analysis_response(result)
        
        return analysis, result
    
    def validate_with_llm(self, rule_analysis: HeaderAnalysis, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         timeout: Optional[int] = None) -> HeaderAnalysis:
        """
        ä½¿ç”¨LLMéªŒè¯è§„åˆ™åˆ†æçš„ç»“æœï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
        
        å‚æ•°:
            rule_analysis: è§„åˆ™åˆ†æçš„ç»“æœ
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        
        è¿”å›:
            éªŒè¯åçš„åˆ†æç»“æœï¼ˆå¦‚æœLLMéªŒè¯å¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœï¼‰
        """
        preview_data = self.get_preview_data()
        merged_info = self.get_merged_info()
        
        # æ„å»ºéªŒè¯æç¤ºè¯
        prompt = self._build_validation_prompt(preview_data, merged_info, rule_analysis)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model, timeout=timeout)
        
        # è§£æLLMéªŒè¯ç»“æœ
        validated = self._parse_validation_response(result, rule_analysis)
        
        return validated
    
    def _build_llm_analysis_prompt(self, preview_data: List[List], merged_info: List[Dict], 
                                   max_col: int) -> str:
        """
        æ„å»ºLLMåˆ†ææç¤ºè¯ï¼ˆå®Œå…¨é‡æ„ç‰ˆï¼‰
        
        é‡‡ç”¨åˆ†æ­¥éª¤æ–¹æ³•è¯†åˆ«å¤æ‚å¤šçº§è¡¨å¤´ï¼š
        1. å…ˆè¯†åˆ«æ— æ•ˆè¡Œ
        2. å†è¯†åˆ«è¡¨å¤´å±‚çº§ç»“æ„
        3. æœ€åç¡®å®šæ•°æ®èµ·å§‹è¡Œ
        """
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºè¡¨æ ¼å½¢å¼ï¼ˆæ˜¾ç¤ºæ›´å¤šåˆ—ï¼Œå¹¶æ ‡è®°è¡Œç‰¹å¾ï¼‰
        num_cols_to_show = min(20, len(preview_data[0]) if preview_data else 20)
        col_headers = " | ".join([f"åˆ—{i+1:2d}" for i in range(num_cols_to_show)])
        table_str = f"è¡Œå· | {col_headers} | è¡Œç‰¹å¾\n" + "-" * 140 + "\n"
        
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join(str(cell)[:12] for cell in row[:num_cols_to_show])
            
            # åˆ†æè¡Œçš„ç‰¹å¾ï¼Œä¾¿äºè¯†åˆ«æ— æ•ˆè¡Œ
            non_empty_count = sum(1 for cell in row[:num_cols_to_show] if cell and str(cell).strip() and str(cell) != "[åˆå¹¶å†…]")
            has_merge = any("[åˆå¹¶" in str(cell) for cell in row[:num_cols_to_show])
            has_text_label = any(
                cell and isinstance(cell, str) and 
                (len(cell) > 2) and 
                not cell.startswith("[æ•°å€¼") and 
                not cell.startswith("[åˆå¹¶") and
                not cell.startswith("[åˆå¹¶å†…")
                for cell in row[:num_cols_to_show]
            )
            
            # æ ‡è®°è¡Œç‰¹å¾ï¼ˆä¾¿äºè¯†åˆ«æ— æ•ˆè¡Œå’Œè¡¨å¤´è¡Œï¼‰
            row_marker = ""
            if non_empty_count <= 3 and not has_merge and not has_text_label:
                row_marker = f"âš ï¸å¯èƒ½æ— æ•ˆè¡Œï¼ˆåªæœ‰{non_empty_count}ä¸ªéç©ºå•å…ƒæ ¼ï¼Œæ— è¡¨å¤´ç‰¹å¾ï¼‰"
            elif has_merge:
                row_marker = "âœ…è¡¨å¤´è¡Œï¼ˆåŒ…å«åˆå¹¶å•å…ƒæ ¼ï¼‰"
            elif has_text_label:
                row_marker = "âœ…å¯èƒ½è¡¨å¤´è¡Œï¼ˆåŒ…å«æ–‡æœ¬æ ‡ç­¾ï¼‰"
            elif non_empty_count > 5:
                row_marker = "ğŸ“Šå¯èƒ½æ•°æ®è¡Œï¼ˆåŒ…å«å¤šä¸ªæ•°å€¼ï¼‰"
            
            table_str += f"  {i:2d}  | {row_str} | {row_marker}\n"
        
        # æ ¼å¼åŒ–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼ˆæŒ‰ç±»å‹åˆ†ç±»ï¼‰
        if merged_info:
            # æŒ‰è¡Œåˆå¹¶å’Œåˆ—åˆå¹¶åˆ†ç±»
            row_merges = [m for m in merged_info if m.get('is_row_merge', False)]
            col_merges = [m for m in merged_info if m.get('is_col_merge', False)]
            both_merges = [m for m in merged_info if m.get('is_row_merge', False) and m.get('is_col_merge', False)]
            
            merged_str = "ã€åˆå¹¶å•å…ƒæ ¼è¯¦ç»†ä¿¡æ¯ã€‘\n\n"
            
            if both_merges:
                merged_str += "åŒæ—¶è·¨è¡Œè·¨åˆ—çš„åˆå¹¶å•å…ƒæ ¼ï¼ˆé€šå¸¸æ˜¯å¤šçº§è¡¨å¤´çš„å…³é”®æ ‡è¯†ï¼‰ï¼š\n"
                for m in both_merges[:20]:
                    merged_str += f"  - è¡Œ{m['rows']} åˆ—{m['cols']} (è·¨{m['row_span']}è¡Œ{m['col_span']}åˆ—): '{m['value']}'\n"
                merged_str += "\n"
            
            if row_merges:
                merged_str += "è·¨è¡Œåˆå¹¶å•å…ƒæ ¼ï¼ˆè¡¨ç¤ºè¡¨å¤´çš„å±‚çº§ç»“æ„ï¼‰ï¼š\n"
                for m in row_merges[:20]:
                    if not (m.get('is_row_merge') and m.get('is_col_merge')):
                        merged_str += f"  - è¡Œ{m['rows']} (è·¨{m['row_span']}è¡Œ): '{m['value']}'\n"
                merged_str += "\n"
            
            if col_merges:
                merged_str += "è·¨åˆ—åˆå¹¶å•å…ƒæ ¼ï¼ˆè¡¨ç¤ºåˆ—çš„åˆ†ç»„ï¼‰ï¼š\n"
                for m in col_merges[:20]:
                    if not (m.get('is_row_merge') and m.get('is_col_merge')):
                        merged_str += f"  - åˆ—{m['cols']} (è·¨{m['col_span']}åˆ—): '{m['value']}'\n"
                merged_str += "\n"
            
            # æŒ‰è¡Œå·æ±‡æ€»ï¼Œä¾¿äºè¯†åˆ«è¡¨å¤´èŒƒå›´
            merged_str += "æŒ‰è¡Œå·æ±‡æ€»ï¼ˆä¾¿äºè¯†åˆ«è¡¨å¤´èŒƒå›´ï¼‰ï¼š\n"
            row_merge_map = {}
            for m in merged_info[:30]:
                row_start = int(m['rows'].split('-')[0])
                row_end = int(m['rows'].split('-')[1])
                for r in range(row_start, row_end + 1):
                    if r not in row_merge_map:
                        row_merge_map[r] = []
                    row_merge_map[r].append(f"'{m['value']}'")
            
            for row_num in sorted(row_merge_map.keys())[:30]:
                merged_str += f"  è¡Œ{row_num}: {', '.join(row_merge_map[row_num][:3])}\n"
        else:
            merged_str = "æ— åˆå¹¶å•å…ƒæ ¼"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªExcelè¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ä½¿ç”¨åˆ†æ­¥éª¤æ–¹æ³•åˆ†æä»¥ä¸‹å¤æ‚å¤šçº§è¡¨å¤´ç»“æ„ã€‚

ã€è¡¨æ ¼é¢„è§ˆã€‘ï¼ˆå‰30è¡Œï¼Œ[æ•°å€¼:xxx]è¡¨ç¤ºæ•°å€¼ç±»å‹ï¼Œ[åˆå¹¶:è¡ŒXåˆ—Y]è¡¨ç¤ºåˆå¹¶å•å…ƒæ ¼ï¼Œ[åˆå¹¶å†…]è¡¨ç¤ºåˆå¹¶å•å…ƒæ ¼å†…éƒ¨ï¼‰
{table_str}

{merged_str}

ã€æ€»åˆ—æ•°ã€‘{max_col}

## åˆ†æä»»åŠ¡

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†æè¡¨æ ¼ç»“æ„ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„ç»“æœã€‚

## æ­¥éª¤1ï¼šè¯†åˆ«æ— æ•ˆè¡Œï¼ˆskip_rowsï¼‰

**ç›®æ ‡**ï¼šæ‰¾å‡ºè¡¨å¤´**ä¹‹å‰**çš„æ— æ•ˆè¡Œï¼ˆå¦‚æ–‡æ¡£æ ‡é¢˜ã€è¯´æ˜æ–‡å­—ã€æ³¨é‡Šã€å…¬å¸åç§°ç­‰ï¼‰

**æ— æ•ˆè¡Œçš„å…¸å‹ç‰¹å¾**ï¼š
1. **åªæœ‰å°‘é‡éç©ºå•å…ƒæ ¼**ï¼ˆå¦‚åªæœ‰1-3ä¸ªå•å…ƒæ ¼æœ‰å€¼ï¼‰
2. **ä¸åŒ…å«è¡¨å¤´ç‰¹å¾**ï¼š
   - ä¸åŒ…å«åˆ—åï¼ˆå¦‚"é”€å”®é¢"ã€"å¢é•¿ç‡"ç­‰ï¼‰
   - ä¸åŒ…å«åˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚"é”€å”®äº‹ä¸šéƒ¨"ã€"åä¸œå¤§åŒº"ç­‰ï¼‰
   - ä¸åŒ…å«åˆå¹¶å•å…ƒæ ¼ï¼ˆåˆå¹¶å•å…ƒæ ¼é€šå¸¸æ˜¯è¡¨å¤´çš„æ ‡è¯†ï¼‰
3. **å†…å®¹ç±»å‹**ï¼ˆè¿™äº›éƒ½æ˜¯æ— æ•ˆè¡Œï¼‰ï¼š
   - æ–‡æ¡£æ ‡é¢˜ï¼ˆå¦‚"2024å¹´åº¦æŠ¥è¡¨"ã€"åœºæ™¯ä¸šåŠ¡ç»Ÿè®¡è¡¨"ï¼‰
   - å…¬å¸åç§°æˆ–éƒ¨é—¨åç§°ï¼ˆå¦‚"XXå…¬å¸"ã€"XXéƒ¨é—¨"ã€"å«æ –æ¢§é¢å¤–ä¼"ï¼‰
   - è¯´æ˜æ–‡å­—æˆ–æ³¨é‡Š
   - åªæœ‰æ•°å­—æ²¡æœ‰æ ‡ç­¾çš„è¡Œï¼ˆå¦‚åªæœ‰"222"ã€"111"ã€"22"è¿™æ ·çš„æ•°å­—ï¼Œæ²¡æœ‰å¯¹åº”çš„åˆ—åæˆ–æ ‡ç­¾ï¼‰
   - å®Œå…¨ç©ºè¡Œ
   - **å¡«æŠ¥è¯´æ˜æ–‡å­—**ï¼ˆå¿…é¡»è¯†åˆ«ä¸ºæ— æ•ˆè¡Œï¼‰ï¼š
     - "å¡«æŠ¥æœºæ„"ã€"å¡«æŠ¥æ—¥æœŸ"ã€"å¡«æŠ¥æœºæ„/æ—¥æœŸ"ã€"å¡«æŠ¥æœºæ„ã€æ—¥æœŸ"
     - "å¡«æŠ¥å•ä½"ã€"å¡«æŠ¥äºº"ã€"å¡«æŠ¥æ—¶é—´"
     - ä»»ä½•åŒ…å«"å¡«æŠ¥"å…³é”®è¯çš„è¡Œ
     - æ ¼å¼å¦‚"ï¼ˆå¡«æŠ¥æœºæ„/æ—¥æœŸï¼‰"ã€"å¡«æŠ¥æœºæ„ï¼šXX"ç­‰

**è¡¨å¤´è¡Œçš„å…¸å‹ç‰¹å¾**ï¼ˆç”¨äºåŒºåˆ†ï¼‰ï¼š
1. **åŒ…å«åˆ—åæˆ–åˆ†ç±»æ ‡ç­¾**ï¼ˆå¦‚"é”€å”®äº‹ä¸šéƒ¨"ã€"çº¿ä¸Šé”€å”®é¢"ç­‰ï¼‰
2. **åŒ…å«åˆå¹¶å•å…ƒæ ¼**ï¼ˆåˆå¹¶å•å…ƒæ ¼æ˜¯è¡¨å¤´çš„é‡è¦æ ‡è¯†ï¼‰
3. **æœ‰æ˜ç¡®çš„å±‚çº§ç»“æ„**ï¼ˆå¦‚å¤§åŒºã€çœä»½ã€åŸå¸‚ã€æŒ‡æ ‡ç­‰ï¼‰
4. **è·¨è¶Šå¤šåˆ—**ï¼ˆè¡¨å¤´é€šå¸¸è·¨è¶Šå¤šåˆ—ï¼Œè€Œä¸æ˜¯åªæœ‰1-2ä¸ªå•å…ƒæ ¼ï¼‰

**åˆ¤æ–­æ­¥éª¤**ï¼š
1. **ä»ç¬¬1è¡Œå¼€å§‹å‘ä¸‹æ£€æŸ¥**
2. **å¦‚æœæŸè¡Œç¬¦åˆæ— æ•ˆè¡Œç‰¹å¾**ï¼Œç»§ç»­æ£€æŸ¥ä¸‹ä¸€è¡Œ
3. **å¦‚æœæŸè¡Œç¬¦åˆè¡¨å¤´è¡Œç‰¹å¾**ï¼ˆç‰¹åˆ«æ˜¯åŒ…å«åˆå¹¶å•å…ƒæ ¼æˆ–åˆ†ç±»æ ‡ç­¾ï¼‰ï¼Œåˆ™è¿™è¡Œæ˜¯è¡¨å¤´å¼€å§‹
4. **skip_rows = è¡¨å¤´å¼€å§‹è¡Œå· - 1**

**ç¤ºä¾‹**ï¼š
- ç¬¬1è¡Œï¼šåªæœ‰æ•°å­—"222"ã€"111"ï¼ˆæ— æ ‡ç­¾ï¼‰ â†’ æ— æ•ˆè¡Œ
- ç¬¬2è¡Œï¼šç©ºè¡Œæˆ–åªæœ‰å°‘é‡æ–‡æœ¬ â†’ æ— æ•ˆè¡Œ
- ç¬¬3è¡Œï¼šå…¬å¸åç§°"XXå…¬å¸"æˆ–"å«æ –æ¢§é¢å¤–ä¼" â†’ æ— æ•ˆè¡Œ
- ç¬¬4è¡Œï¼šå¡«æŠ¥è¯´æ˜"å¡«æŠ¥æœºæ„/æ—¥æœŸ"æˆ–"ï¼ˆå¡«æŠ¥æœºæ„/æ—¥æœŸï¼‰" â†’ **æ— æ•ˆè¡Œ**ï¼ˆå¿…é¡»è¯†åˆ«ï¼‰
- ç¬¬5è¡Œï¼šåŒ…å«"é”€å”®äº‹ä¸šéƒ¨"å’Œåˆå¹¶å•å…ƒæ ¼ â†’ **è¿™æ˜¯è¡¨å¤´å¼€å§‹ï¼**
- **ç»“æœ**ï¼šskip_rows=4ï¼ˆå‰4è¡Œéƒ½æ˜¯æ— æ•ˆè¡Œï¼ŒåŒ…æ‹¬å¡«æŠ¥è¯´æ˜è¡Œï¼‰

**æ³¨æ„**ï¼šè¡¨å¤´è¡Œä¸èƒ½ç®—ä½œæ— æ•ˆè¡Œï¼å¦‚æœç¬¬1è¡Œå°±åŒ…å«è¡¨å¤´ç‰¹å¾ï¼ˆå¦‚åˆå¹¶å•å…ƒæ ¼ã€åˆ†ç±»æ ‡ç­¾ï¼‰ï¼Œåˆ™ skip_rows=0

## æ­¥éª¤2ï¼šè¯†åˆ«è¡¨å¤´å±‚çº§ç»“æ„ï¼ˆheader_rowsï¼‰

**ç›®æ ‡**ï¼šæ‰¾å‡ºæ‰€æœ‰è¡¨å¤´è¡Œï¼ŒåŒ…æ‹¬å¤šçº§è¡¨å¤´çš„æ‰€æœ‰å±‚çº§

**åˆ¤æ–­æ ‡å‡†**ï¼š
1. **è¡¨å¤´è¡Œçš„ç‰¹å¾**ï¼š
   - åŒ…å«åˆ—åã€åˆ†ç±»æ ‡ç­¾ã€åˆ†ç»„ä¿¡æ¯ç­‰æ–‡æœ¬å†…å®¹
   - å¯èƒ½åŒ…å«åˆå¹¶å•å…ƒæ ¼ï¼ˆåˆå¹¶å•å…ƒæ ¼æ˜¯è¡¨å¤´çš„é‡è¦æ ‡è¯†ï¼‰
   - é€šå¸¸ä¸åŒ…å«å¤§é‡æ•°å€¼æ•°æ®

2. **åˆå¹¶å•å…ƒæ ¼çš„å¤„ç†ï¼ˆå…³é”®ï¼‰**ï¼š
   - **åˆå¹¶å•å…ƒæ ¼è·¨è¶Šçš„æ‰€æœ‰è¡Œéƒ½æ˜¯è¡¨å¤´çš„ä¸€éƒ¨åˆ†**
   - ä¾‹å¦‚ï¼šå¦‚æœåˆå¹¶å•å…ƒæ ¼è¦†ç›–è¡Œ1-3ï¼Œåˆ™è¡Œ1ã€è¡Œ2ã€è¡Œ3éƒ½æ˜¯è¡¨å¤´
   - å³ä½¿æŸäº›è¡Œçœ‹èµ·æ¥"ç©º"ï¼ˆå€¼åªåœ¨åˆå¹¶åŒºåŸŸå·¦ä¸Šè§’ï¼‰ï¼Œè¿™äº›è¡Œä»ç„¶æ˜¯è¡¨å¤´
   - æŸ¥çœ‹ã€åˆå¹¶å•å…ƒæ ¼è¯¦ç»†ä¿¡æ¯ã€‘ï¼Œæ‰¾å‡ºæ‰€æœ‰è¢«åˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„è¡Œ

3. **å¤šçº§è¡¨å¤´è¯†åˆ«**ï¼š
   - ç¬¬1å±‚ï¼šå¯èƒ½æœ‰è·¨å¤šè¡Œçš„å¤§åˆ†ç±»ï¼ˆå¦‚"é”€å”®äº‹ä¸šéƒ¨"ï¼‰
   - ç¬¬2å±‚ï¼šå¯èƒ½æœ‰è·¨å¤šè¡Œçš„ä¸­åˆ†ç±»ï¼ˆå¦‚"åä¸œå¤§åŒº"ï¼‰
   - ç¬¬3å±‚ï¼šå¯èƒ½æœ‰è·¨å¤šè¡Œçš„å°åˆ†ç±»ï¼ˆå¦‚"æ±Ÿè‹çœ"ï¼‰
   - ç¬¬4å±‚ï¼šå…·ä½“çš„åˆ—åï¼ˆå¦‚"çº¿ä¸Šé”€å”®é¢"ï¼‰
   - **æ‰€æœ‰å±‚çº§çš„è¡Œéƒ½è¦è®¡å…¥ header_rows**

4. **è¯†åˆ«æ–¹æ³•**ï¼š
   - ä»ã€åˆå¹¶å•å…ƒæ ¼æŒ‰è¡Œå·æ±‡æ€»ã€‘ä¸­ï¼Œæ‰¾å‡ºæ‰€æœ‰åŒ…å«åˆå¹¶å•å…ƒæ ¼çš„è¡Œ
   - è¿™äº›è¡Œé€šå¸¸éƒ½æ˜¯è¡¨å¤´çš„ä¸€éƒ¨åˆ†
   - ç»§ç»­å‘ä¸‹æŸ¥æ‰¾ï¼Œç›´åˆ°æ‰¾åˆ°ç¬¬ä¸€è¡ŒåŒ…å«å¤§é‡æ•°å€¼æ•°æ®çš„è¡Œï¼ˆè¿™æ˜¯æ•°æ®è¡Œï¼‰

## æ­¥éª¤3ï¼šç¡®å®šæ•°æ®èµ·å§‹è¡Œï¼ˆdata_start_rowï¼‰

**è®¡ç®—å…¬å¼**ï¼šdata_start_row = skip_rows + header_rows + 1

**éªŒè¯æ–¹æ³•**ï¼š
- æ•°æ®è¡Œé€šå¸¸åŒ…å«å¤§é‡æ•°å€¼æ•°æ®ï¼ˆæ ‡è®°ä¸º[æ•°å€¼:xxx]ï¼‰
- æ•°æ®è¡Œä¸å†æ˜¯è¡¨å¤´æ–‡æœ¬æˆ–åˆ†ç±»æ ‡ç­¾

## æ­¥éª¤4ï¼šç¡®å®šæ•°æ®èµ·å§‹åˆ—ï¼ˆstart_colï¼‰

**ç›®æ ‡**ï¼šæ‰¾å‡ºç¬¬ä¸€ä¸ªè¡¨å¤´è¡Œä¸­ç¬¬ä¸€ä¸ªéç©ºè¡¨å¤´å¼€å§‹çš„åˆ—å·

**åˆ¤æ–­æ ‡å‡†**ï¼š
- å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰è¡¨å¤´ï¼Œåˆ™ start_col=1
- å¦‚æœå‰å‡ åˆ—ä¸ºç©ºï¼ˆå¦‚ç¬¬1ã€2åˆ—ä¸ºç©ºï¼‰ï¼Œä»ç¬¬3åˆ—å¼€å§‹æœ‰è¡¨å¤´ï¼Œåˆ™ start_col=3

## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

```json
{{
    "skip_rows": <è¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œæ•°ï¼Œå¦‚æœç¬¬1è¡Œå°±æ˜¯è¡¨å¤´åˆ™å¡«0>,
    "header_rows": <è¡¨å¤´å ç”¨çš„æ€»è¡Œæ•°ï¼ˆåŒ…æ‹¬æ‰€æœ‰è¡¨å¤´è¡Œå’Œåˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„æ‰€æœ‰è¡Œï¼‰>,
    "header_type": "<singleæˆ–multi>",
    "data_start_row": <æ•°æ®å¼€å§‹è¡Œå·ï¼ˆ1-indexedï¼‰ï¼Œå¿…é¡»ç­‰äºskip_rows+header_rows+1>,
    "start_col": <æ•°æ®èµ·å§‹åˆ—å·ï¼ˆ1-indexedï¼‰>,
    "valid_cols": null,
    "confidence": "<high/medium/low>",
     "reason": "<è¯¦ç»†è¯´æ˜ï¼š\n1. å¦‚ä½•è¯†åˆ«æ— æ•ˆè¡Œï¼ˆskip_rowsï¼‰ï¼Œç‰¹åˆ«è¯´æ˜æ˜¯å¦åŒ…å«å¡«æŠ¥è¯´æ˜ã€å…¬å¸åç§°ã€åªæœ‰æ•°å­—çš„è¡Œç­‰\n2. å¦‚ä½•è¯†åˆ«è¡¨å¤´å±‚çº§ç»“æ„ï¼ˆheader_rowsï¼‰ï¼Œç‰¹åˆ«è¯´æ˜åˆå¹¶å•å…ƒæ ¼çš„å¤„ç†\n3. å¦‚ä½•ç¡®å®šæ•°æ®èµ·å§‹è¡Œå’Œèµ·å§‹åˆ—>"
}}
```

## å…³é”®è§„åˆ™

1. **æ— æ•ˆè¡Œè¯†åˆ«ï¼ˆé‡è¦ï¼‰**ï¼š
   - **åªæœ‰æ•°å­—æ²¡æœ‰æ ‡ç­¾çš„è¡Œæ˜¯æ— æ•ˆè¡Œ**ï¼ˆå¦‚åªæœ‰"222"ã€"111"ã€"22"è¿™æ ·çš„æ•°å­—ï¼Œæ²¡æœ‰"é”€å”®é¢"ã€"å¢é•¿ç‡"ç­‰æ ‡ç­¾ï¼‰
   - **å…¬å¸åç§°ã€éƒ¨é—¨åç§°è¡Œæ˜¯æ— æ•ˆè¡Œ**ï¼ˆå¦‚"XXå…¬å¸"ã€"XXéƒ¨é—¨"ã€"å«æ –æ¢§é¢å¤–ä¼"ï¼‰
   - **å¡«æŠ¥è¯´æ˜è¡Œæ˜¯æ— æ•ˆè¡Œ**ï¼ˆå¦‚"å¡«æŠ¥æœºæ„"ã€"å¡«æŠ¥æ—¥æœŸ"ã€"å¡«æŠ¥æœºæ„/æ—¥æœŸ"ã€"ï¼ˆå¡«æŠ¥æœºæ„/æ—¥æœŸï¼‰"ç­‰ï¼Œä»»ä½•åŒ…å«"å¡«æŠ¥"å…³é”®è¯çš„è¡Œï¼‰
   - **ç©ºè¡Œæˆ–åªæœ‰å°‘é‡æ–‡æœ¬çš„è¡Œå¯èƒ½æ˜¯æ— æ•ˆè¡Œ**
   - **ä¸åŒ…å«åˆå¹¶å•å…ƒæ ¼ã€åˆ—åã€åˆ†ç±»æ ‡ç­¾çš„è¡Œé€šå¸¸æ˜¯æ— æ•ˆè¡Œ**

2. **è¡¨å¤´è¡Œè¯†åˆ«ï¼ˆé‡è¦ï¼‰**ï¼š
   - **åŒ…å«åˆå¹¶å•å…ƒæ ¼çš„è¡Œé€šå¸¸æ˜¯è¡¨å¤´**ï¼ˆåˆå¹¶å•å…ƒæ ¼æ˜¯è¡¨å¤´çš„é‡è¦æ ‡è¯†ï¼‰
   - **åŒ…å«åˆ†ç±»æ ‡ç­¾çš„è¡Œæ˜¯è¡¨å¤´**ï¼ˆå¦‚"é”€å”®äº‹ä¸šéƒ¨"ã€"åä¸œå¤§åŒº"ç­‰ï¼‰
   - **åŒ…å«åˆ—åçš„è¡Œæ˜¯è¡¨å¤´**ï¼ˆå¦‚"çº¿ä¸Šé”€å”®é¢"ã€"å¢é•¿ç‡"ç­‰ï¼‰

3. **è¡¨å¤´è¡Œä¸èƒ½ç®—ä½œskip_rows**ï¼šå¦‚æœ1-5è¡Œéƒ½æ˜¯è¡¨å¤´ï¼Œåˆ™ skip_rows=0, header_rows=5

4. **åˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„æ‰€æœ‰è¡Œéƒ½æ˜¯è¡¨å¤´**ï¼šä¸è¦é—æ¼ä»»ä½•è¢«åˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„è¡Œ

5. **ä¸è¦å› ä¸ºè¡Œçœ‹èµ·æ¥"ç©º"å°±è®¤ä¸ºå®ƒä¸æ˜¯è¡¨å¤´**ï¼šåˆå¹¶å•å…ƒæ ¼çš„å€¼åªåœ¨å·¦ä¸Šè§’æ˜¾ç¤º

6. **è¡Œå·å’Œåˆ—å·éƒ½ä»1å¼€å§‹è®¡æ•°**

7. **valid_cols å§‹ç»ˆè®¾ä¸º null**

8. **åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹**

## å¤æ‚ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŒ…å«æ— æ•ˆè¡Œçš„å¤æ‚è¡¨å¤´

å¦‚æœè¡¨æ ¼ç»“æ„æ˜¯ï¼š
- ç¬¬1è¡Œï¼šåªæœ‰æ•°å­—"222"ã€"111"ï¼ˆæ— è¡¨å¤´ç‰¹å¾ï¼Œæ— æ•ˆè¡Œï¼‰
- ç¬¬2è¡Œï¼šç©ºè¡Œæˆ–åªæœ‰å°‘é‡æ–‡æœ¬ï¼ˆæ— æ•ˆè¡Œï¼‰
- ç¬¬3è¡Œï¼šå…¬å¸åç§°"XXå…¬å¸"æˆ–"å«æ –æ¢§é¢å¤–ä¼"å’Œæ•°å­—"22"ï¼ˆæ— æ•ˆè¡Œï¼‰
- ç¬¬4è¡Œï¼šå¡«æŠ¥è¯´æ˜"å¡«æŠ¥æœºæ„/æ—¥æœŸ"æˆ–"ï¼ˆå¡«æŠ¥æœºæ„/æ—¥æœŸï¼‰"ï¼ˆ**æ— æ•ˆè¡Œï¼Œå¿…é¡»è¯†åˆ«**ï¼‰
- ç¬¬5è¡Œï¼šåˆå¹¶å•å…ƒæ ¼ï¼ˆè¡Œ5ï¼Œåˆ—D-Iï¼‰æ˜¾ç¤º"é”€å”®äº‹ä¸šéƒ¨"ï¼ˆè¡¨å¤´å¼€å§‹ï¼ï¼‰
- ç¬¬6è¡Œï¼šåˆå¹¶å•å…ƒæ ¼æ˜¾ç¤º"åä¸œå¤§åŒº"ã€"ååŒ—å¤§åŒº"ç­‰ï¼ˆè¡¨å¤´ï¼‰
- ç¬¬7-9è¡Œï¼šå¤šçº§è¡¨å¤´ï¼ˆè¡¨å¤´ï¼‰
- ç¬¬10è¡Œï¼šæ•°æ®å¼€å§‹ï¼ˆåŒ…å«æ•°å€¼ï¼‰

**åˆ†æè¿‡ç¨‹**ï¼š
1. **è¯†åˆ«æ— æ•ˆè¡Œ**ï¼š
   - ç¬¬1è¡Œï¼šåªæœ‰æ•°å­—ï¼Œæ— è¡¨å¤´ç‰¹å¾ â†’ æ— æ•ˆè¡Œ
   - ç¬¬2è¡Œï¼šç©ºè¡Œ â†’ æ— æ•ˆè¡Œ
   - ç¬¬3è¡Œï¼šå…¬å¸åç§°ï¼Œæ— è¡¨å¤´ç‰¹å¾ â†’ æ— æ•ˆè¡Œ
   - ç¬¬4è¡Œï¼šå¡«æŠ¥è¯´æ˜"å¡«æŠ¥æœºæ„/æ—¥æœŸ" â†’ **æ— æ•ˆè¡Œ**ï¼ˆåŒ…å«"å¡«æŠ¥"å…³é”®è¯ï¼‰
   - ç¬¬5è¡Œï¼šåŒ…å«"é”€å”®äº‹ä¸šéƒ¨"å’Œåˆå¹¶å•å…ƒæ ¼ â†’ **è¿™æ˜¯è¡¨å¤´å¼€å§‹ï¼**
   - skip_rows=4ï¼ˆå‰4è¡Œéƒ½æ˜¯æ— æ•ˆè¡Œï¼ŒåŒ…æ‹¬å¡«æŠ¥è¯´æ˜è¡Œï¼‰

2. **è¯†åˆ«è¡¨å¤´**ï¼š
   - ç¬¬5è¡Œï¼šåˆå¹¶å•å…ƒæ ¼æ˜¾ç¤º"é”€å”®äº‹ä¸šéƒ¨" â†’ è¡¨å¤´
   - ç¬¬6-9è¡Œï¼šå¤šçº§è¡¨å¤´ç»“æ„ â†’ è¡¨å¤´
   - header_rows=5ï¼ˆè¡Œ5-9éƒ½æ˜¯è¡¨å¤´ï¼‰

3. **ç¡®å®šæ•°æ®èµ·å§‹è¡Œ**ï¼š
   - data_start_row=10ï¼ˆskip_rows+header_rows+1=4+5+1ï¼‰

**æ­£ç¡®è¾“å‡º**ï¼š
```json
{{
    "skip_rows": 4,
    "header_rows": 5,
    "header_type": "multi",
    "data_start_row": 10,
    "start_col": 1,
    "valid_cols": null,
    "confidence": "high",
     "reason": "ç¬¬1-4è¡Œæ˜¯æ— æ•ˆè¡Œï¼šç¬¬1è¡Œåªæœ‰æ•°å­—æ— è¡¨å¤´ç‰¹å¾ï¼Œç¬¬2è¡Œç©ºè¡Œï¼Œç¬¬3è¡Œå…¬å¸åç§°ï¼Œç¬¬4è¡Œå¡«æŠ¥è¯´æ˜'å¡«æŠ¥æœºæ„/æ—¥æœŸ'ã€‚ç¬¬5è¡Œå¼€å§‹æ˜¯è¡¨å¤´ï¼ˆåŒ…å«'é”€å”®äº‹ä¸šéƒ¨'å’Œåˆå¹¶å•å…ƒæ ¼ï¼‰ï¼Œæ‰€ä»¥skip_rows=4ã€‚ç¬¬5-9è¡Œæ˜¯å¤šçº§è¡¨å¤´ï¼Œæ‰€ä»¥header_rows=5ã€‚ç¬¬10è¡Œå¼€å§‹åŒ…å«æ•°å€¼æ•°æ®ï¼Œæ‰€ä»¥data_start_row=10ã€‚"
}}
```

### ç¤ºä¾‹2ï¼šæ— æ— æ•ˆè¡Œçš„è¡¨å¤´

å¦‚æœè¡¨æ ¼ç»“æ„æ˜¯ï¼š
- ç¬¬1è¡Œï¼šåˆå¹¶å•å…ƒæ ¼ï¼ˆè¡Œ1-2ï¼Œåˆ—1-5ï¼‰æ˜¾ç¤º"é”€å”®äº‹ä¸šéƒ¨"ï¼ˆå¤§åˆ†ç±»ï¼‰
- ç¬¬3è¡Œï¼šåˆå¹¶å•å…ƒæ ¼ï¼ˆè¡Œ3-4ï¼Œåˆ—1-3ï¼‰æ˜¾ç¤º"åä¸œå¤§åŒº"ï¼ˆä¸­åˆ†ç±»ï¼‰
- ç¬¬5è¡Œï¼šå…·ä½“åˆ—åï¼ˆ"çº¿ä¸Šé”€å”®é¢"ã€"çº¿ä¸‹é”€å”®é¢"ç­‰ï¼‰
- ç¬¬6è¡Œï¼šæ•°æ®å¼€å§‹ï¼ˆåŒ…å«æ•°å€¼ï¼‰

**åˆ†æè¿‡ç¨‹**ï¼š
1. skip_rows=0ï¼ˆç¬¬1è¡Œå°±æ˜¯è¡¨å¤´ï¼‰
2. header_rows=5ï¼ˆè¡Œ1-5éƒ½æ˜¯è¡¨å¤´ï¼ŒåŒ…æ‹¬åˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„è¡Œ2å’Œè¡Œ4ï¼‰
3. data_start_row=6ï¼ˆskip_rows+header_rows+1=0+5+1ï¼‰

**æ­£ç¡®è¾“å‡º**ï¼š
```json
{{
    "skip_rows": 0,
    "header_rows": 5,
    "header_type": "multi",
    "data_start_row": 6,
    "start_col": 1,
    "valid_cols": null,
    "confidence": "high",
    "reason": "ç¬¬1è¡Œå¼€å§‹å°±æ˜¯è¡¨å¤´ï¼Œskip_rows=0ã€‚ç¬¬1è¡Œæœ‰åˆå¹¶å•å…ƒæ ¼ï¼ˆè¡Œ1-2ï¼‰ï¼Œç¬¬3è¡Œæœ‰åˆå¹¶å•å…ƒæ ¼ï¼ˆè¡Œ3-4ï¼‰ï¼Œç¬¬5è¡Œæ˜¯å…·ä½“åˆ—åï¼Œæ‰€ä»¥header_rows=5ã€‚ç¬¬6è¡Œå¼€å§‹åŒ…å«æ•°å€¼æ•°æ®ï¼Œæ‰€ä»¥data_start_row=6ã€‚"
}}
```"""
        
        return prompt
    
    def _build_validation_prompt(self, preview_data: List[List], merged_info: List[Dict], 
                                rule_analysis: HeaderAnalysis) -> str:
        """
        æ„å»ºLLMéªŒè¯æç¤ºè¯ï¼ˆé‡æ„ç‰ˆï¼‰
        
        ä½¿ç”¨ä¸åˆ†ææ–¹æ³•ä¸€è‡´çš„æ ¼å¼å’Œé€»è¾‘
        """
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºè¡¨æ ¼å½¢å¼ï¼ˆæ˜¾ç¤ºæ›´å¤šåˆ—ï¼‰
        num_cols_to_show = min(20, len(preview_data[0]) if preview_data else 20)
        col_headers = " | ".join([f"åˆ—{i+1:2d}" for i in range(num_cols_to_show)])
        table_str = f"è¡Œå· | {col_headers}\n" + "-" * 120 + "\n"
        
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join(str(cell)[:12] for cell in row[:num_cols_to_show])
            table_str += f"  {i:2d}  | {row_str}\n"
        
        # æ ¼å¼åŒ–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼ˆä¸åˆ†ææ–¹æ³•ä¸€è‡´ï¼‰
        if merged_info:
            # æŒ‰è¡Œåˆå¹¶å’Œåˆ—åˆå¹¶åˆ†ç±»
            row_merges = [m for m in merged_info if m.get('is_row_merge', False)]
            col_merges = [m for m in merged_info if m.get('is_col_merge', False)]
            both_merges = [m for m in merged_info if m.get('is_row_merge', False) and m.get('is_col_merge', False)]
            
            merged_str = "ã€åˆå¹¶å•å…ƒæ ¼è¯¦ç»†ä¿¡æ¯ã€‘\n\n"
            
            if both_merges:
                merged_str += "åŒæ—¶è·¨è¡Œè·¨åˆ—çš„åˆå¹¶å•å…ƒæ ¼ï¼š\n"
                for m in both_merges[:20]:
                    merged_str += f"  - è¡Œ{m['rows']} åˆ—{m['cols']} (è·¨{m['row_span']}è¡Œ{m['col_span']}åˆ—): '{m['value']}'\n"
                merged_str += "\n"
            
            if row_merges:
                merged_str += "è·¨è¡Œåˆå¹¶å•å…ƒæ ¼ï¼š\n"
                for m in row_merges[:20]:
                    if not (m.get('is_row_merge') and m.get('is_col_merge')):
                        merged_str += f"  - è¡Œ{m['rows']} (è·¨{m['row_span']}è¡Œ): '{m['value']}'\n"
                merged_str += "\n"
            
            # æŒ‰è¡Œå·æ±‡æ€»
            merged_str += "æŒ‰è¡Œå·æ±‡æ€»ï¼š\n"
            row_merge_map = {}
            for m in merged_info[:30]:
                row_start = int(m['rows'].split('-')[0])
                row_end = int(m['rows'].split('-')[1])
                for r in range(row_start, row_end + 1):
                    if r not in row_merge_map:
                        row_merge_map[r] = []
                    row_merge_map[r].append(f"'{m['value']}'")
            
            for row_num in sorted(row_merge_map.keys())[:30]:
                merged_str += f"  è¡Œ{row_num}: {', '.join(row_merge_map[row_num][:3])}\n"
        else:
            merged_str = "æ— åˆå¹¶å•å…ƒæ ¼"
        
        prompt = f"""è¯·éªŒè¯ä»¥ä¸‹Excelè¡¨æ ¼çš„è¡¨å¤´åˆ†æç»“æœæ˜¯å¦æ­£ç¡®ã€‚

ã€è¡¨æ ¼é¢„è§ˆã€‘ï¼ˆå‰30è¡Œï¼Œ[æ•°å€¼:xxx]è¡¨ç¤ºæ•°å€¼ç±»å‹ï¼Œ[åˆå¹¶:è¡ŒXåˆ—Y]è¡¨ç¤ºåˆå¹¶å•å…ƒæ ¼ï¼Œ[åˆå¹¶å†…]è¡¨ç¤ºåˆå¹¶å•å…ƒæ ¼å†…éƒ¨ï¼‰
{table_str}

{merged_str}

ã€å½“å‰åˆ†æç»“æœã€‘
- è·³è¿‡è¡Œæ•°(skip_rows): {rule_analysis.skip_rows} ï¼ˆè¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œæ•°ï¼‰
- è¡¨å¤´è¡Œæ•°(header_rows): {rule_analysis.header_rows} ï¼ˆæ‰€æœ‰è¡¨å¤´è¡Œçš„æ€»æ•°ï¼ŒåŒ…æ‹¬åˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„æ‰€æœ‰è¡Œï¼‰
- è¡¨å¤´ç±»å‹: {rule_analysis.header_type}
- æ•°æ®èµ·å§‹è¡Œ: {rule_analysis.data_start_row} ï¼ˆåº”è¯¥ç­‰äº skip_rows + header_rows + 1ï¼‰
- åˆ†æåŸå› : {rule_analysis.reason}

è¯·éªŒè¯è¿™ä¸ªç»“æœæ˜¯å¦åˆç†ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "is_valid": <trueæˆ–falseï¼Œè¡¨ç¤ºç»“æœæ˜¯å¦åˆç†>,
    "confidence": "<high/medium/low>",
    "suggestions": {{
        "skip_rows": <å»ºè®®çš„è·³è¿‡è¡Œæ•°ï¼Œå¦‚æœåˆç†åˆ™ä¸å½“å‰ç»“æœç›¸åŒ>,
        "header_rows": <å»ºè®®çš„è¡¨å¤´è¡Œæ•°ï¼Œå¦‚æœåˆç†åˆ™ä¸å½“å‰ç»“æœç›¸åŒ>,
        "header_type": "<singleæˆ–multi>",
        "data_start_row": <å»ºè®®çš„æ•°æ®èµ·å§‹è¡Œï¼Œåº”è¯¥ç­‰äºskip_rows+header_rows+1>
    }},
    "reason": "<éªŒè¯è¯´æ˜ï¼šå¦‚æœåˆç†ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆï¼›å¦‚æœä¸åˆç†ï¼ŒæŒ‡å‡ºé—®é¢˜å¹¶ç»™å‡ºå»ºè®®>"
}}

## éªŒè¯è¦ç‚¹ï¼ˆé‡è¦ï¼‰

### 1. skip_rows éªŒè¯
- **å…³é”®**ï¼šskip_rows åªè®¡ç®—**è¡¨å¤´ä¹‹å‰**çš„æ— æ•ˆè¡Œï¼ˆå¦‚æ–‡æ¡£æ ‡é¢˜ã€æ³¨é‡Šç­‰ï¼‰
- **å¸¸è§é”™è¯¯**ï¼šä¸è¦æŠŠè¡¨å¤´è¡Œç®—ä½œskip_rows
- **åˆ¤æ–­æ ‡å‡†**ï¼š
  - å¦‚æœç¬¬1è¡Œå°±æ˜¯è¡¨å¤´ï¼ˆåŒ…å«åˆ—åã€åˆ†ç±»æ ‡ç­¾ï¼‰ï¼Œåˆ™ skip_rows åº”è¯¥ä¸º 0
  - å¦‚æœå‰Nè¡Œæ˜¯æ–‡æ¡£æ ‡é¢˜ç­‰éè¡¨å¤´å†…å®¹ï¼Œç¬¬N+1è¡Œæ‰æ˜¯è¡¨å¤´ï¼Œåˆ™ skip_rows åº”è¯¥ä¸º N

### 2. header_rows éªŒè¯
- **å…³é”®**ï¼šheader_rows åº”è¯¥åŒ…å«**æ‰€æœ‰è¡¨å¤´è¡Œ**ï¼ŒåŒ…æ‹¬å¤šçº§è¡¨å¤´çš„æ‰€æœ‰è¡Œ
- **åˆ¤æ–­æ ‡å‡†**ï¼š
  - å•è¡¨å¤´ï¼šåªæœ‰ä¸€è¡Œè¡¨å¤´ â†’ header_rows=1
  - å¤šçº§è¡¨å¤´ï¼šå¦‚æœ1-5è¡Œéƒ½æ˜¯è¡¨å¤´ï¼Œåˆ™ header_rows=5
  - **åˆå¹¶å•å…ƒæ ¼å¤„ç†ï¼ˆé‡è¦ï¼‰**ï¼š
    - åˆå¹¶å•å…ƒæ ¼è·¨è¶Šçš„æ‰€æœ‰è¡Œéƒ½åº”è¯¥è®¡å…¥ header_rows
    - å³ä½¿åˆå¹¶å•å…ƒæ ¼è®©æŸäº›è¡Œçœ‹èµ·æ¥"ç©º"ï¼ˆå€¼åªåœ¨åˆå¹¶åŒºåŸŸå·¦ä¸Šè§’ï¼‰ï¼Œè¿™äº›è¡Œä»ç„¶æ˜¯è¡¨å¤´çš„ä¸€éƒ¨åˆ†
    - æŸ¥çœ‹ã€åˆå¹¶å•å…ƒæ ¼ã€‘ä¿¡æ¯ï¼Œç¡®è®¤æ‰€æœ‰è¢«åˆå¹¶çš„è¡Œéƒ½åŒ…å«åœ¨ header_rows ä¸­
    - ä¾‹å¦‚ï¼šå¦‚æœåˆå¹¶å•å…ƒæ ¼æ˜¾ç¤º"è¡Œ1-2"ï¼Œåˆ™è¡Œ1å’Œè¡Œ2éƒ½åº”è¯¥è®¡å…¥ header_rows

### 3. data_start_row éªŒè¯
- **è®¡ç®—å…¬å¼**ï¼šdata_start_row å¿…é¡»ç­‰äº skip_rows + header_rows + 1
- **åˆ¤æ–­æ ‡å‡†**ï¼šæ•°æ®è¡Œé€šå¸¸åŒ…å«æ•°å€¼æ•°æ®ï¼Œä¸å†æ˜¯è¡¨å¤´æ–‡æœ¬

### 4. å¸¸è§é”™è¯¯æ£€æŸ¥
- âŒ **é”™è¯¯**ï¼šå¦‚æœ1-5è¡Œéƒ½æ˜¯è¡¨å¤´ï¼Œä½†åˆ†æç»“æœæ˜¯ skip_rows=3, header_rows=3
  - **æ­£ç¡®**ï¼šåº”è¯¥æ˜¯ skip_rows=0, header_rows=5
- âŒ **é”™è¯¯**ï¼šæŠŠè¡¨å¤´è¡Œè¯¯åˆ¤ä¸ºéœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œ
- âŒ **é”™è¯¯**ï¼šé—æ¼äº†å¤šçº§è¡¨å¤´çš„æŸäº›è¡Œ
- âŒ **é”™è¯¯ï¼ˆåˆå¹¶å•å…ƒæ ¼ï¼‰**ï¼šå¦‚æœåˆå¹¶å•å…ƒæ ¼è¦†ç›–è¡Œ1-2ï¼Œä½†åªè®¡ç®—äº†è¡Œ1ï¼Œé—æ¼äº†è¡Œ2
  - **æ­£ç¡®**ï¼šåˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„æ‰€æœ‰è¡Œï¼ˆè¡Œ1-2ï¼‰éƒ½åº”è¯¥è®¡å…¥ header_rows
- âŒ **é”™è¯¯ï¼ˆåˆå¹¶å•å…ƒæ ¼ï¼‰**ï¼šå› ä¸ºåˆå¹¶å•å…ƒæ ¼è®©æŸäº›è¡Œçœ‹èµ·æ¥"ç©º"ï¼Œå°±è®¤ä¸ºè¿™äº›è¡Œä¸æ˜¯è¡¨å¤´
  - **æ­£ç¡®**ï¼šå³ä½¿è¡Œçœ‹èµ·æ¥ç©ºï¼ˆå€¼åœ¨åˆå¹¶åŒºåŸŸå·¦ä¸Šè§’ï¼‰ï¼Œè¿™äº›è¡Œä»ç„¶æ˜¯è¡¨å¤´çš„ä¸€éƒ¨åˆ†

## éªŒè¯æ­¥éª¤

1. **é¦–å…ˆæŸ¥çœ‹åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯**ï¼šè¯†åˆ«å“ªäº›è¡Œè¢«åˆå¹¶å•å…ƒæ ¼è¦†ç›–ï¼Œè¿™äº›è¡Œéƒ½åº”è¯¥è®¡å…¥ header_rows
2. **ç¡®è®¤è¡¨å¤´èŒƒå›´**ï¼šä»ç¬¬å‡ è¡Œåˆ°ç¬¬å‡ è¡Œæ˜¯è¡¨å¤´ï¼Ÿï¼ˆåŒ…æ‹¬åˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„æ‰€æœ‰è¡Œï¼‰
3. **æ£€æŸ¥skip_rows**ï¼šè¡¨å¤´ä¹‹å‰æ˜¯å¦æœ‰æ— æ•ˆè¡Œï¼Ÿå¦‚æœæ²¡æœ‰ï¼Œskip_rowsåº”è¯¥ä¸º0
4. **éªŒè¯header_rows**ï¼š
   - æ˜¯å¦åŒ…å«äº†æ‰€æœ‰è¡¨å¤´è¡Œï¼Ÿ
   - **ç‰¹åˆ«æ³¨æ„**ï¼šåˆå¹¶å•å…ƒæ ¼è¦†ç›–çš„æ‰€æœ‰è¡Œæ˜¯å¦éƒ½åŒ…å«åœ¨å†…ï¼Ÿ
   - ä¸è¦å› ä¸ºæŸäº›è¡Œçœ‹èµ·æ¥"ç©º"å°±è®¤ä¸ºå®ƒä»¬ä¸æ˜¯è¡¨å¤´
5. **éªŒè¯data_start_row**ï¼šæ˜¯å¦ç­‰äº skip_rows + header_rows + 1ï¼Ÿ

å¦‚æœå½“å‰åˆ†æç»“æœåˆç†ï¼Œä¿æŒåŸç»“æœï¼›å¦‚æœä¸åˆç†ï¼Œåœ¨suggestionsä¸­ç»™å‡ºä¿®æ­£å»ºè®®ã€‚

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""
        
        return prompt
    
    def _call_llm(self, prompt: str, llm_api_key: Optional[str] = None, 
                  llm_base_url: Optional[str] = None, llm_model: Optional[str] = None,
                  timeout: Optional[int] = None) -> str:
        """è°ƒç”¨LLM APIï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰
        
        å‚æ•°:
            prompt: æç¤ºè¯
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»é…ç½®è¯»å–
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        base_url = llm_base_url if llm_base_url is not None else EXCEL_LLM_BASE_URL
        model = llm_model if llm_model is not None else EXCEL_LLM_MODEL
        
        logger.info("=" * 60)
        logger.info("ğŸ¤– è°ƒç”¨ LLM API è¿›è¡ŒExcelè¡¨æ ¼åˆ†æ")
        logger.info(f"ğŸ”— EXCEL_LLM_BASE_URL: {base_url}")
        logger.info(f"ğŸ“Œ æ¨¡å‹: {model}")
        logger.info(f"ğŸ”‘ API Key: {'å·²é…ç½®' if api_key else 'æœªé…ç½®'}")
        
        if not api_key:
            logger.error("âŒ æœªé…ç½® LLM API Keyï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return None
            
        url = base_url
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # ä½¿ç”¨æµå¼è°ƒç”¨ä»¥æ”¯æŒ thinking åŠŸèƒ½
        base_payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.4,
            "max_tokens": 1000,  # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒæ›´è¯¦ç»†çš„åˆ†æ
            "stream": True,  # æµå¼è°ƒç”¨
        }
        
        # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤90ç§’
        request_timeout = timeout if timeout is not None else 90
        
        logger.info(f"ğŸ“¡ å‘é€ LLM API è¯·æ±‚åˆ°: {url} (æµå¼è°ƒç”¨)")
        logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.info(f"â±ï¸ è¶…æ—¶è®¾ç½®: {request_timeout} ç§’")
        
        try:
            # ä¼˜å…ˆå°è¯•å¯ç”¨ thinking åŠŸèƒ½
            payload_with_thinking = base_payload.copy()
            payload_with_thinking["enable_thinking"] = True
            
            logger.debug(f"ğŸ“¦ è¯·æ±‚ payload (å¯ç”¨ thinking): {json.dumps(payload_with_thinking, ensure_ascii=False, indent=2)}")
            
            response = requests.post(
                url, 
                headers=headers, 
                json=payload_with_thinking, 
                timeout=request_timeout,
                stream=True  # å¯ç”¨æµå¼å“åº”
            )
            
            # å¦‚æœå¯ç”¨ thinking å¤±è´¥ï¼Œå›é€€åˆ°ä¸ä½¿ç”¨ thinking
            if response.status_code != 200:
                try:
                    error_json = response.json()
                    if "enable_thinking" in str(error_json).lower():
                        logger.warning("âš ï¸ å¯ç”¨ thinking å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ thinking")
                        payload_no_thinking = base_payload.copy()
                        logger.debug(f"ğŸ“¦ è¯·æ±‚ payload (ä¸ä½¿ç”¨ thinking): {json.dumps(payload_no_thinking, ensure_ascii=False, indent=2)}")
                        response = requests.post(
                            url, 
                            headers=headers, 
                            json=payload_no_thinking, 
                            timeout=request_timeout,
                            stream=True
                        )
                except:
                    pass
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¾“å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if response.status_code != 200:
                error_detail = ""
                try:
                    # å¯¹äºæµå¼å“åº”ï¼Œå°è¯•è¯»å–é”™è¯¯ä¿¡æ¯
                    error_text = ""
                    for line in response.iter_lines():
                        if line:
                            line_str = line.decode('utf-8')
                            if line_str.startswith('data: '):
                                line_str = line_str[6:]
                            try:
                                error_json = json.loads(line_str)
                                error_detail = json.dumps(error_json, ensure_ascii=False, indent=2)
                                break
                            except:
                                error_text += line_str + "\n"
                    if not error_detail:
                        error_detail = error_text or response.text
                except:
                    try:
                        error_detail = response.text
                    except:
                        error_detail = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ… (çŠ¶æ€ç : {response.status_code})"
                
                logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                logger.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…:\n{error_detail}")
                logger.error(f"ğŸ”— è¯·æ±‚ URL: {url}")
                logger.error(f"ğŸ“¦ è¯·æ±‚ payload: {json.dumps(base_payload, ensure_ascii=False, indent=2)}")
                return None
            
            # å¤„ç†æµå¼å“åº”
            full_content = ""
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    # è·³è¿‡ SSE æ ¼å¼çš„å‰ç¼€ "data: "
                    if line_str.startswith('data: '):
                        line_str = line_str[6:]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                    if line_str.strip() == '[DONE]':
                        break
                    
                    # è§£æ JSON
                    try:
                        chunk_data = json.loads(line_str)
                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                            delta = chunk_data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                full_content += content
                    except json.JSONDecodeError:
                        # å¿½ç•¥æ— æ³•è§£æçš„è¡Œï¼ˆå¯èƒ½æ˜¯ç©ºè¡Œæˆ–å…¶ä»–æ ¼å¼ï¼‰
                        continue
            
            if not full_content:
                logger.warning("âš ï¸ LLM æµå¼å“åº”ä¸ºç©º")
                return None
            
            logger.info("âœ… LLM API è°ƒç”¨æˆåŠŸ")
            logger.info("=" * 60)
            logger.info("ğŸ“ LLM å“åº”å†…å®¹:")
            logger.info("=" * 60)
            logger.info(full_content)
            logger.info("=" * 60)
            
            return full_content
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥ (ç½‘ç»œé”™è¯¯): {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_json = e.response.json()
                    logger.error(f"ğŸ“‹ API é”™è¯¯å“åº”: {json.dumps(error_json, ensure_ascii=False, indent=2)}")
                except:
                    logger.error(f"ğŸ“‹ API é”™è¯¯å“åº” (æ–‡æœ¬): {e.response.text}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
    
    def _parse_llm_analysis_response(self, response: str) -> HeaderAnalysis:
        """è§£æLLMåˆ†æç»“æœï¼ˆåŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰"""
        if not response:
            raise ValueError("LLMå“åº”ä¸ºç©º")
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            # è§£ææœ‰æ•ˆåˆ—
            valid_cols = data.get('valid_cols')
            if valid_cols is None:
                # å¦‚æœä¸ºnullï¼Œè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
                valid_cols = None
            elif isinstance(valid_cols, list):
                # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                valid_cols = [int(col) for col in valid_cols if isinstance(col, (int, str)) and str(col).isdigit()]
                # å¦‚æœåˆ—è¡¨ä¸ºç©ºæˆ–åŒ…å«æ‰€æœ‰åˆ—ï¼Œè®¾ä¸ºNone
                max_col = self.ws.max_column
                if not valid_cols or set(valid_cols) == set(range(1, max_col + 1)):
                    valid_cols = None
            else:
                valid_cols = None
            
            # è§£æèµ·å§‹åˆ—ï¼ˆé»˜è®¤ä¸º1ï¼‰
            start_col = int(data.get('start_col', 1))
            if start_col < 1:
                start_col = 1
            
            # æ„å»ºHeaderAnalysiså¯¹è±¡
            analysis = HeaderAnalysis(
                skip_rows=int(data.get('skip_rows', 0)),
                header_rows=int(data.get('header_rows', 1)),
                header_type=data.get('header_type', 'single'),
                data_start_row=int(data.get('data_start_row', 1)),
                start_col=start_col,
                confidence=data.get('confidence', 'medium'),
                reason=f"LLMåˆ†æ: {data.get('reason', '')}",
                valid_cols=valid_cols
            )
            
            logger.info(f"âœ… LLMåˆ†æå®Œæˆ:")
            logger.info(f"  - è·³è¿‡è¡Œæ•°: {analysis.skip_rows}")
            logger.info(f"  - è¡¨å¤´è¡Œæ•°: {analysis.header_rows}")
            logger.info(f"  - è¡¨å¤´ç±»å‹: {analysis.header_type}")
            logger.info(f"  - æ•°æ®èµ·å§‹è¡Œ: {analysis.data_start_row}")
            logger.info(f"  - æ•°æ®èµ·å§‹åˆ—: {analysis.start_col}")
            logger.info(f"  - ç½®ä¿¡åº¦: {analysis.confidence}")
            
            return analysis
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"âŒ è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
            logger.error(f"ğŸ“‹ å“åº”å†…å®¹: {response[:500]}")
            raise ValueError(f"è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
    
    def _parse_validation_response(self, response: str, rule_analysis: HeaderAnalysis) -> HeaderAnalysis:
        """è§£æLLMéªŒè¯ç»“æœï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
        if not response:
            # LLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœ
            return rule_analysis
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            is_valid = data.get('is_valid', True)
            suggestions = data.get('suggestions', {})
            
            if is_valid:
                # LLMè®¤ä¸ºè§„åˆ™åˆ†æç»“æœåˆç†ï¼Œä¿æŒåŸç»“æœä½†æ›´æ–°ç½®ä¿¡åº¦å’ŒåŸå› 
                return HeaderAnalysis(
                    skip_rows=rule_analysis.skip_rows,
                    header_rows=rule_analysis.header_rows,
                    header_type=rule_analysis.header_type,
                    data_start_row=rule_analysis.data_start_row,
                    start_col=rule_analysis.start_col,  # ä¿æŒåŸæœ‰çš„èµ·å§‹åˆ—
                    confidence=data.get('confidence', 'high'),  # LLMéªŒè¯é€šè¿‡ï¼Œç½®ä¿¡åº¦æå‡
                    reason=f"è§„åˆ™åˆ†æ+LLMéªŒè¯: {data.get('reason', 'éªŒè¯é€šè¿‡')}",
                    valid_cols=rule_analysis.valid_cols  # ä¿æŒåŸæœ‰çš„åˆ—è¿‡æ»¤ç»“æœ
                )
            else:
                # LLMè®¤ä¸ºä¸åˆç†ï¼Œä½¿ç”¨LLMçš„å»ºè®®
                # æ³¨æ„ï¼šLLMå¯èƒ½å»ºè®®ä¿®æ”¹è¡¨å¤´è¡Œæ•°ï¼Œä½†åˆ—è¿‡æ»¤ç»“æœä»ç„¶ä¿ç•™
                return HeaderAnalysis(
                    skip_rows=suggestions.get('skip_rows', rule_analysis.skip_rows),
                    header_rows=suggestions.get('header_rows', rule_analysis.header_rows),
                    header_type=suggestions.get('header_type', rule_analysis.header_type),
                    data_start_row=suggestions.get('data_start_row', rule_analysis.data_start_row),
                    start_col=suggestions.get('start_col', rule_analysis.start_col),  # ä¿æŒæˆ–ä½¿ç”¨å»ºè®®çš„èµ·å§‹åˆ—
                    confidence=data.get('confidence', 'medium'),
                    reason=f"è§„åˆ™åˆ†æ+LLMä¿®æ­£: {data.get('reason', 'LLMå»ºè®®ä¿®æ­£')}",
                    valid_cols=rule_analysis.valid_cols  # ä¿æŒåŸæœ‰çš„åˆ—è¿‡æ»¤ç»“æœ
                )
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"è§£æLLMéªŒè¯å“åº”å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸè§„åˆ™åˆ†æç»“æœ")
        
        # è§£æå¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœ
        return rule_analysis
    
    # å·²åºŸå¼ƒï¼šè§„åˆ™åˆ†ææ–¹æ³•ï¼Œç°åœ¨å¿…é¡»ä½¿ç”¨LLMåˆ†æ
    # def analyze_with_rules(self) -> HeaderAnalysis:
    #     """åŸºäºè§„åˆ™çš„åˆ†æï¼ˆå·²åºŸå¼ƒï¼Œç°åœ¨å¿…é¡»ä½¿ç”¨LLMåˆ†æï¼‰"""
    #     max_col = self.ws.max_column
    #     skip_rows = 0
    #     header_rows = 1
    #     
    #     # æ£€æµ‹éœ€è¦è·³è¿‡çš„è¡Œ
    #     for row in range(1, min(6, self.ws.max_row + 1)):
    #         row_values = [self.get_cell_value(row, col) for col in range(1, max_col + 1)]
    #         non_empty = sum(1 for v in row_values if v is not None)
    #         
    #         # å¦‚æœåªæœ‰å¾ˆå°‘çš„éç©ºå•å…ƒæ ¼ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜è¡Œ
    #         if non_empty <= 2 and non_empty < max_col * 0.3:
    #             skip_rows = row
    #         else:
    #             break
    #     
    #     # æ£€æµ‹è¡¨å¤´è¡Œæ•°
    #     header_start = skip_rows + 1
    #     
    #     # æ£€æŸ¥åˆå¹¶å•å…ƒæ ¼
    #     max_merged_row = 0
    #     for merged_range in self.ws.merged_cells.ranges:
    #         if merged_range.min_row > skip_rows:
    #             if merged_range.max_row > max_merged_row:
    #                 max_merged_row = merged_range.max_row
    #     
    #     if max_merged_row > header_start:
    #         header_rows = max_merged_row - skip_rows
    #     
    #     # æ£€æµ‹æ•°æ®è¡Œå¼€å§‹ä½ç½®
    #     data_start = skip_rows + header_rows + 1
    #     for row in range(header_start, min(skip_rows + 10, self.ws.max_row + 1)):
    #         row_values = [self.get_cell_value(row, col) for col in range(1, max_col + 1)]
    #         non_empty = sum(1 for v in row_values if v is not None)
    #         numeric = sum(1 for v in row_values if isinstance(v, (int, float)) and not isinstance(v, bool))
    #         
    #         if non_empty > 0 and numeric / max(non_empty, 1) > 0.4:
    #             data_start = row
    #             header_rows = row - skip_rows - 1
    #             break
    #     
    #     header_type = 'multi' if header_rows > 1 else 'single'
    #     
    #     return HeaderAnalysis(
    #         skip_rows=skip_rows,
    #         header_rows=max(1, header_rows),
    #         header_type=header_type,
    #         data_start_row=data_start,
    #         confidence='medium',
    #         reason='åŸºäºè§„åˆ™åˆ†æ',
    #         valid_cols=None
    #     )
    
    def _detect_valid_columns(self, skip_rows: int, header_rows: int, data_start_row: int) -> List[int]:
        """
        æ£€æµ‹æœ‰æ•ˆåˆ—ï¼ˆè¿‡æ»¤æ— æ•ˆåˆ—ï¼‰
        
        æ— æ•ˆåˆ—çš„åˆ¤æ–­æ ‡å‡†ï¼š
        1. è¡¨å¤´åŒºåŸŸå®Œå…¨ä¸ºç©º
        2. æ•°æ®åŒºåŸŸå®Œå…¨ä¸ºç©ºæˆ–æ²¡æœ‰æ•°å€¼æ•°æ®
        
        è¿”å›: æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰
        """
        max_col = self.ws.max_column
        header_start = skip_rows + 1
        header_end = skip_rows + header_rows
        valid_cols = []
        
        logger.info("ğŸ” å¼€å§‹æ£€æµ‹æ— æ•ˆåˆ—...")
        
        for col in range(1, max_col + 1):
            # æ£€æŸ¥è¡¨å¤´åŒºåŸŸæ˜¯å¦æœ‰å†…å®¹
            has_header = False
            for row in range(header_start, header_end + 1):
                value = self.get_cell_value(row, col)
                if value is not None and str(value).strip():
                    has_header = True
                    break
            
            # æ£€æŸ¥æ•°æ®åŒºåŸŸæ˜¯å¦æœ‰æ•°å€¼æ•°æ®
            has_data = False
            numeric_count = 0
            total_count = 0
            for row in range(data_start_row, min(data_start_row + 10, self.ws.max_row + 1)):
                value = self.ws.cell(row, col).value
                if value is not None:
                    total_count += 1
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        numeric_count += 1
                        has_data = True
            
            # å¦‚æœè¡¨å¤´æœ‰å†…å®¹æˆ–æ•°æ®åŒºåŸŸæœ‰æ•°å€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆåˆ—
            if has_header or has_data:
                valid_cols.append(col)
                logger.debug(f"âœ… åˆ— {col}: æœ‰æ•ˆ (è¡¨å¤´: {has_header}, æ•°æ®: {has_data}, æ•°å€¼: {numeric_count}/{total_count})")
            else:
                logger.info(f"âŒ åˆ— {col}: æ— æ•ˆ (è¡¨å¤´ä¸ºç©ºä¸”æ•°æ®ä¸ºç©º)")
        
        logger.info(f"ğŸ“Š åˆ—è¿‡æ»¤ç»“æœ: æ€»åˆ—æ•° {max_col}, æœ‰æ•ˆåˆ—æ•° {len(valid_cols)}, æ— æ•ˆåˆ—æ•° {max_col - len(valid_cols)}")
        
        # å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆï¼Œè¿”å›Noneï¼ˆè¡¨ç¤ºä¸éœ€è¦è¿‡æ»¤ï¼‰
        if len(valid_cols) == max_col:
            return None
        
        return valid_cols
    
    def extract_headers(self, analysis: HeaderAnalysis) -> Tuple[List[str], Dict[str, Dict]]:
        """
        æ ¹æ®åˆ†æç»“æœæå–è¡¨å¤´
        è¿”å›: (åˆ—ååˆ—è¡¨, åˆ—ç»“æ„å…ƒæ•°æ®)
        """
        max_col = self.ws.max_column
        header_start = analysis.skip_rows + 1
        header_end = analysis.skip_rows + analysis.header_rows
        
        # ç¡®å®šè¦å¤„ç†çš„åˆ—ï¼šä» start_col å¼€å§‹ï¼Œå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåˆ™å–äº¤é›†
        all_cols = list(range(analysis.start_col, max_col + 1))
        if analysis.valid_cols is not None:
            # å–äº¤é›†ï¼šä» start_col å¼€å§‹ï¼Œä¸”åœ¨ valid_cols ä¸­çš„åˆ—
            cols_to_process = [col for col in all_cols if col in analysis.valid_cols]
        else:
            cols_to_process = all_cols
        
        logger.info(f"ğŸ“‹ æå–è¡¨å¤´: å¤„ç† {len(cols_to_process)} åˆ—")
        
        column_metadata = {}
        
        if analysis.header_type == 'single':
            # å•è¡¨å¤´
            headers = []
            for col in cols_to_process:
                value = self.get_cell_value(header_start, col)
                col_name = str(value) if value else f'Column_{col}'
                headers.append(col_name)
                column_metadata[col_name] = {"level1": col_name}
            
            headers = self._handle_duplicate_names(headers)
            # æ›´æ–°å…ƒæ•°æ®çš„key
            column_metadata = {h: {"level1": h} for h in headers}
            return headers, column_metadata
        
        else:
            # å¤šè¡¨å¤´ï¼šå±•å¹³
            column_headers = []
            original_metadata_list = []  # ä¿å­˜åŸå§‹å…ƒæ•°æ®åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºå¯¹åº”
            
            for col in cols_to_process:
                parts = []
                levels = {}
                for row_idx, row in enumerate(range(header_start, header_end + 1), 1):
                    value = self.get_cell_value(row, col)
                    if value is not None:
                        part = str(value).strip()
                        parts.append(part)
                        levels[f"level{row_idx}"] = part
                
                # å»é‡è¿ç»­ç›¸åŒå€¼
                unique_parts = []
                for p in parts:
                    if not unique_parts or p != unique_parts[-1]:
                        unique_parts.append(p)
                
                col_name = '_'.join(unique_parts) if unique_parts else f'Column_{col}'
                column_headers.append(col_name)
                original_metadata_list.append(levels)  # æŒ‰é¡ºåºä¿å­˜å…ƒæ•°æ®
            
            # å¤„ç†é‡å¤åˆ—å
            column_headers = self._handle_duplicate_names(column_headers)
            
            # é‡æ–°æ˜ å°„å…ƒæ•°æ®ï¼šä½¿ç”¨ç´¢å¼•å¯¹åº”å…³ç³»
            new_metadata = {}
            for i, header in enumerate(column_headers):
                # ä½¿ç”¨ç´¢å¼•ç›´æ¥è·å–å¯¹åº”çš„å…ƒæ•°æ®
                if i < len(original_metadata_list):
                    new_metadata[header] = original_metadata_list[i]
                else:
                    # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œåˆ›å»ºé»˜è®¤å…ƒæ•°æ®
                    logger.warning(f"âš ï¸ ç´¢å¼•è¶…å‡ºèŒƒå›´: i={i}, headersé•¿åº¦={len(column_headers)}, metadataé•¿åº¦={len(original_metadata_list)}")
                    new_metadata[header] = {"level1": header}
            
            return column_headers, new_metadata
    
    def _handle_duplicate_names(self, names: List[str]) -> List[str]:
        """å¤„ç†é‡å¤åˆ—å"""
        counts = defaultdict(int)
        result = []
        for name in names:
            if counts[name] > 0:
                result.append(f"{name}_{counts[name]}")
            else:
                result.append(name)
            counts[name] += 1
        return result
    
    def to_dataframe(self, analysis: HeaderAnalysis = None, use_llm_validate: bool = False,
                    llm_api_key: Optional[str] = None,
                    llm_base_url: Optional[str] = None,
                    llm_model: Optional[str] = None,
                    preprocessing_timeout: Optional[int] = None) -> Tuple[pd.DataFrame, HeaderAnalysis, Dict[str, Dict], Optional[str]]:
        """
        è½¬æ¢ä¸ºDataFrame
        
        å‚æ•°:
            analysis: é¢„å…ˆçš„åˆ†æç»“æœï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨LLMè‡ªåŠ¨åˆ†æ
            use_llm_validate: å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        
        è¿”å›:
            (DataFrame, åˆ†æç»“æœ, åˆ—ç»“æ„å…ƒæ•°æ®, LLMåŸå§‹å“åº”)
        """
        llm_response = None
        if analysis is None:
            # å¿…é¡»ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼ˆåŒæ—¶åˆ†æè¡Œå’Œåˆ—ï¼‰
            logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨LLMåˆ†æExcelè¡¨æ ¼ç»“æ„ï¼ˆè¡Œå’Œåˆ—ï¼‰...")
            
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
            api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
            if not api_key:
                raise ValueError("LLM APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒExcelåˆ†æã€‚è¯·é…ç½®EXCEL_LLM_API_KEYæˆ–ä¼ å…¥llm_api_keyå‚æ•°")
            
            # ä½¿ç”¨LLMç›´æ¥åˆ†æï¼ˆåŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰
            analysis, llm_response = self.analyze_with_llm(
                llm_api_key=llm_api_key,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
                timeout=preprocessing_timeout
            )
            logger.info("âœ… LLMåˆ†æå®Œæˆï¼ˆå·²åŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰")
            # ä¿å­˜LLMå“åº”åˆ°å®ä¾‹å˜é‡ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨
            self._llm_analysis_response = llm_response
        
        headers, column_metadata = self.extract_headers(analysis)
        
        # ç¡®å®šè¦è¯»å–çš„åˆ—ï¼šä» start_col å¼€å§‹ï¼Œå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåˆ™å–äº¤é›†
        max_col = self.ws.max_column
        all_cols = list(range(analysis.start_col, max_col + 1))
        if analysis.valid_cols is not None:
            # å–äº¤é›†ï¼šä» start_col å¼€å§‹ï¼Œä¸”åœ¨ valid_cols ä¸­çš„åˆ—
            cols_to_read = [col for col in all_cols if col in analysis.valid_cols]
        else:
            cols_to_read = all_cols
        
        logger.info(f"ğŸ“Š è¯»å–æ•°æ®: ä»ç¬¬ {analysis.start_col} åˆ—å¼€å§‹ï¼Œå…± {len(cols_to_read)} åˆ—")
        
        # è¯»å–æ•°æ®
        data = []
        for row in range(analysis.data_start_row, self.ws.max_row + 1):
            row_data = []
            for col in cols_to_read:
                row_data.append(self.ws.cell(row, col).value)
            if any(v is not None for v in row_data):
                data.append(row_data)
        
        df = pd.DataFrame(data, columns=headers)
        
        # æ™ºèƒ½ç±»å‹è½¬æ¢ï¼šå°è¯•å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—
        logger.info("ğŸ”„ å¼€å§‹æ™ºèƒ½ç±»å‹è½¬æ¢...")
        def smart_convert_value(value):
            """æ™ºèƒ½è½¬æ¢å€¼ï¼šå°è¯•å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—"""
            if value is None:
                return value
            if isinstance(value, (int, float)):
                return value
            if isinstance(value, str):
                # å»é™¤å‰åç©ºæ ¼
                value = value.strip()
                if not value:  # ç©ºå­—ç¬¦ä¸²
                    return None
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                try:
                    # å°è¯•æ•´æ•°ï¼ˆæ”¯æŒè´Ÿæ•°ï¼‰
                    if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                        return int(value)
                    # å°è¯•æµ®ç‚¹æ•°ï¼ˆæ”¯æŒç§‘å­¦è®¡æ•°æ³•ï¼‰
                    return float(value)
                except (ValueError, AttributeError):
                    # è½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå­—ç¬¦ä¸²
                    return value
            return value
        
        # å¯¹æ¯åˆ—åº”ç”¨æ™ºèƒ½è½¬æ¢
        for col in df.columns:
            original_type = df[col].dtype
            df[col] = df[col].apply(smart_convert_value)
            new_type = df[col].dtype
            if original_type != new_type:
                logger.debug(f"  åˆ— '{col}': {original_type} â†’ {new_type}")
        
        # ä½¿ç”¨ pandas çš„ convert_dtypes è¿›ä¸€æ­¥ä¼˜åŒ–ç±»å‹æ¨æ–­
        df = df.convert_dtypes()
        
        logger.info(f"âœ… DataFrame åˆ›å»ºå®Œæˆ: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        logger.info(f"ğŸ“Š æ•°æ®ç±»å‹ä¼˜åŒ–å®Œæˆ")
        return df, analysis, column_metadata, llm_response
    
    def close(self):
        """å…³é—­å·¥ä½œç°¿å¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            self.wb.close()
        except Exception:
            pass
        
        # åˆ é™¤ä¸´æ—¶è½¬æ¢çš„ .xlsx æ–‡ä»¶
        if self._temp_xlsx_path and os.path.exists(self._temp_xlsx_path):
            try:
                os.remove(self._temp_xlsx_path)
                logger.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self._temp_xlsx_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {self._temp_xlsx_path}, é”™è¯¯: {e}")


def process_excel_file(
    filepath: str,
    output_dir: str,
    sheet_name: str = None,
    use_llm_validate: bool = False,  # å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œç°åœ¨æ€»æ˜¯ä½¿ç”¨LLM
    output_filename: str = None,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    preprocessing_timeout: Optional[int] = None,
    excel_processing_timeout: Optional[int] = None  # Excelå¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œåœ¨LLMåˆ†æä¹‹å‰
) -> ExcelProcessResult:
    """
    å¤„ç†Excelæ–‡ä»¶çš„ä¸»å‡½æ•°
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        sheet_name: å·¥ä½œè¡¨åç§°
        use_llm_validate: å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ã€‚ç°åœ¨æ€»æ˜¯ä½¿ç”¨LLMè¿›è¡Œåˆ†æ
        output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼Œå¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        excel_processing_timeout: Excelå¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œåœ¨LLMåˆ†æä¹‹å‰ï¼Œé»˜è®¤10ç§’
    
    è¿”å›:
        ExcelProcessResult
    
    æ³¨æ„:
        ç°åœ¨å¿…é¡»ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼Œä¸å†æ”¯æŒè§„åˆ™åˆ†æã€‚è¯·ç¡®ä¿æä¾›llm_api_keyå‚æ•°ã€‚
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # è®°å½•å¼€å§‹æ—¶é—´ï¼Œç”¨äºè¶…æ—¶æ£€æŸ¥
        start_time = time.time()
        excel_processing_timeout_seconds = excel_processing_timeout if excel_processing_timeout is not None else 10
        
        # å¤„ç†Excelï¼ˆç°åœ¨æ€»æ˜¯ä½¿ç”¨LLMåˆ†æï¼‰
        processor = SmartHeaderProcessor(filepath, sheet_name)
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ï¼ˆåœ¨LLMåˆ†æä¹‹å‰ï¼‰
        elapsed_time = time.time() - start_time
        if elapsed_time > excel_processing_timeout_seconds:
            processor.close()
            error_msg = "Excelå†…å®¹è¿‡å¤šæˆ–æ ¼å¼å¤ªå¤æ‚ï¼Œè§£æå¤±è´¥"
            logger.error(f"âŒ Excelå¤„ç†è¶…æ—¶: è€—æ—¶ {elapsed_time:.2f}ç§’ï¼Œè¶…è¿‡é™åˆ¶ {excel_processing_timeout_seconds}ç§’")
            return ExcelProcessResult(
                success=False,
                header_analysis=None,
                processed_file_path=None,
                metadata_file_path=None,
                column_names=[],
                column_metadata={},
                row_count=0,
                error_message=error_msg
            )
        
        df, analysis, column_metadata, llm_response = processor.to_dataframe(
            use_llm_validate=True,  # æ€»æ˜¯ä½¿ç”¨LLMï¼Œå¿½ç•¥ä¼ å…¥çš„use_llm_validateå‚æ•°
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            preprocessing_timeout=preprocessing_timeout
        )
        processor.close()
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_filename:
            base_name = Path(filepath).stem
            output_filename = f"{base_name}_processed"
        
        # ä¿å­˜CSV
        csv_path = os.path.join(output_dir, f"{output_filename}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # æå–å­—æ®µå€¼æ ·æœ¬ï¼ˆåˆ†ç»„èšåˆåçš„å¸¸è§å€¼ï¼‰
        logger.info("ğŸ“Š æå–å­—æ®µå€¼æ ·æœ¬...")
        column_value_samples = extract_column_value_samples(df, max_samples_per_column=10)
        
        # å°†å€¼æ ·æœ¬ä¿¡æ¯åˆå¹¶åˆ°åˆ—å…ƒæ•°æ®ä¸­
        for col_name, samples in column_value_samples.items():
            if col_name in column_metadata:
                column_metadata[col_name]["value_samples"] = samples
            else:
                # å¦‚æœåˆ—ä¸åœ¨å…ƒæ•°æ®ä¸­ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œåˆ›å»ºæ–°çš„å…ƒæ•°æ®é¡¹
                column_metadata[col_name] = {"value_samples": samples}
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "header_analysis": analysis.to_dict(),
            "column_metadata": column_metadata,
            "column_names": list(df.columns),
            "row_count": len(df),
            "original_file": os.path.basename(filepath)
        }
        metadata_path = os.path.join(output_dir, f"{output_filename}_metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å¤„ç†åçš„JSONå…ƒæ•°æ®ï¼ˆæš‚æ—¶æ³¨é‡Šï¼‰
        # logger.info("=" * 80)
        # logger.info("ğŸ“„ å¤„ç†åçš„JSONå…ƒæ•°æ®:")
        # logger.info("=" * 80)
        # logger.info(json.dumps(metadata, ensure_ascii=False, indent=2))
        # logger.info("=" * 80)
        
        return ExcelProcessResult(
            success=True,
            header_analysis=analysis,
            processed_file_path=csv_path,
            metadata_file_path=metadata_path,
            column_names=list(df.columns),
            column_metadata=column_metadata,
            row_count=len(df),
            error_message=None,
            llm_analysis_response=llm_response
        )
        
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return ExcelProcessResult(
            success=False,
            header_analysis=None,
            processed_file_path=None,
            metadata_file_path=None,
            column_names=[],
            column_metadata={},
            row_count=0,
            error_message=error_msg
        )


def get_sheet_names(filepath: str) -> List[str]:
    """è·å–Excelæ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œè¡¨åç§°"""
    try:
        wb = load_workbook(filepath)
        sheets = wb.sheetnames
        wb.close()
        return sheets
    except Exception as e:
        return []


def extract_column_value_samples(
    df: pd.DataFrame,
    max_samples_per_column: int = 10,
    max_unique_ratio: float = 0.5
) -> Dict[str, Dict[str, Any]]:
    """
    æå–æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼æ ·æœ¬ï¼ˆé€šè¿‡åˆ†ç»„èšåˆï¼‰
    
    å‚æ•°:
        df: æ•°æ®æ¡†
        max_samples_per_column: æ¯ä¸ªå­—æ®µæœ€å¤šä¿ç•™çš„æ ·æœ¬æ•°é‡
        max_unique_ratio: å¦‚æœå”¯ä¸€å€¼å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹ï¼Œåˆ™åªæä¾›ç»Ÿè®¡ä¿¡æ¯è€Œä¸ç»Ÿè®¡é¢‘ç‡
    
    è¿”å›:
        å­—å…¸ï¼Œkeyä¸ºåˆ—åï¼Œvalueä¸ºåŒ…å«å¸¸è§å€¼å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    column_samples = {}
    
    for col_name in df.columns:
        col_data = df[col_name]
        
        # è·³è¿‡å®Œå…¨ä¸ºç©ºçš„åˆ—
        if col_data.isna().all():
            continue
        
        # è®¡ç®—éç©ºå€¼æ•°é‡
        non_null_count = col_data.notna().sum()
        if non_null_count == 0:
            continue
        
        # è®¡ç®—å”¯ä¸€å€¼æ•°é‡
        unique_count = col_data.nunique()
        unique_ratio = unique_count / non_null_count if non_null_count > 0 else 1.0
        
        sample_info = {
            "total_count": len(col_data),
            "non_null_count": int(non_null_count),
            "null_count": int(col_data.isna().sum()),
            "unique_count": int(unique_count),
            "data_type": str(col_data.dtype)
        }
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
        is_numeric = pd.api.types.is_numeric_dtype(col_data)
        
        if is_numeric:
            # æ•°å€¼ç±»å‹ï¼šæä¾›ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼ï¼ˆå¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼‰
            sample_info["is_numeric"] = True
            non_null_data = col_data.dropna()
            if len(non_null_data) > 0:
                sample_info["min"] = float(non_null_data.min())
                sample_info["max"] = float(non_null_data.max())
                sample_info["mean"] = float(non_null_data.mean())
                sample_info["median"] = float(non_null_data.median())
            else:
                sample_info["min"] = None
                sample_info["max"] = None
                sample_info["mean"] = None
                sample_info["median"] = None
            
            # å¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œä¹Ÿç»Ÿè®¡é¢‘ç‡
            if unique_ratio <= max_unique_ratio and unique_count <= 100:
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
            elif unique_count <= max_samples_per_column:
                # å³ä½¿å”¯ä¸€å€¼æ¯”ä¾‹é«˜ï¼Œä½†å¦‚æœæ€»æ•°ä¸å¤šï¼Œä¹Ÿå±•ç¤ºæ‰€æœ‰å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œå±•ç¤ºæ‰€æœ‰å€¼"
        else:
            # éæ•°å€¼ç±»å‹ï¼šç»Ÿè®¡é¢‘ç‡
            sample_info["is_numeric"] = False
            
            # å¦‚æœå”¯ä¸€å€¼å¤ªå¤šï¼Œåªæä¾›ç»Ÿè®¡ä¿¡æ¯
            if unique_ratio > max_unique_ratio:
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œä»…å±•ç¤ºéƒ¨åˆ†å¸¸è§å€¼"
                # ä»ç„¶å±•ç¤ºå‰Nä¸ªæœ€å¸¸è§çš„å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
            else:
                # å”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œç»Ÿè®¡æ‰€æœ‰å€¼çš„é¢‘ç‡
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
        
        column_samples[col_name] = sample_info
    
    return column_samples


def _build_column_hierarchy_tree(column_metadata: Dict[str, Dict]) -> str:
    """
    æ„å»ºåˆ—å±‚çº§ç»“æ„çš„æ ‘å½¢å±•ç¤º
    
    å‚æ•°:
        column_metadata: åˆ—å…ƒæ•°æ®å­—å…¸
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æ ‘å½¢ç»“æ„å­—ç¬¦ä¸²
    """
    if not column_metadata:
        return ""
    
    # æ„å»ºæ ‘å½¢ç»“æ„
    tree = {}
    
    for col_name, meta in column_metadata.items():
        # è·å–æ‰€æœ‰å±‚çº§
        levels = []
        level_keys = sorted([k for k in meta.keys() if k.startswith('level')], 
                          key=lambda x: int(x.replace('level', '')))
        for level_key in level_keys:
            value = meta.get(level_key)
            if value and str(value).strip():
                levels.append(str(value).strip())
        
        # å¦‚æœæ²¡æœ‰å±‚çº§ä¿¡æ¯ï¼Œä½¿ç”¨åˆ—åæœ¬èº«
        if not levels:
            levels = [col_name]
        
        # æ„å»ºæ ‘
        current = tree
        for i, level_value in enumerate(levels):
            if level_value not in current:
                current[level_value] = {}
            current = current[level_value]
    
    # é€’å½’ç”Ÿæˆæ ‘å½¢å­—ç¬¦ä¸²
    def _format_tree(node: Dict, prefix: str = "", is_last: bool = True, depth: int = 0) -> List[str]:
        lines = []
        items = list(node.items())
        
        for idx, (key, children) in enumerate(items):
            is_last_item = (idx == len(items) - 1)
            current_prefix = "â””â”€ " if is_last_item else "â”œâ”€ "
            
            if children:
                # æœ‰å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
                next_prefix = prefix + ("   " if is_last_item else "â”‚  ")
                child_lines = _format_tree(children, next_prefix, is_last_item, depth + 1)
                lines.extend(child_lines)
            else:
                # å¶å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
        
        return lines
    
    tree_lines = _format_tree(tree)
    return "\n".join(tree_lines)


def generate_analysis_prompt(
    process_result: ExcelProcessResult,
    custom_prompt: str = None,
    include_metadata: bool = True
) -> str:
    """
    æ ¹æ®Excelå¤„ç†ç»“æœç”Ÿæˆæ•°æ®åˆ†ææç¤ºè¯
    
    å‚æ•°:
        process_result: Excelå¤„ç†ç»“æœ
        custom_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        include_metadata: æ˜¯å¦åŒ…å«åˆ—ç»“æ„å…ƒæ•°æ®
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æç¤ºè¯
    """
    if not process_result.success:
        return ""
    
    # åŸºç¡€ä¿¡æ¯
    prompt_parts = []
    
    # æ·»åŠ è¯­è¨€è¦æ±‚ï¼ˆå¿…é¡»åœ¨æœ€å‰é¢ï¼‰
    prompt_parts.append("**é‡è¦è¦æ±‚ï¼šè¯·ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æå’Œå›ç­”ï¼ŒåŒ…æ‹¬ä»£ç æ³¨é‡Šã€åˆ†ææŠ¥å‘Šç­‰æ‰€æœ‰å†…å®¹ã€‚**")
    prompt_parts.append("")
    prompt_parts.append("**ç¦æ­¢è¦æ±‚ï¼šè¯·ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š**")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ matplotlibã€plotlyã€seaborn ç­‰ç»˜å›¾åº“")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ plt.figure()ã€plt.plot()ã€plt.savefig() ç­‰ç»˜å›¾å‡½æ•°")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ .plot()ã€.hist() ç­‰ pandas ç»˜å›¾æ–¹æ³•")
    prompt_parts.append("- ä¸è¦ä¿å­˜ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼ˆ.pngã€.jpgã€.svg ç­‰ï¼‰")
    prompt_parts.append("**è¯·ä¸“æ³¨äºæ•°æ®åˆ†æå’Œç»Ÿè®¡è®¡ç®—ï¼Œä¸è¦ç”Ÿæˆå¯è§†åŒ–ä»£ç ã€‚**")
    prompt_parts.append("")
    
    if custom_prompt:
        prompt_parts.append(custom_prompt)
    else:
        prompt_parts.append("è¯·å¯¹ä¸Šä¼ çš„æ•°æ®è¿›è¡Œå…¨é¢åˆ†æï¼Œç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šã€‚")
    
    # æ·»åŠ æ•°æ®æ–‡ä»¶ä¿¡æ¯ï¼ˆé‡è¦ï¼šå‘Šè¯‰AIéœ€è¦è¯»å–CSVæ–‡ä»¶ï¼‰
    if process_result.processed_file_path:
        csv_filename = os.path.basename(process_result.processed_file_path)
        prompt_parts.append(f"\n\n## æ•°æ®æ–‡ä»¶")
        prompt_parts.append(f"**é‡è¦ï¼šå·¥ä½œç©ºé—´ä¸­å·²å‡†å¤‡å¥½å¤„ç†åçš„CSVæ•°æ®æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºï¼š`{csv_filename}`**")
        prompt_parts.append(f"")
        prompt_parts.append(f"**è¯·åŠ¡å¿…ä½¿ç”¨ä»¥ä¸‹ä»£ç è¯»å–æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æï¼š**")
        prompt_parts.append(f"```python")
        prompt_parts.append(f"import pandas as pd")
        prompt_parts.append(f"")
        prompt_parts.append(f"# è¯»å–å¤„ç†åçš„CSVæ–‡ä»¶")
        prompt_parts.append(f"df = pd.read_csv('{csv_filename}')")
        prompt_parts.append(f"print(f'æ•°æ®å½¢çŠ¶: {{df.shape}}')")
        prompt_parts.append(f"print(f'åˆ—å: {{list(df.columns)}}')")
        prompt_parts.append(f"```")
        prompt_parts.append(f"")
        prompt_parts.append(f"**æ³¨æ„ï¼š**")
        prompt_parts.append(f"- CSVæ–‡ä»¶å·²ä¿å­˜åœ¨å½“å‰å·¥ä½œç©ºé—´ç›®å½•ä¸­")
        prompt_parts.append(f"- è¯·ä½¿ç”¨ `pd.read_csv('{csv_filename}')` è¯»å–æ•°æ®")
        prompt_parts.append(f"- ä¸è¦ä»…æ ¹æ®å…ƒæ•°æ®è¿›è¡Œåˆ†æï¼Œå¿…é¡»è¯»å–å®é™…æ•°æ®æ–‡ä»¶è¿›è¡Œè®¡ç®—")
        prompt_parts.append(f"")
    
    # æ·»åŠ æ•°æ®æ¦‚å†µ
    prompt_parts.append(f"\n## æ•°æ®æ¦‚å†µ")
    prompt_parts.append(f"- æ•°æ®è¡Œæ•°: {process_result.row_count}")
    prompt_parts.append(f"- åˆ—æ•°: {len(process_result.column_names)}")
    
    # æ·»åŠ è¡¨å¤´ç±»å‹ä¿¡æ¯ï¼ˆä»…ä¿ç•™å¯¹åˆ†ææœ‰ç”¨çš„ä¿¡æ¯ï¼‰
    if process_result.header_analysis:
        ha = process_result.header_analysis
        if ha.header_type == 'multi':
            prompt_parts.append(f"\n## è¡¨å¤´ç»“æ„")
            prompt_parts.append(f"- è¡¨å¤´ç±»å‹: å¤šçº§è¡¨å¤´ï¼ˆ{ha.header_rows}å±‚ï¼‰")
    
    # æ·»åŠ åˆ—ç»“æ„å…ƒæ•°æ®ï¼ˆå¸®åŠ©AIç†è§£åˆ—ä¹‹é—´çš„å…³ç³»ï¼‰
    if include_metadata and process_result.column_metadata:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šçº§ç»“æ„
        has_multi_level = any(
            len(meta) > 1 
            for meta in process_result.column_metadata.values()
        )
        
        if has_multi_level:
            prompt_parts.append(f"\n## åˆ—å±‚çº§ç»“æ„ï¼ˆå¤šçº§è¡¨å¤´è¯­ä¹‰å…³ç³»ï¼‰")
            prompt_parts.append("ä»¥ä¸‹æ ‘å½¢ç»“æ„å±•ç¤ºäº†åˆ—ä¹‹é—´çš„å±‚çº§åˆ†ç»„å…³ç³»ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„ä¸šåŠ¡å«ä¹‰ï¼š")
            prompt_parts.append("")
            hierarchy_tree = _build_column_hierarchy_tree(process_result.column_metadata)
            if hierarchy_tree:
                prompt_parts.append(hierarchy_tree)
            else:
                # å¦‚æœæ ‘å½¢æ„å»ºå¤±è´¥ï¼Œä½¿ç”¨åˆ†ç»„å±•ç¤º
                groups = defaultdict(list)
                for col_name, meta in process_result.column_metadata.items():
                    level1 = meta.get('level1', col_name)
                    groups[level1].append(col_name)
                
                for group, cols in groups.items():
                    if len(cols) > 1:
                        prompt_parts.append(f"- {group}: {', '.join(cols)}")
    
    # æ·»åŠ å®Œæ•´çš„åˆ—ååˆ—è¡¨
    prompt_parts.append(f"\n## å®Œæ•´åˆ—ååˆ—è¡¨")
    if len(process_result.column_names) <= 30:
        # å¦‚æœåˆ—æ•°ä¸å¤šï¼Œå…¨éƒ¨å±•ç¤º
        for idx, col_name in enumerate(process_result.column_names, 1):
            prompt_parts.append(f"{idx}. {col_name}")
    else:
        # å¦‚æœåˆ—æ•°å¾ˆå¤šï¼Œå±•ç¤ºå‰20ä¸ªå’Œå10ä¸ª
        for idx, col_name in enumerate(process_result.column_names[:20], 1):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"... (çœç•¥ä¸­é—´ {len(process_result.column_names) - 30} åˆ—) ...")
        for idx, col_name in enumerate(process_result.column_names[-10:], len(process_result.column_names) - 9):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"\n(å…± {len(process_result.column_names)} åˆ—)")
    
    # æ·»åŠ å­—æ®µå€¼æ ·æœ¬ä¿¡æ¯ï¼ˆä»¥JSONæ ¼å¼æä¾›ï¼Œæ›´ç»“æ„åŒ–ï¼‰
    if include_metadata and process_result.column_metadata:
        prompt_parts.append(f"\n## å­—æ®µå€¼æ ·æœ¬ï¼ˆå¸¸è§å€¼ç»Ÿè®¡ï¼‰")
        prompt_parts.append("ä»¥ä¸‹JSONæ ¼å¼å±•ç¤ºäº†æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼åŠå…¶å‡ºç°é¢‘ç‡ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„å®é™…å†…å®¹ï¼š")
        prompt_parts.append("")
        
        # æ„å»ºåŒ…å«å€¼æ ·æœ¬çš„column_metadata JSON
        column_metadata_with_samples = {}
        for col_name in process_result.column_names:
            if col_name in process_result.column_metadata:
                column_metadata_with_samples[col_name] = process_result.column_metadata[col_name]
        
        # å°†column_metadataè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
        prompt_parts.append("```json")
        prompt_parts.append(json.dumps(column_metadata_with_samples, ensure_ascii=False, indent=2))
        prompt_parts.append("```")
        prompt_parts.append("")
        
        prompt_parts.append("**è¯´æ˜ï¼š**")
        prompt_parts.append("- æ¯ä¸ªå­—æ®µçš„å…ƒæ•°æ®åŒ…å« `value_samples` å­—æ®µï¼Œå…¶ä¸­åŒ…å«è¯¥å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼")
        prompt_parts.append("- `value_samples.top_values` æ•°ç»„å±•ç¤ºäº†å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼åŠå…¶å‡ºç°æ¬¡æ•°")
        prompt_parts.append("- å¯¹äºæ•°å€¼ç±»å‹å­—æ®µï¼Œè¿˜åŒ…å« `min`ã€`max`ã€`mean`ã€`median` ç­‰ç»Ÿè®¡ä¿¡æ¯")
    
    # åœ¨æœ«å°¾å†æ¬¡å¼ºè°ƒè¦æ±‚
    prompt_parts.append("\n\n**å†æ¬¡æé†’ï¼šè¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æã€ä»£ç æ³¨é‡Šå’ŒæŠ¥å‘Šæ’°å†™ï¼Œä¸”ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ã€‚**")
    
    full_prompt = '\n'.join(prompt_parts)
    
    # æ‰“å°ç”Ÿæˆçš„æç¤ºè¯
    logger.info("=" * 80)
    logger.info("ğŸ“ ç”Ÿæˆçš„AIåˆ†ææç¤ºè¯:")
    logger.info("=" * 80)
    logger.info(full_prompt)
    logger.info("=" * 80)
    
    return full_prompt

