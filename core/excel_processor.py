"""
Excelæ™ºèƒ½å¤„ç†æ¨¡å—
æ”¯æŒï¼š
1. è‡ªåŠ¨è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ³¨é‡Šã€æ ‡é¢˜ç­‰ï¼‰
2. å•è¡¨å¤´/å¤šè¡¨å¤´è‡ªåŠ¨è¯†åˆ«
3. å¯é€‰è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
4. åˆå¹¶å•å…ƒæ ¼å¤„ç†
5. åˆ—ç»“æ„å…ƒæ•°æ®ç”Ÿæˆ
"""

import pandas as pd
import json
import re
import os
import sys
import requests
import logging
import tempfile
import shutil
import time
import zipfile
from openpyxl import load_workbook
from typing import Tuple, List, Dict, Optional, Any
from collections import defaultdict
from dataclasses import dataclass, asdict, field
from pathlib import Path

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥é…ç½®ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨å»¶è¿Ÿå¯¼å…¥ï¼‰

from .config import EXCEL_LLM_API_KEY, EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL



@dataclass
class HeaderAnalysis:
    """è¡¨å¤´åˆ†æç»“æœ"""
    skip_rows: int          # éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œæ•°
    header_rows: int        # è¡¨å¤´å ç”¨çš„è¡Œæ•°
    header_type: str        # 'single' æˆ– 'multi'
    data_start_row: int     # æ•°æ®å¼€å§‹è¡Œï¼ˆ1-indexedï¼‰
    confidence: str         # ç½®ä¿¡åº¦: high/medium/low
    reason: str             # åˆ†æåŸå› è¯´æ˜
    start_col: int = 1      # æ•°æ®èµ·å§‹åˆ—ï¼ˆ1-indexedï¼‰ï¼Œç¬¬ä¸€ä¸ªè¡¨å¤´è¡Œä¸­ç¬¬ä¸€ä¸ªéç©ºè¡¨å¤´å¼€å§‹çš„åˆ—
    valid_cols: Optional[List[int]] = None  # æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        if result.get('valid_cols') is None:
            result['valid_cols'] = None
        return result


@dataclass
class ExcelProcessResult:
    """Excelå¤„ç†ç»“æœ"""
    success: bool
    header_analysis: Optional[HeaderAnalysis]
    processed_file_path: Optional[str]      # å¤„ç†åçš„CSVæ–‡ä»¶è·¯å¾„
    metadata_file_path: Optional[str]       # å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„
    column_names: List[str]                 # åˆ—ååˆ—è¡¨
    column_metadata: Dict[str, Dict]        # åˆ—ç»“æ„å…ƒæ•°æ®
    row_count: int                          # æ•°æ®è¡Œæ•°
    error_message: Optional[str]            # é”™è¯¯ä¿¡æ¯
    llm_analysis_response: Optional[str] = None  # LLMåˆ†æåŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "success": self.success,
            "header_analysis": self.header_analysis.to_dict() if self.header_analysis else None,
            "processed_file_path": self.processed_file_path,
            "metadata_file_path": self.metadata_file_path,
            "column_names": self.column_names,
            "column_metadata": self.column_metadata,
            "row_count": self.row_count,
            "error_message": self.error_message
        }


class SmartHeaderProcessor:
    """æ™ºèƒ½è¡¨å¤´å¤„ç†å™¨"""
    
    def __init__(self, filepath: str, sheet_name: str = None, load_timeout: int = 60, read_timeout: int = 10, debug_print_header_analysis: bool = False, max_file_size_mb: Optional[int] = None, max_rows: Optional[int] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½è¡¨å¤´å¤„ç†å™¨
        
        å‚æ•°:
            filepath: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
            load_timeout: åŠ è½½Excelæ–‡ä»¶çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’
            read_timeout: è¯»å–Excelæ•°æ®çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
            debug_print_header_analysis: æ˜¯å¦æµå¼æ‰“å°åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰ï¼Œé»˜è®¤False
            max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        self.filepath = filepath
        self.sheet_name = sheet_name
        self.file_ext = Path(filepath).suffix.lower()
        self._temp_xlsx_path = None  # ç”¨äºå­˜å‚¨ä¸´æ—¶è½¬æ¢çš„ .xlsx æ–‡ä»¶è·¯å¾„
        self.read_timeout = read_timeout  # è¯»å–æ•°æ®çš„è¶…æ—¶æ—¶é—´
        self.debug_print_header_analysis = debug_print_header_analysis  # æ˜¯å¦æµå¼æ‰“å°åŸå§‹æ•°æ®
        
        # æ–‡ä»¶é¢„æ£€æŸ¥ï¼ˆåœ¨åŠ è½½ä¹‹å‰ï¼‰
        logger.info(f"ğŸ” [DEBUG] SmartHeaderProcessor.__init__: å¼€å§‹æ–‡ä»¶é¢„æ£€æŸ¥")
        try:
            # åŸºç¡€æ£€æŸ¥ï¼ˆæ–‡ä»¶å­˜åœ¨ã€å¤§å°ã€å¯è¯»ï¼‰
            _validate_excel_file_basic(filepath, max_file_size_mb=max_file_size_mb)
            logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: åŸºç¡€æ£€æŸ¥é€šè¿‡")
            
            # å¦‚æœæ˜¯ .xlsx æ ¼å¼ï¼ŒéªŒè¯ZIPæ ¼å¼
            if self.file_ext == '.xlsx':
                _validate_xlsx_format(filepath, timeout=0.5)
                logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: ZIPæ ¼å¼éªŒè¯é€šè¿‡")
                
                # Excelç»“æ„éªŒè¯ï¼ˆå¯é€‰ï¼Œè¾ƒæ…¢ï¼‰
                _validate_excel_structure(filepath, timeout=2.0)
                logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: Excelç»“æ„éªŒè¯é€šè¿‡")
                
                # è¡Œæ•°æ£€æŸ¥ï¼ˆè¶…è¿‡é…ç½®çš„æœ€å¤§è¡Œæ•°ç›´æ¥æ‹’ç»ï¼‰
                max_rows_value = max_rows if max_rows is not None else 10000
                _validate_excel_row_count(filepath, sheet_name=sheet_name, max_rows=max_rows_value, timeout=5.0)
                logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: è¡Œæ•°æ£€æŸ¥é€šè¿‡ï¼ˆé™åˆ¶: {max_rows_value} è¡Œï¼‰")
        except Exception as e:
            logger.error(f"âŒ [DEBUG] SmartHeaderProcessor.__init__: æ–‡ä»¶é¢„æ£€æŸ¥å¤±è´¥: {e}")
            raise
        
        # å¦‚æœæ˜¯ .xls æ ¼å¼ï¼Œå…ˆè½¬æ¢ä¸º .xlsxï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        if self.file_ext == '.xls':
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ° .xls æ ¼å¼æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸º .xlsx...")
            self._temp_xlsx_path = self._convert_xls_to_xlsx(filepath, timeout=load_timeout)
            actual_filepath = self._temp_xlsx_path
            logger.info(f"âœ… è½¬æ¢å®Œæˆ: {self._temp_xlsx_path}")
        else:
            actual_filepath = filepath
        
        # ç»Ÿä¸€ä½¿ç”¨ openpyxl è¯»å–ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ read_only æ¨¡å¼ï¼Œå› ä¸ºéœ€è¦è®¿é—® merged_cells å±æ€§æ¥å¤„ç†åˆå¹¶å•å…ƒæ ¼
        logger.info(f"â³ [DEBUG] SmartHeaderProcessor.__init__: å¼€å§‹åŠ è½½å·¥ä½œç°¿ï¼Œè¶…æ—¶: {load_timeout}ç§’")
        self.wb = self._load_workbook_with_timeout(actual_filepath, timeout=load_timeout)
        logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: å·¥ä½œç°¿åŠ è½½å®Œæˆ")
        # ä¿®å¤ï¼šæ˜ç¡®ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼Œè€Œä¸æ˜¯ä¾èµ– wb.activeï¼ˆactiveå¯èƒ½æ˜¯ç”¨æˆ·æœ€åæŸ¥çœ‹çš„å·¥ä½œè¡¨ï¼‰
        logger.info(f"â³ [DEBUG] SmartHeaderProcessor.__init__: å¼€å§‹é€‰æ‹©å·¥ä½œè¡¨")
        if sheet_name:
            self.ws = self.wb[sheet_name]
        else:
            # æ˜ç¡®ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼ˆç´¢å¼•0ï¼‰ï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´
            if not self.wb.sheetnames:
                raise ValueError("Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨")
            self.ws = self.wb[self.wb.sheetnames[0]]
        logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: å·¥ä½œè¡¨é€‰æ‹©å®Œæˆ")
        # æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        logger.info(f"â³ [DEBUG] SmartHeaderProcessor.__init__: å¼€å§‹æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„ï¼Œè¶…æ—¶: {load_timeout}ç§’")
        self.merged_cells_map = self._build_merged_cells_map_with_timeout(timeout=load_timeout)
        logger.info(f"âœ… [DEBUG] SmartHeaderProcessor.__init__: åˆå¹¶å•å…ƒæ ¼æ˜ å°„æ„å»ºå®Œæˆ")
    
    def _load_workbook_with_timeout(self, filepath: str, timeout: int = 60):
        """å¸¦è¶…æ—¶ä¿æŠ¤çš„ load_workbook"""
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
        
        def _load():
            """åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½å·¥ä½œç°¿"""
            try:
                # äºŒæ¬¡éªŒè¯ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼Œå› ä¸ºå¯èƒ½å·²ç»åœ¨ __init__ ä¸­æ£€æŸ¥è¿‡ï¼‰
                # åªåšåŸºç¡€æ£€æŸ¥ï¼Œä¸åšZIPéªŒè¯ï¼ˆé¿å…é‡å¤ï¼‰
                file_ext = Path(filepath).suffix.lower()
                if file_ext == '.xlsx':
                    # å¿«é€ŸZIPå¤´éªŒè¯
                    with open(filepath, 'rb') as f:
                        header = f.read(4)
                        if header != b'PK\x03\x04':
                            raise ValueError(f"ä¸æ˜¯æœ‰æ•ˆçš„Excelæ–‡ä»¶ï¼ˆZIPæ ¼å¼é”™è¯¯ï¼‰: {filepath}")
                
                return load_workbook(filepath, data_only=True)
            except Exception as e:
                logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {e}")
                raise
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_load)
                try:
                    wb = future.result(timeout=timeout)
                    return wb
                except FutureTimeoutError:
                    logger.error(f"åŠ è½½Excelæ–‡ä»¶è¶…æ—¶: {filepath} (è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
                    future.cancel()
                    raise TimeoutError(f"åŠ è½½Excelæ–‡ä»¶è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰: {filepath}")
        except Exception as e:
            if isinstance(e, TimeoutError):
                raise
            logger.error(f"åŠ è½½Excelæ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {filepath}, é”™è¯¯: {e}")
            raise
    
    def _convert_xls_to_xlsx(self, xls_path: str, timeout: int = 60) -> str:
        """
        å°† .xls æ–‡ä»¶è½¬æ¢ä¸º .xlsx æ ¼å¼ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        
        å‚æ•°:
            xls_path: .xls æ–‡ä»¶è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’
        
        è¿”å›:
            ä¸´æ—¶ .xlsx æ–‡ä»¶è·¯å¾„
        """
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
        
        def _convert():
            """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè½¬æ¢"""
            try:
                # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
                excel_file = pd.ExcelFile(xls_path, engine='xlrd')
                
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                temp_dir = os.path.dirname(xls_path)
                temp_xlsx_path = os.path.join(
                    temp_dir, 
                    f"{Path(xls_path).stem}_converted_{os.getpid()}.xlsx"
                )
                
                # ä½¿ç”¨ ExcelWriter å†™å…¥æ‰€æœ‰å·¥ä½œè¡¨
                with pd.ExcelWriter(temp_xlsx_path, engine='openpyxl') as writer:
                    for sheet_name in excel_file.sheet_names:
                        df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='xlrd')
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                logger.info(f"âœ… .xls æ–‡ä»¶å·²è½¬æ¢ä¸º .xlsx: {temp_xlsx_path}")
                return temp_xlsx_path
            except Exception as e:
                logger.error(f"âŒ è½¬æ¢ .xls æ–‡ä»¶å¤±è´¥: {e}")
                raise ValueError(
                    f"æ— æ³•è½¬æ¢ .xls æ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²å®‰è£… xlrd åº“: pip install xlrdã€‚é”™è¯¯: {str(e)}"
                )
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_convert)
                try:
                    result = future.result(timeout=timeout)
                    return result
                except FutureTimeoutError:
                    logger.error(f"è½¬æ¢ .xls æ–‡ä»¶è¶…æ—¶: {xls_path} (è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
                    future.cancel()
                    raise TimeoutError(f"è½¬æ¢ .xls æ–‡ä»¶è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰: {xls_path}")
        except Exception as e:
            if isinstance(e, TimeoutError):
                raise
            logger.error(f"è½¬æ¢ .xls æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {xls_path}, é”™è¯¯: {e}")
            raise
    
    def _build_merged_cells_map(self) -> Dict[Tuple[int, int], str]:
        """æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¸å¸¦è¶…æ—¶ï¼‰"""
        merged_map = {}
        try:
            for merged_range in self.ws.merged_cells.ranges:
                min_row, min_col = merged_range.min_row, merged_range.min_col
                value = self.ws.cell(min_row, min_col).value
                for row in range(merged_range.min_row, merged_range.max_row + 1):
                    for col in range(merged_range.min_col, merged_range.max_col + 1):
                        merged_map[(row, col)] = value
        except Exception as e:
            # å¦‚æœæ— æ³•è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›ç©ºå­—å…¸
            logger.warning(f"âš ï¸ æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„æ—¶å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨ç©ºæ˜ å°„")
        
        return merged_map
    
    def _build_merged_cells_map_with_timeout(self, timeout: int = 10) -> Dict[Tuple[int, int], str]:
        """å¸¦è¶…æ—¶ä¿æŠ¤çš„æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„"""
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
        
        def _build():
            """åœ¨åå°çº¿ç¨‹ä¸­æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„"""
            try:
                return self._build_merged_cells_map()
            except Exception as e:
                logger.error(f"æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„å¤±è´¥: {e}")
                raise
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_build)
                try:
                    merged_map = future.result(timeout=timeout)
                    return merged_map
                except FutureTimeoutError:
                    logger.error(f"æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„è¶…æ—¶: {self.filepath} (è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
                    future.cancel()
                    # è¶…æ—¶æ—¶è¿”å›ç©ºå­—å…¸ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“åç»­å¤„ç†
                    logger.warning(f"âš ï¸ æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„è¶…æ—¶ï¼Œå°†ä½¿ç”¨ç©ºæ˜ å°„")
                    return {}
        except Exception as e:
            # å¦‚æœå‘ç”Ÿå…¶ä»–å¼‚å¸¸ï¼Œä¹Ÿè¿”å›ç©ºå­—å…¸
            logger.warning(f"âš ï¸ æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„æ—¶å‘ç”Ÿå¼‚å¸¸: {e}ï¼Œå°†ä½¿ç”¨ç©ºæ˜ å°„")
            return {}
    
    def get_cell_value(self, row: int, col: int) -> Any:
        """è·å–å•å…ƒæ ¼å€¼ï¼Œå¤„ç†åˆå¹¶å•å…ƒæ ¼"""
        if (row, col) in self.merged_cells_map:
            return self.merged_cells_map[(row, col)]
        return self.ws.cell(row, col).value
    
    def get_preview_data(self, max_rows: int = 15, max_cols: int = 25) -> List[List[Any]]:
        """
        è·å–é¢„è§ˆæ•°æ®ç”¨äºåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        ç›´æ¥è¯»å–åŸå§‹æ•°æ®ï¼Œä¸åšä»»ä½•å¤„ç†ï¼ˆåŒ…æ‹¬åˆå¹¶å•å…ƒæ ¼å¤„ç†ï¼‰
        å¸¦è¶…æ—¶ä¿æŠ¤
        """
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
        
        def _read_data():
            """åœ¨åå°çº¿ç¨‹ä¸­è¯»å–æ•°æ®"""
            actual_max_col = min(self.ws.max_column, max_cols)
            actual_max_row = min(self.ws.max_row, max_rows)
            
            data = []
            for row in range(1, actual_max_row + 1):
                row_data = []
                for col in range(1, actual_max_col + 1):
                    # ç›´æ¥è¯»å–åŸå§‹å€¼ï¼Œä¸åšä»»ä½•å¤„ç†
                    value = self.ws.cell(row, col).value
                    row_data.append(value)
                data.append(row_data)
            return data
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_read_data)
                try:
                    data = future.result(timeout=self.read_timeout)
                    
                    # å¦‚æœå¯ç”¨è°ƒè¯•æ‰“å°ï¼Œæµå¼æ‰“å°åŸå§‹æ•°æ®
                    if self.debug_print_header_analysis:
                        print("=" * 80)
                        print("ã€åŸå§‹Excelæ•°æ® - æµå¼æ‰“å°ã€‘ï¼ˆå‰15è¡Œï¼Œå‰25åˆ—ï¼‰")
                        print("=" * 80)
                        sys.stdout.flush()
                        
                        for i, row in enumerate(data, 1):
                            print(f"è¡Œ{i}: {row}")
                            sys.stdout.flush()
                        
                        print("=" * 80)
                        sys.stdout.flush()
                    
                    return data
                except FutureTimeoutError:
                    logger.error(f"è¯»å–Excelæ•°æ®è¶…æ—¶: {self.filepath} (è¶…æ—¶æ—¶é—´: {self.read_timeout}ç§’)")
                    future.cancel()
                    raise TimeoutError(f"è¯»å–Excelæ•°æ®è¶…æ—¶ï¼ˆ{self.read_timeout}ç§’ï¼‰: {self.filepath}")
        except Exception as e:
            if isinstance(e, TimeoutError):
                raise
            logger.error(f"è¯»å–Excelæ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {self.filepath}, é”™è¯¯: {e}")
            raise
    
    
    def analyze_with_llm(self, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         timeout: Optional[int] = None,
                         thinking_callback: Optional[callable] = None) -> Tuple[HeaderAnalysis, str]:
        """
        ä½¿ç”¨LLMç›´æ¥åˆ†æExcelè¡¨æ ¼çš„è¡Œå’Œåˆ—ç»“æ„ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        å‚æ•°:
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
            thinking_callback: ç”¨äºæµå¼è¾“å‡º thinking å†…å®¹çš„å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            (åˆ†æç»“æœ, LLMåŸå§‹å“åº”)ï¼ˆå¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼‰
        """
        # ç›´æ¥è¯»å–å‰15è¡Œã€25åˆ—çš„åŸå§‹æ•°æ®ï¼Œä¸åšä»»ä½•å¤„ç†
        preview_data = self.get_preview_data(max_rows=15, max_cols=25)
        max_col = self.ws.max_column
        
        # æ„å»ºç®€åŒ–çš„åˆ†ææç¤ºè¯
        prompt = self._build_llm_analysis_prompt(preview_data, max_col)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model, timeout=timeout, thinking_callback=thinking_callback)
        
        if not result:
            raise ValueError("LLMåˆ†æå¤±è´¥ï¼šæ— æ³•è·å–LLMå“åº”ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        
        # è§£æLLMåˆ†æç»“æœ
        analysis = self._parse_llm_analysis_response(result)
        
        return analysis, result
    
    def validate_with_llm(self, rule_analysis: HeaderAnalysis, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         timeout: Optional[int] = None) -> HeaderAnalysis:
        """
        ä½¿ç”¨LLMéªŒè¯è§„åˆ™åˆ†æçš„ç»“æœï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
        
        å‚æ•°:
            rule_analysis: è§„åˆ™åˆ†æçš„ç»“æœ
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        
        è¿”å›:
            éªŒè¯åçš„åˆ†æç»“æœï¼ˆå¦‚æœLLMéªŒè¯å¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœï¼‰
        """
        preview_data = self.get_preview_data()
        
        # æ„å»ºéªŒè¯æç¤ºè¯
        prompt = self._build_validation_prompt(preview_data, rule_analysis)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model, timeout=timeout)
        
        # è§£æLLMéªŒè¯ç»“æœ
        validated = self._parse_validation_response(result, rule_analysis)
        
        return validated
    
    def _build_llm_analysis_prompt(self, preview_data: List[List], 
                                   max_col: int) -> str:
        """
        æ„å»ºLLMåˆ†ææç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        ç›´æ¥è¯»å–åŸå§‹æ•°æ®ï¼Œä¸åšä»»ä½•å¤„ç†ï¼Œè®©LLMç›´æ¥è¯†åˆ«
        """
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºç®€å•çš„è¡¨æ ¼å½¢å¼
        num_cols = len(preview_data[0]) if preview_data else 0
        num_rows = len(preview_data)
        
        # æ„å»ºç®€å•çš„è¡¨æ ¼å­—ç¬¦ä¸²
        table_str = "ã€ExcelåŸå§‹æ•°æ®ã€‘ï¼ˆå‰15è¡Œï¼Œå‰25åˆ—ï¼‰\n\n"
        table_str += "è¡Œå· | " + " | ".join([f"åˆ—{i+1}" for i in range(num_cols)]) + "\n"
        table_str += "-" * (8 + num_cols * 15) + "\n"
        
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join([str(cell) if cell is not None else "" for cell in row])
            table_str += f"  {i:2d}  | {row_str}\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªExcelè¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹Excelè¡¨æ ¼çš„åŸå§‹æ•°æ®ï¼Œè¯†åˆ«è¡¨å¤´ç»“æ„ã€‚

{table_str}

ã€æ€»åˆ—æ•°ã€‘{max_col}

## åˆ†æä»»åŠ¡

è¯·åˆ†æè¡¨æ ¼ç»“æ„ï¼Œè¯†åˆ«ï¼š
1. **æ— æ•ˆè¡Œï¼ˆskip_rowsï¼‰**ï¼šè¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œï¼ˆå¦‚æ–‡æ¡£æ ‡é¢˜ã€è¯´æ˜æ–‡å­—ã€æ³¨é‡Šã€å…¬å¸åç§°ã€å¡«æŠ¥è¯´æ˜ç­‰ï¼‰
2. **è¡¨å¤´è¡Œæ•°ï¼ˆheader_rowsï¼‰**ï¼šæ‰€æœ‰è¡¨å¤´è¡Œï¼ŒåŒ…æ‹¬å¤šçº§è¡¨å¤´çš„æ‰€æœ‰å±‚çº§
3. **è¡¨å¤´ç±»å‹ï¼ˆheader_typeï¼‰**ï¼šsingleï¼ˆå•è¡¨å¤´ï¼‰æˆ– multiï¼ˆå¤šçº§è¡¨å¤´ï¼‰
4. **æ•°æ®èµ·å§‹è¡Œï¼ˆdata_start_rowï¼‰**ï¼šæ•°æ®å¼€å§‹çš„è¡Œå·ï¼Œå¿…é¡»ç­‰äº skip_rows + header_rows + 1
5. **æ•°æ®èµ·å§‹åˆ—ï¼ˆstart_colï¼‰**ï¼šç¬¬ä¸€ä¸ªè¡¨å¤´è¡Œä¸­ç¬¬ä¸€ä¸ªéç©ºè¡¨å¤´å¼€å§‹çš„åˆ—å·

## è¯†åˆ«è§„åˆ™

### æ— æ•ˆè¡Œç‰¹å¾ï¼š
- æ–‡æ¡£æ ‡é¢˜ï¼ˆå¦‚"2024å¹´åº¦æŠ¥è¡¨"ï¼‰
- å…¬å¸åç§°æˆ–éƒ¨é—¨åç§°ï¼ˆå¦‚"XXå…¬å¸"ã€"XXéƒ¨é—¨"ï¼‰
- å¡«æŠ¥è¯´æ˜ï¼ˆå¦‚"å¡«æŠ¥æœºæ„"ã€"å¡«æŠ¥æ—¥æœŸ"ã€"å¡«æŠ¥æœºæ„/æ—¥æœŸ"ç­‰ï¼Œä»»ä½•åŒ…å«"å¡«æŠ¥"å…³é”®è¯çš„è¡Œï¼‰
- åªæœ‰æ•°å­—æ²¡æœ‰æ ‡ç­¾çš„è¡Œï¼ˆå¦‚åªæœ‰"222"ã€"111"ç­‰æ•°å­—ï¼Œæ²¡æœ‰å¯¹åº”çš„åˆ—åï¼‰
- å®Œå…¨ç©ºè¡Œæˆ–åªæœ‰å°‘é‡æ–‡æœ¬çš„è¡Œ

### è¡¨å¤´è¡Œç‰¹å¾ï¼š
- åŒ…å«åˆ—åæˆ–åˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚"é”€å”®äº‹ä¸šéƒ¨"ã€"åä¸œå¤§åŒº"ã€"çº¿ä¸Šé”€å”®é¢"ç­‰ï¼‰
- æœ‰æ˜ç¡®çš„å±‚çº§ç»“æ„ï¼ˆå¤šçº§è¡¨å¤´ï¼‰
- é€šå¸¸ä¸åŒ…å«å¤§é‡æ•°å€¼æ•°æ®

### æ•°æ®è¡Œç‰¹å¾ï¼š
- åŒ…å«å¤§é‡æ•°å€¼æ•°æ®
- ä¸å†æ˜¯è¡¨å¤´æ–‡æœ¬æˆ–åˆ†ç±»æ ‡ç­¾

## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

```json
{{
    "skip_rows": <è¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œæ•°ï¼Œå¦‚æœç¬¬1è¡Œå°±æ˜¯è¡¨å¤´åˆ™å¡«0>,
    "header_rows": <è¡¨å¤´å ç”¨çš„æ€»è¡Œæ•°>,
    "header_type": "<singleæˆ–multi>",
    "data_start_row": <æ•°æ®å¼€å§‹è¡Œå·ï¼ˆ1-indexedï¼‰ï¼Œå¿…é¡»ç­‰äºskip_rows+header_rows+1>,
    "start_col": <æ•°æ®èµ·å§‹åˆ—å·ï¼ˆ1-indexedï¼‰>,
    "valid_cols": null,
    "confidence": "<high/medium/low>",
    "reason": "<è¯¦ç»†è¯´æ˜è¯†åˆ«è¿‡ç¨‹>"
}}
```

## æ³¨æ„äº‹é¡¹

1. è¡Œå·å’Œåˆ—å·éƒ½ä»1å¼€å§‹è®¡æ•°
2. data_start_row å¿…é¡»ç­‰äº skip_rows + header_rows + 1
3. valid_cols å§‹ç»ˆè®¾ä¸º null
4. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹
5. å¦‚æœç¬¬1è¡Œå°±æ˜¯è¡¨å¤´ï¼Œåˆ™ skip_rows=0
6. å¤šçº§è¡¨å¤´çš„æ‰€æœ‰è¡Œéƒ½è¦è®¡å…¥ header_rows

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        return prompt
    
    def _build_validation_prompt(self, preview_data: List[List], 
                                rule_analysis: HeaderAnalysis) -> str:
        """
        æ„å»ºLLMéªŒè¯æç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        """
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºç®€å•çš„è¡¨æ ¼å½¢å¼
        num_cols = len(preview_data[0]) if preview_data else 0
        
        table_str = "ã€ExcelåŸå§‹æ•°æ®ã€‘ï¼ˆå‰15è¡Œï¼Œå‰25åˆ—ï¼‰\n\n"
        table_str += "è¡Œå· | " + " | ".join([f"åˆ—{i+1}" for i in range(num_cols)]) + "\n"
        table_str += "-" * (8 + num_cols * 15) + "\n"
        
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join([str(cell) if cell is not None else "" for cell in row])
            table_str += f"  {i:2d}  | {row_str}\n"
        
        prompt = f"""è¯·éªŒè¯ä»¥ä¸‹Excelè¡¨æ ¼çš„è¡¨å¤´åˆ†æç»“æœæ˜¯å¦æ­£ç¡®ã€‚

{table_str}

ã€å½“å‰åˆ†æç»“æœã€‘
- è·³è¿‡è¡Œæ•°(skip_rows): {rule_analysis.skip_rows}
- è¡¨å¤´è¡Œæ•°(header_rows): {rule_analysis.header_rows}
- è¡¨å¤´ç±»å‹: {rule_analysis.header_type}
- æ•°æ®èµ·å§‹è¡Œ: {rule_analysis.data_start_row} ï¼ˆåº”è¯¥ç­‰äº skip_rows + header_rows + 1ï¼‰
- åˆ†æåŸå› : {rule_analysis.reason}

è¯·éªŒè¯è¿™ä¸ªç»“æœæ˜¯å¦åˆç†ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "is_valid": <trueæˆ–false>,
    "confidence": "<high/medium/low>",
    "suggestions": {{
        "skip_rows": <å»ºè®®çš„è·³è¿‡è¡Œæ•°>,
        "header_rows": <å»ºè®®çš„è¡¨å¤´è¡Œæ•°>,
        "header_type": "<singleæˆ–multi>",
        "data_start_row": <å»ºè®®çš„æ•°æ®èµ·å§‹è¡Œ>
    }},
    "reason": "<éªŒè¯è¯´æ˜>"
}}

## éªŒè¯è¦ç‚¹

1. skip_rows åªè®¡ç®—è¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œï¼ˆå¦‚æ–‡æ¡£æ ‡é¢˜ã€æ³¨é‡Šç­‰ï¼‰ï¼Œä¸è¦æŠŠè¡¨å¤´è¡Œç®—ä½œskip_rows
2. header_rows åº”è¯¥åŒ…å«æ‰€æœ‰è¡¨å¤´è¡Œï¼ŒåŒ…æ‹¬å¤šçº§è¡¨å¤´çš„æ‰€æœ‰è¡Œ
3. data_start_row å¿…é¡»ç­‰äº skip_rows + header_rows + 1

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        return prompt
    
    def _call_llm(self, prompt: str, llm_api_key: Optional[str] = None, 
                  llm_base_url: Optional[str] = None, llm_model: Optional[str] = None,
                  timeout: Optional[int] = None, thinking_callback: Optional[callable] = None) -> str:
        """è°ƒç”¨LLM APIï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰
        
        å‚æ•°:
            prompt: æç¤ºè¯
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
            thinking_callback: ç”¨äºæµå¼è¾“å‡º thinking å†…å®¹çš„å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        """
        # æä¾›é»˜è®¤å›è°ƒå‡½æ•°ï¼Œç¡®ä¿ thinking å†…å®¹æ€»æ˜¯è¢«æ¨é€ï¼ˆä¸æ£€æŸ¥æ¡ä»¶ï¼‰
        if thinking_callback is None:
            # é»˜è®¤å›è°ƒå‡½æ•°ï¼šåªè¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆä¸æ¨é€åˆ°æ’ä»¶ï¼‰
            def default_callback(content: str):
                pass  # ç©ºå›è°ƒï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
            thinking_callback = default_callback
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»é…ç½®è¯»å–
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        base_url = llm_base_url if llm_base_url is not None else EXCEL_LLM_BASE_URL
        model = llm_model if llm_model is not None else EXCEL_LLM_MODEL
        
        logger.info("=" * 60)
        logger.info("ğŸ¤– è°ƒç”¨ LLM API è¿›è¡ŒExcelè¡¨æ ¼åˆ†æ")
        logger.info(f"ğŸ”— EXCEL_LLM_BASE_URL: {base_url}")
        logger.info(f"ğŸ“Œ æ¨¡å‹: {model}")
        logger.info(f"ğŸ”‘ API Key: {'å·²é…ç½®' if api_key else 'æœªé…ç½®'}")
        logger.info("ğŸ’­ Thinking æµå¼è¾“å‡º: å·²å¯ç”¨ï¼ˆé»˜è®¤å¼€å¯ï¼‰")
        
        if not api_key:
            logger.error("âŒ æœªé…ç½® LLM API Keyï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return None
            
        url = base_url
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # ä½¿ç”¨æµå¼è°ƒç”¨ä»¥æ”¯æŒ thinking åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        base_payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.4,
            "max_tokens": 1000,  # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒæ›´è¯¦ç»†çš„åˆ†æ
            "stream": True,  # æµå¼è°ƒç”¨ï¼ˆå¿…é¡»å¯ç”¨ä»¥æ”¯æŒ thinkingï¼‰
        }
        
        # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤90ç§’
        request_timeout = timeout if timeout is not None else 90
        
        logger.info(f"ğŸ“¡ å‘é€ LLM API è¯·æ±‚åˆ°: {url} (æµå¼è°ƒç”¨)")
        logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.info(f"â±ï¸ è¶…æ—¶è®¾ç½®: {request_timeout} ç§’")
        
        try:
            # é»˜è®¤å¯ç”¨ thinking åŠŸèƒ½ï¼ˆæµå¼è¾“å‡ºï¼‰
            payload_with_thinking = base_payload.copy()
            payload_with_thinking["enable_thinking"] = True  # é»˜è®¤å¯ç”¨ thinking
            
            logger.info("ğŸ’­ å·²å¯ç”¨ Thinking åŠŸèƒ½ï¼Œå°†å®æ—¶æµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹")
            logger.debug(f"ğŸ“¦ è¯·æ±‚ payload (å¯ç”¨ thinking): {json.dumps(payload_with_thinking, ensure_ascii=False, indent=2)}")
            
            response = requests.post(
                url, 
                headers=headers, 
                json=payload_with_thinking, 
                timeout=request_timeout,
                stream=True  # å¯ç”¨æµå¼å“åº”
            )
            
            # å¦‚æœå¯ç”¨ thinking å¤±è´¥ï¼Œå›é€€åˆ°ä¸ä½¿ç”¨ thinking
            if response.status_code != 200:
                try:
                    error_json = response.json()
                    if "enable_thinking" in str(error_json).lower():
                        logger.warning("âš ï¸ API ä¸æ”¯æŒ enable_thinking å‚æ•°ï¼Œå°†å›é€€åˆ°ä¸ä½¿ç”¨ thinking")
                        logger.warning("ğŸ’­ æ³¨æ„ï¼šThinking æµå¼è¾“å‡ºå°†ä¸å¯ç”¨ï¼ˆAPI ä¸æ”¯æŒï¼‰")
                        payload_no_thinking = base_payload.copy()
                        logger.debug(f"ğŸ“¦ è¯·æ±‚ payload (ä¸ä½¿ç”¨ thinking): {json.dumps(payload_no_thinking, ensure_ascii=False, indent=2)}")
                        response = requests.post(
                            url, 
                            headers=headers, 
                            json=payload_no_thinking, 
                            timeout=request_timeout,
                            stream=True
                        )
                except:
                    pass
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¾“å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if response.status_code != 200:
                error_detail = ""
                try:
                    # å¯¹äºæµå¼å“åº”ï¼Œå°è¯•è¯»å–é”™è¯¯ä¿¡æ¯
                    error_text = ""
                    for line in response.iter_lines():
                        if line:
                            line_str = line.decode('utf-8')
                            if line_str.startswith('data: '):
                                line_str = line_str[6:]
                            try:
                                error_json = json.loads(line_str)
                                error_detail = json.dumps(error_json, ensure_ascii=False, indent=2)
                                break
                            except:
                                error_text += line_str + "\n"
                    if not error_detail:
                        error_detail = error_text or response.text
                except:
                    try:
                        error_detail = response.text
                    except:
                        error_detail = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ… (çŠ¶æ€ç : {response.status_code})"
                
                logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                logger.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…:\n{error_detail}")
                logger.error(f"ğŸ”— è¯·æ±‚ URL: {url}")
                logger.error(f"ğŸ“¦ è¯·æ±‚ payload: {json.dumps(base_payload, ensure_ascii=False, indent=2)}")
                return None
            
            # å¤„ç†æµå¼å“åº”
            full_content = ""
            full_thinking = ""  # ä¿å­˜å®Œæ•´çš„ thinking å†…å®¹
            thinking_started = False  # æ ‡è®°æ˜¯å¦å·²ç»å¼€å§‹è¾“å‡º thinking
            
            logger.info("=" * 60)
            logger.info("ğŸ§  å¼€å§‹æ¥æ”¶ LLM æµå¼å“åº”ï¼ˆåŒ…å« thinking è¿‡ç¨‹ï¼‰")
            logger.info("ğŸ’­ Thinking æµå¼è¾“å‡ºå·²å¯ç”¨ï¼Œå°†å®æ—¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹")
            logger.info("=" * 60)
            
            # ç”¨äºè°ƒè¯•ï¼šè®°å½•ç¬¬ä¸€ä¸ª chunk çš„å®Œæ•´ç»“æ„
            first_chunk_logged = False
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    # è·³è¿‡ SSE æ ¼å¼çš„å‰ç¼€ "data: "
                    if line_str.startswith('data: '):
                        line_str = line_str[6:]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                    if line_str.strip() == '[DONE]':
                        break
                    
                    # è§£æ JSON
                    try:
                        chunk_data = json.loads(line_str)
                        
                        # è°ƒè¯•ï¼šè¾“å‡ºç¬¬ä¸€ä¸ª chunk çš„å®Œæ•´ç»“æ„ï¼ˆå¸®åŠ©äº†è§£ API å“åº”æ ¼å¼ï¼‰
                        if not first_chunk_logged:
                            logger.info("=" * 60)
                            logger.info("ğŸ” ç¬¬ä¸€ä¸ª Chunk å®Œæ•´ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰:")
                            logger.info("=" * 60)
                            logger.info(json.dumps(chunk_data, ensure_ascii=False, indent=2))
                            logger.info("=" * 60)
                            first_chunk_logged = True
                        
                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                            choice = chunk_data['choices'][0]
                            delta = choice.get('delta', {})
                            finish_reason = choice.get('finish_reason')
                            
                            # æ£€æµ‹æ˜¯å¦æœ‰ thinking ç›¸å…³å­—æ®µï¼ˆå³ä½¿å†…å®¹ä¸ºç©ºä¹Ÿè¦æ£€æµ‹ï¼‰
                            has_thinking_field = False
                            thinking_content = None
                            
                            # æ–¹å¼1: delta.reasoning_contentï¼ˆQwen ç­‰æ¨¡å‹ä½¿ç”¨ï¼‰
                            if 'reasoning_content' in delta:
                                has_thinking_field = True
                                thinking_content = delta.get('reasoning_content', '')
                            
                            # æ–¹å¼2: delta.thinkingï¼ˆæœ€å¸¸è§ï¼‰
                            elif 'thinking' in delta:
                                has_thinking_field = True
                                thinking_content = delta.get('thinking', '')
                            
                            # æ–¹å¼3: delta.reasoningï¼ˆæŸäº› API ä½¿ç”¨ï¼‰
                            elif 'reasoning' in delta:
                                has_thinking_field = True
                                thinking_content = delta.get('reasoning', '')
                            
                            # æ–¹å¼4: choice ä¸­ç›´æ¥æœ‰ thinking å­—æ®µ
                            elif 'thinking' in choice:
                                has_thinking_field = True
                                thinking_content = choice.get('thinking', '')
                            
                            # æ–¹å¼5: finish_reason ä¸º thinking æ—¶ï¼Œæ•´ä¸ª delta å¯èƒ½æ˜¯ thinking
                            elif finish_reason == 'thinking':
                                has_thinking_field = True
                                # å¦‚æœ finish_reason æ˜¯ thinkingï¼Œå°è¯•ä» delta ä¸­æå–
                                if delta:
                                    # å°è¯•è·å–æ‰€æœ‰é content çš„å­—æ®µä½œä¸º thinking
                                    thinking_dict = {k: v for k, v in delta.items() if k != 'content' and k != 'role'}
                                    if thinking_dict:
                                        # ä¼˜å…ˆä½¿ç”¨ reasoning_content
                                        if 'reasoning_content' in thinking_dict:
                                            thinking_content = thinking_dict['reasoning_content']
                                        else:
                                            thinking_content = json.dumps(thinking_dict, ensure_ascii=False)
                                    else:
                                        thinking_content = str(delta)
                            
                            # æ–¹å¼6: æ£€æŸ¥æ•´ä¸ª chunk_data ä¸­æ˜¯å¦æœ‰ thinking å­—æ®µ
                            elif 'thinking' in chunk_data:
                                has_thinking_field = True
                                thinking_content = chunk_data.get('thinking', '')
                            
                            # æ–¹å¼7: æ£€æŸ¥æ•´ä¸ª chunk_data ä¸­æ˜¯å¦æœ‰ reasoning_content å­—æ®µ
                            elif 'reasoning_content' in chunk_data:
                                has_thinking_field = True
                                thinking_content = chunk_data.get('reasoning_content', '')
                            
                            # å¦‚æœæ£€æµ‹åˆ° thinking å­—æ®µï¼ˆå³ä½¿å†…å®¹ä¸ºç©ºï¼‰ï¼Œæ ‡è®°ä¸ºå·²å¼€å§‹
                            if has_thinking_field and not thinking_started:
                                thinking_prefix = "ğŸ’­ [Thinking] "
                                logger.info("ğŸ’­ å¼€å§‹è¾“å‡º Thinking è¿‡ç¨‹...")
                                # æ€»æ˜¯è°ƒç”¨å›è°ƒå‡½æ•°ï¼ˆä¸æ£€æŸ¥æ¡ä»¶ï¼Œç¡®ä¿æ€»æ˜¯æ¨é€ï¼‰
                                thinking_callback(thinking_prefix)
                                # åˆå§‹åŒ–æ—¥å¿—è®¡æ•°å™¨
                                if not hasattr(self, '_thinking_log_count'):
                                    self._thinking_log_count = 0
                                self._thinking_log_count += 1
                                logger.info(f"ğŸ’­ [DEBUG] å·²è°ƒç”¨ thinking_callback æ¨é€å‰ç¼€ #{self._thinking_log_count}: '{thinking_prefix}'")
                                thinking_started = True
                            
                            # å®æ—¶è¾“å‡º thinking å†…å®¹ï¼ˆç«‹å³è¾“å‡ºï¼Œä¸ç§¯ç´¯ï¼‰
                            # æ³¨æ„ï¼šåªè¦æ£€æµ‹åˆ° thinking å†…å®¹ï¼ˆåŒ…æ‹¬ç©ºå­—ç¬¦ä¸²ï¼‰ï¼Œå°±ç«‹å³æ¨é€
                            if thinking_content is not None:
                                # ç¡®ä¿ thinking_content æ˜¯å­—ç¬¦ä¸²
                                if not isinstance(thinking_content, str):
                                    thinking_content = str(thinking_content)
                                
                                # ç´¯ç§¯ thinking å†…å®¹ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
                                full_thinking += thinking_content
                                # æ€»æ˜¯è°ƒç”¨å›è°ƒå‡½æ•°ï¼ˆä¸æ£€æŸ¥æ¡ä»¶ï¼Œç¡®ä¿æ€»æ˜¯æ¨é€ï¼Œå³ä½¿å†…å®¹ä¸ºç©ºï¼‰
                                thinking_callback(thinking_content)
                                
                                # å‡å°‘æ—¥å¿—é¢‘ç‡ï¼šæ¯30ä¸ªchunkè®°å½•ä¸€æ¬¡ï¼Œæˆ–å†…å®¹é•¿åº¦ > 50 æ—¶è®°å½•
                                if not hasattr(self, '_thinking_log_count'):
                                    self._thinking_log_count = 0
                                self._thinking_log_count += 1
                                if self._thinking_log_count % 30 == 1 or len(thinking_content) > 50:
                                    logger.info(f"ğŸ’­ [DEBUG] å·²è°ƒç”¨ thinking_callback æ¨é€å†…å®¹ #{self._thinking_log_count}: {len(thinking_content)} å­—ç¬¦, å†…å®¹é¢„è§ˆ: '{thinking_content[:100] if len(thinking_content) > 100 else thinking_content}'")
                            
                            # æå–æ™®é€š content å†…å®¹
                            content = delta.get('content', '')
                            if content:
                                full_content += content
                            
                            # è°ƒè¯•ï¼šè¾“å‡º chunk ç»“æ„ï¼ˆä»…åœ¨ debug æ¨¡å¼ä¸‹ï¼‰
                            if logger.isEnabledFor(logging.DEBUG):
                                logger.debug(f"ğŸ“¦ Chunk: finish_reason={finish_reason}, delta_keys={list(delta.keys())}, has_thinking={'thinking' in delta or 'thinking' in choice}, has_content=bool(content)")
                            
                    except json.JSONDecodeError:
                        # å¿½ç•¥æ— æ³•è§£æçš„è¡Œï¼ˆå¯èƒ½æ˜¯ç©ºè¡Œæˆ–å…¶ä»–æ ¼å¼ï¼‰
                        continue
            
            # Thinking æµå¼è¾“å‡ºå®Œæˆ
            if thinking_started:
                logger.info("ğŸ’­ Thinking æµå¼è¾“å‡ºå®Œæˆ")
            elif first_chunk_logged and not full_thinking:
                # å¦‚æœæ”¶åˆ°äº† chunk ä½†æ²¡æœ‰ thinking å†…å®¹ï¼Œå¯èƒ½æ˜¯ API ä¸æ”¯æŒæˆ–æ¨¡å‹æœªç”Ÿæˆ thinking
                logger.info("ğŸ’­ æ³¨æ„ï¼šå·²æ¥æ”¶å“åº”ä½†æœªæ£€æµ‹åˆ° Thinking å†…å®¹ï¼ˆå¯èƒ½ API ä¸æ”¯æŒæˆ–æ¨¡å‹æœªç”Ÿæˆ thinkingï¼‰")
            
            # è¾“å‡ºå®Œæ•´çš„ thinking è¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
            if full_thinking:
                logger.info("=" * 60)
                logger.info("ğŸ§  LLM Thinking è¿‡ç¨‹ï¼ˆå®Œæ•´ï¼‰:")
                logger.info("=" * 60)
                logger.info(full_thinking)
                logger.info("=" * 60)
            
            if not full_content:
                logger.warning("âš ï¸ LLM æµå¼å“åº”ä¸ºç©º")
                return None
            
            logger.info("âœ… LLM API è°ƒç”¨æˆåŠŸ")
            logger.info("=" * 60)
            logger.info("ğŸ“ LLM å“åº”å†…å®¹:")
            logger.info("=" * 60)
            logger.info(full_content)
            logger.info("=" * 60)
            
            return full_content
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥ (ç½‘ç»œé”™è¯¯): {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_json = e.response.json()
                    logger.error(f"ğŸ“‹ API é”™è¯¯å“åº”: {json.dumps(error_json, ensure_ascii=False, indent=2)}")
                except:
                    logger.error(f"ğŸ“‹ API é”™è¯¯å“åº” (æ–‡æœ¬): {e.response.text}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
    
    def _parse_llm_analysis_response(self, response: str) -> HeaderAnalysis:
        """è§£æLLMåˆ†æç»“æœï¼ˆåŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰"""
        if not response:
            raise ValueError("LLMå“åº”ä¸ºç©º")
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            # è§£ææœ‰æ•ˆåˆ—
            valid_cols = data.get('valid_cols')
            if valid_cols is None:
                # å¦‚æœä¸ºnullï¼Œè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
                valid_cols = None
            elif isinstance(valid_cols, list):
                # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                valid_cols = [int(col) for col in valid_cols if isinstance(col, (int, str)) and str(col).isdigit()]
                # å¦‚æœåˆ—è¡¨ä¸ºç©ºæˆ–åŒ…å«æ‰€æœ‰åˆ—ï¼Œè®¾ä¸ºNone
                max_col = self.ws.max_column
                if not valid_cols or set(valid_cols) == set(range(1, max_col + 1)):
                    valid_cols = None
            else:
                valid_cols = None
            
            # è§£æèµ·å§‹åˆ—ï¼ˆé»˜è®¤ä¸º1ï¼‰
            start_col = int(data.get('start_col', 1))
            if start_col < 1:
                start_col = 1
            
            # æ„å»ºHeaderAnalysiså¯¹è±¡
            analysis = HeaderAnalysis(
                skip_rows=int(data.get('skip_rows', 0)),
                header_rows=int(data.get('header_rows', 1)),
                header_type=data.get('header_type', 'single'),
                data_start_row=int(data.get('data_start_row', 1)),
                start_col=start_col,
                confidence=data.get('confidence', 'medium'),
                reason=f"LLMåˆ†æ: {data.get('reason', '')}",
                valid_cols=valid_cols
            )
            
            logger.info(f"âœ… LLMåˆ†æå®Œæˆ:")
            logger.info(f"  - è·³è¿‡è¡Œæ•°: {analysis.skip_rows}")
            logger.info(f"  - è¡¨å¤´è¡Œæ•°: {analysis.header_rows}")
            logger.info(f"  - è¡¨å¤´ç±»å‹: {analysis.header_type}")
            logger.info(f"  - æ•°æ®èµ·å§‹è¡Œ: {analysis.data_start_row}")
            logger.info(f"  - æ•°æ®èµ·å§‹åˆ—: {analysis.start_col}")
            logger.info(f"  - ç½®ä¿¡åº¦: {analysis.confidence}")
            
            return analysis
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"âŒ è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
            logger.error(f"ğŸ“‹ å“åº”å†…å®¹: {response[:500]}")
            raise ValueError(f"è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
    
    def _parse_validation_response(self, response: str, rule_analysis: HeaderAnalysis) -> HeaderAnalysis:
        """è§£æLLMéªŒè¯ç»“æœï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
        if not response:
            # LLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœ
            return rule_analysis
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            is_valid = data.get('is_valid', True)
            suggestions = data.get('suggestions', {})
            
            if is_valid:
                # LLMè®¤ä¸ºè§„åˆ™åˆ†æç»“æœåˆç†ï¼Œä¿æŒåŸç»“æœä½†æ›´æ–°ç½®ä¿¡åº¦å’ŒåŸå› 
                return HeaderAnalysis(
                    skip_rows=rule_analysis.skip_rows,
                    header_rows=rule_analysis.header_rows,
                    header_type=rule_analysis.header_type,
                    data_start_row=rule_analysis.data_start_row,
                    start_col=rule_analysis.start_col,  # ä¿æŒåŸæœ‰çš„èµ·å§‹åˆ—
                    confidence=data.get('confidence', 'high'),  # LLMéªŒè¯é€šè¿‡ï¼Œç½®ä¿¡åº¦æå‡
                    reason=f"è§„åˆ™åˆ†æ+LLMéªŒè¯: {data.get('reason', 'éªŒè¯é€šè¿‡')}",
                    valid_cols=rule_analysis.valid_cols  # ä¿æŒåŸæœ‰çš„åˆ—è¿‡æ»¤ç»“æœ
                )
            else:
                # LLMè®¤ä¸ºä¸åˆç†ï¼Œä½¿ç”¨LLMçš„å»ºè®®
                # æ³¨æ„ï¼šLLMå¯èƒ½å»ºè®®ä¿®æ”¹è¡¨å¤´è¡Œæ•°ï¼Œä½†åˆ—è¿‡æ»¤ç»“æœä»ç„¶ä¿ç•™
                return HeaderAnalysis(
                    skip_rows=suggestions.get('skip_rows', rule_analysis.skip_rows),
                    header_rows=suggestions.get('header_rows', rule_analysis.header_rows),
                    header_type=suggestions.get('header_type', rule_analysis.header_type),
                    data_start_row=suggestions.get('data_start_row', rule_analysis.data_start_row),
                    start_col=suggestions.get('start_col', rule_analysis.start_col),  # ä¿æŒæˆ–ä½¿ç”¨å»ºè®®çš„èµ·å§‹åˆ—
                    confidence=data.get('confidence', 'medium'),
                    reason=f"è§„åˆ™åˆ†æ+LLMä¿®æ­£: {data.get('reason', 'LLMå»ºè®®ä¿®æ­£')}",
                    valid_cols=rule_analysis.valid_cols  # ä¿æŒåŸæœ‰çš„åˆ—è¿‡æ»¤ç»“æœ
                )
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"è§£æLLMéªŒè¯å“åº”å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸè§„åˆ™åˆ†æç»“æœ")
        
        # è§£æå¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœ
        return rule_analysis
    
    def _detect_valid_columns(self, skip_rows: int, header_rows: int, data_start_row: int) -> List[int]:
        """
        æ£€æµ‹æœ‰æ•ˆåˆ—ï¼ˆè¿‡æ»¤æ— æ•ˆåˆ—ï¼‰
        
        æ— æ•ˆåˆ—çš„åˆ¤æ–­æ ‡å‡†ï¼š
        1. è¡¨å¤´åŒºåŸŸå®Œå…¨ä¸ºç©º
        2. æ•°æ®åŒºåŸŸå®Œå…¨ä¸ºç©ºæˆ–æ²¡æœ‰æ•°å€¼æ•°æ®
        
        è¿”å›: æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰
        """
        max_col = self.ws.max_column
        header_start = skip_rows + 1
        header_end = skip_rows + header_rows
        valid_cols = []
        
        logger.info("ğŸ” å¼€å§‹æ£€æµ‹æ— æ•ˆåˆ—...")
        
        for col in range(1, max_col + 1):
            # æ£€æŸ¥è¡¨å¤´åŒºåŸŸæ˜¯å¦æœ‰å†…å®¹
            has_header = False
            for row in range(header_start, header_end + 1):
                value = self.get_cell_value(row, col)
                if value is not None and str(value).strip():
                    has_header = True
                    break
            
            # æ£€æŸ¥æ•°æ®åŒºåŸŸæ˜¯å¦æœ‰æ•°å€¼æ•°æ®
            has_data = False
            numeric_count = 0
            total_count = 0
            for row in range(data_start_row, min(data_start_row + 10, self.ws.max_row + 1)):
                value = self.ws.cell(row, col).value
                if value is not None:
                    total_count += 1
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        numeric_count += 1
                        has_data = True
            
            # å¦‚æœè¡¨å¤´æœ‰å†…å®¹æˆ–æ•°æ®åŒºåŸŸæœ‰æ•°å€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆåˆ—
            if has_header or has_data:
                valid_cols.append(col)
                logger.debug(f"âœ… åˆ— {col}: æœ‰æ•ˆ (è¡¨å¤´: {has_header}, æ•°æ®: {has_data}, æ•°å€¼: {numeric_count}/{total_count})")
            else:
                logger.info(f"âŒ åˆ— {col}: æ— æ•ˆ (è¡¨å¤´ä¸ºç©ºä¸”æ•°æ®ä¸ºç©º)")
        
        logger.info(f"ğŸ“Š åˆ—è¿‡æ»¤ç»“æœ: æ€»åˆ—æ•° {max_col}, æœ‰æ•ˆåˆ—æ•° {len(valid_cols)}, æ— æ•ˆåˆ—æ•° {max_col - len(valid_cols)}")
        
        # å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆï¼Œè¿”å›Noneï¼ˆè¡¨ç¤ºä¸éœ€è¦è¿‡æ»¤ï¼‰
        if len(valid_cols) == max_col:
            return None
        
        return valid_cols
    
    def extract_headers(self, analysis: HeaderAnalysis) -> Tuple[List[str], Dict[str, Dict]]:
        """
        æ ¹æ®åˆ†æç»“æœæå–è¡¨å¤´
        è¿”å›: (åˆ—ååˆ—è¡¨, åˆ—ç»“æ„å…ƒæ•°æ®)
        """
        max_col = self.ws.max_column
        header_start = analysis.skip_rows + 1
        header_end = analysis.skip_rows + analysis.header_rows
        
        # ç¡®å®šè¦å¤„ç†çš„åˆ—ï¼šä» start_col å¼€å§‹ï¼Œå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåˆ™å–äº¤é›†
        all_cols = list(range(analysis.start_col, max_col + 1))
        if analysis.valid_cols is not None:
            # å–äº¤é›†ï¼šä» start_col å¼€å§‹ï¼Œä¸”åœ¨ valid_cols ä¸­çš„åˆ—
            cols_to_process = [col for col in all_cols if col in analysis.valid_cols]
        else:
            cols_to_process = all_cols
        
        logger.info(f"ğŸ“‹ æå–è¡¨å¤´: å¤„ç† {len(cols_to_process)} åˆ—")
        
        # è°ƒè¯•ï¼šæµå¼æ‰“å°åŸå§‹å¤šçº§è¡¨å¤´ï¼ˆä¸åšä»»ä½•ç¾åŒ–ï¼‰
        print("=" * 80)
        print("ã€åŸå§‹å¤šçº§è¡¨å¤´ - æµå¼æ‰“å°ã€‘")
        print(f"è¡¨å¤´è¡ŒèŒƒå›´: ç¬¬ {header_start} è¡Œåˆ°ç¬¬ {header_end} è¡Œ")
        print(f"å¤„ç†åˆ—èŒƒå›´: ç¬¬ {analysis.start_col} åˆ—åˆ°ç¬¬ {max_col} åˆ—")
        print("=" * 80)
        sys.stdout.flush()
        
        for row in range(header_start, header_end + 1):
            row_values = []
            for col in cols_to_process:
                value = self.get_cell_value(row, col)
                row_values.append(value)
            # ç›´æ¥æ‰“å°ï¼Œä¸åšä»»ä½•ç¾åŒ–
            print(f"è¡Œ{row}: {row_values}")
            sys.stdout.flush()
        
        print("=" * 80)
        sys.stdout.flush()
        
        column_metadata = {}
        
        if analysis.header_type == 'single':
            # å•è¡¨å¤´
            headers = []
            for col in cols_to_process:
                value = self.get_cell_value(header_start, col)
                col_name = str(value) if value else f'Column_{col}'
                headers.append(col_name)
                column_metadata[col_name] = {"level1": col_name}
            
            headers = self._handle_duplicate_names(headers)
            # æ›´æ–°å…ƒæ•°æ®çš„key
            column_metadata = {h: {"level1": h} for h in headers}
            return headers, column_metadata
        
        else:
            # å¤šè¡¨å¤´ï¼šå±•å¹³
            column_headers = []
            original_metadata_list = []  # ä¿å­˜åŸå§‹å…ƒæ•°æ®åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºå¯¹åº”
            
            for col in cols_to_process:
                parts = []
                levels = {}
                for row_idx, row in enumerate(range(header_start, header_end + 1), 1):
                    value = self.get_cell_value(row, col)
                    if value is not None:
                        part = str(value).strip()
                        parts.append(part)
                        levels[f"level{row_idx}"] = part
                
                # å»é‡è¿ç»­ç›¸åŒå€¼
                unique_parts = []
                for p in parts:
                    if not unique_parts or p != unique_parts[-1]:
                        unique_parts.append(p)
                
                col_name = '_'.join(unique_parts) if unique_parts else f'Column_{col}'
                column_headers.append(col_name)
                original_metadata_list.append(levels)  # æŒ‰é¡ºåºä¿å­˜å…ƒæ•°æ®
            
            # å¤„ç†é‡å¤åˆ—å
            column_headers = self._handle_duplicate_names(column_headers)
            
            # é‡æ–°æ˜ å°„å…ƒæ•°æ®ï¼šä½¿ç”¨ç´¢å¼•å¯¹åº”å…³ç³»
            new_metadata = {}
            for i, header in enumerate(column_headers):
                # ä½¿ç”¨ç´¢å¼•ç›´æ¥è·å–å¯¹åº”çš„å…ƒæ•°æ®
                if i < len(original_metadata_list):
                    new_metadata[header] = original_metadata_list[i]
                else:
                    # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œåˆ›å»ºé»˜è®¤å…ƒæ•°æ®
                    logger.warning(f"âš ï¸ ç´¢å¼•è¶…å‡ºèŒƒå›´: i={i}, headersé•¿åº¦={len(column_headers)}, metadataé•¿åº¦={len(original_metadata_list)}")
                    new_metadata[header] = {"level1": header}
            
            return column_headers, new_metadata
    
    def _handle_duplicate_names(self, names: List[str]) -> List[str]:
        """å¤„ç†é‡å¤åˆ—å"""
        counts = defaultdict(int)
        result = []
        for name in names:
            if counts[name] > 0:
                result.append(f"{name}_{counts[name]}")
            else:
                result.append(name)
            counts[name] += 1
        return result
    
    def to_dataframe(self, analysis: HeaderAnalysis = None, use_llm_validate: bool = False,
                    llm_api_key: Optional[str] = None,
                    llm_base_url: Optional[str] = None,
                    llm_model: Optional[str] = None,
                    preprocessing_timeout: Optional[int] = None,
                    thinking_callback: Optional[callable] = None) -> Tuple[pd.DataFrame, HeaderAnalysis, Dict[str, Dict], Optional[str]]:
        """
        è½¬æ¢ä¸ºDataFrame
        
        å‚æ•°:
            analysis: é¢„å…ˆçš„åˆ†æç»“æœï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨LLMè‡ªåŠ¨åˆ†æ
            use_llm_validate: å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        
        è¿”å›:
            (DataFrame, åˆ†æç»“æœ, åˆ—ç»“æ„å…ƒæ•°æ®, LLMåŸå§‹å“åº”)
        """
        llm_response = None
        if analysis is None:
            # å¿…é¡»ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼ˆåŒæ—¶åˆ†æè¡Œå’Œåˆ—ï¼‰
            logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨LLMåˆ†æExcelè¡¨æ ¼ç»“æ„ï¼ˆè¡Œå’Œåˆ—ï¼‰...")
            
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
            api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
            if not api_key:
                raise ValueError("LLM APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒExcelåˆ†æã€‚è¯·é…ç½®EXCEL_LLM_API_KEYæˆ–ä¼ å…¥llm_api_keyå‚æ•°")
            
            # ä½¿ç”¨LLMç›´æ¥åˆ†æï¼ˆåŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰
            analysis, llm_response = self.analyze_with_llm(
                llm_api_key=llm_api_key,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
                timeout=preprocessing_timeout,
                thinking_callback=thinking_callback
            )
            logger.info("âœ… LLMåˆ†æå®Œæˆï¼ˆå·²åŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰")
            # ä¿å­˜LLMå“åº”åˆ°å®ä¾‹å˜é‡ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨
            self._llm_analysis_response = llm_response
        
        headers, column_metadata = self.extract_headers(analysis)
        
        # ç¡®å®šè¦è¯»å–çš„åˆ—ï¼šä» start_col å¼€å§‹ï¼Œå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåˆ™å–äº¤é›†
        max_col = self.ws.max_column
        all_cols = list(range(analysis.start_col, max_col + 1))
        if analysis.valid_cols is not None:
            # å–äº¤é›†ï¼šä» start_col å¼€å§‹ï¼Œä¸”åœ¨ valid_cols ä¸­çš„åˆ—
            cols_to_read = [col for col in all_cols if col in analysis.valid_cols]
        else:
            cols_to_read = all_cols
        
        logger.info(f"ğŸ“Š è¯»å–æ•°æ®: ä»ç¬¬ {analysis.start_col} åˆ—å¼€å§‹ï¼Œå…± {len(cols_to_read)} åˆ—")
        
        # è¯»å–æ•°æ®
        data = []
        for row in range(analysis.data_start_row, self.ws.max_row + 1):
            row_data = []
            for col in cols_to_read:
                row_data.append(self.ws.cell(row, col).value)
            if any(v is not None for v in row_data):
                data.append(row_data)
        
        df = pd.DataFrame(data, columns=headers)
        
        # æ™ºèƒ½ç±»å‹è½¬æ¢ï¼šå°è¯•å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—
        logger.info("ğŸ”„ å¼€å§‹æ™ºèƒ½ç±»å‹è½¬æ¢...")
        def smart_convert_value(value):
            """æ™ºèƒ½è½¬æ¢å€¼ï¼šå°è¯•å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—"""
            if value is None:
                return value
            if isinstance(value, (int, float)):
                return value
            if isinstance(value, str):
                # å»é™¤å‰åç©ºæ ¼
                value = value.strip()
                if not value:  # ç©ºå­—ç¬¦ä¸²
                    return None
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                try:
                    # å°è¯•æ•´æ•°ï¼ˆæ”¯æŒè´Ÿæ•°ï¼‰
                    if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                        return int(value)
                    # å°è¯•æµ®ç‚¹æ•°ï¼ˆæ”¯æŒç§‘å­¦è®¡æ•°æ³•ï¼‰
                    return float(value)
                except (ValueError, AttributeError):
                    # è½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå­—ç¬¦ä¸²
                    return value
            return value
        
        # å¯¹æ¯åˆ—åº”ç”¨æ™ºèƒ½è½¬æ¢
        for col in df.columns:
            original_type = df[col].dtype
            df[col] = df[col].apply(smart_convert_value)
            new_type = df[col].dtype
            if original_type != new_type:
                logger.debug(f"  åˆ— '{col}': {original_type} â†’ {new_type}")
        
        # ä½¿ç”¨ pandas çš„ convert_dtypes è¿›ä¸€æ­¥ä¼˜åŒ–ç±»å‹æ¨æ–­
        df = df.convert_dtypes()
        
        logger.info(f"âœ… DataFrame åˆ›å»ºå®Œæˆ: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        logger.info(f"ğŸ“Š æ•°æ®ç±»å‹ä¼˜åŒ–å®Œæˆ")
        return df, analysis, column_metadata, llm_response
    
    def close(self):
        """å…³é—­å·¥ä½œç°¿å¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            self.wb.close()
        except Exception:
            pass
        
        # åˆ é™¤ä¸´æ—¶è½¬æ¢çš„ .xlsx æ–‡ä»¶
        if self._temp_xlsx_path and os.path.exists(self._temp_xlsx_path):
            try:
                os.remove(self._temp_xlsx_path)
                logger.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self._temp_xlsx_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {self._temp_xlsx_path}, é”™è¯¯: {e}")


def _validate_excel_file_basic(filepath: str, max_file_size_mb: Optional[int] = None) -> None:
    """
    åŸºç¡€æ–‡ä»¶æ£€æŸ¥ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼‰
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    å¼‚å¸¸:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: æ–‡ä»¶ä¸ºç©ºæˆ–è¿‡å¤§
        PermissionError: æ–‡ä»¶ä¸å¯è¯»
    """
    from .config import EXCEL_MAX_FILE_SIZE_MB
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è¯»
    if not os.access(filepath, os.R_OK):
        raise PermissionError(f"Excelæ–‡ä»¶ä¸å¯è¯»: {filepath}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(filepath)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæ–‡ä»¶
    if file_size == 0:
        raise ValueError(f"Excelæ–‡ä»¶ä¸ºç©ºï¼ˆ0å­—èŠ‚ï¼‰: {filepath}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
    max_size_mb = max_file_size_mb if max_file_size_mb is not None else EXCEL_MAX_FILE_SIZE_MB
    max_size_bytes = max_size_mb * 1024 * 1024
    
    if file_size > max_size_bytes:
        file_size_mb = file_size / 1024 / 1024
        raise ValueError(f"Excelæ–‡ä»¶è¿‡å¤§: {file_size_mb:.2f}MBï¼Œè¶…è¿‡é™åˆ¶ï¼ˆ{max_size_mb}MBï¼‰: {filepath}")
    
    # å¦‚æœæ–‡ä»¶è¾ƒå¤§ï¼ˆ> 100MBï¼‰ï¼Œè®°å½•è­¦å‘Š
    if file_size > 100 * 1024 * 1024:
        file_size_mb = file_size / 1024 / 1024
        logger.warning(f"âš ï¸ Excelæ–‡ä»¶è¾ƒå¤§: {file_size_mb:.2f}MBï¼Œå¤„ç†å¯èƒ½è¾ƒæ…¢: {filepath}")


def _validate_xlsx_format(filepath: str, timeout: float = 0.5) -> None:
    """
    éªŒè¯ .xlsx æ–‡ä»¶çš„ZIPæ ¼å¼ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’
    
    å¼‚å¸¸:
        ValueError: ZIPæ ¼å¼é”™è¯¯æˆ–æ–‡ä»¶æŸå
    """
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    def _validate():
        """åœ¨åå°çº¿ç¨‹ä¸­éªŒè¯ZIPæ ¼å¼"""
        try:
            # æ£€æŸ¥ZIPæ–‡ä»¶å¤´ï¼ˆå‰4ä¸ªå­—èŠ‚ï¼‰
            with open(filepath, 'rb') as f:
                header = f.read(4)
                if header != b'PK\x03\x04':
                    raise ValueError(f"ä¸æ˜¯æœ‰æ•ˆçš„Excelæ–‡ä»¶ï¼ˆZIPæ ¼å¼é”™è¯¯ï¼‰: {filepath}")
            
            # å°è¯•æ‰“å¼€ZIPæ–‡ä»¶éªŒè¯
            try:
                with zipfile.ZipFile(filepath, 'r') as zf:
                    # å°è¯•è¯»å–ZIPæ–‡ä»¶åˆ—è¡¨ï¼ˆä¸å®é™…è§£å‹ï¼‰
                    zf.namelist()
            except zipfile.BadZipFile:
                raise ValueError(f"Excelæ–‡ä»¶æŸåï¼ˆZIPæ ¼å¼æ— æ•ˆï¼‰: {filepath}")
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸å¯èƒ½æ˜¯æƒé™é—®é¢˜ç­‰ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­
                logger.warning(f"âš ï¸ ZIPéªŒè¯æ—¶å‡ºç°å¼‚å¸¸ï¼ˆå¯èƒ½ä¸å½±å“ä½¿ç”¨ï¼‰: {e}")
        except Exception as e:
            if isinstance(e, ValueError):
                raise
            logger.error(f"éªŒè¯ZIPæ ¼å¼å¤±è´¥: {filepath}, é”™è¯¯: {e}")
            raise ValueError(f"éªŒè¯Excelæ–‡ä»¶æ ¼å¼å¤±è´¥: {str(e)}")
    
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_validate)
            try:
                future.result(timeout=timeout)
            except FutureTimeoutError:
                logger.warning(f"âš ï¸ ZIPæ ¼å¼éªŒè¯è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œä½†ç»§ç»­å¤„ç†: {filepath}")
                # ZIPéªŒè¯è¶…æ—¶ä¸é˜»å¡ï¼Œåªè®°å½•è­¦å‘Š
                future.cancel()
    except Exception as e:
        if isinstance(e, ValueError):
            raise
        logger.warning(f"âš ï¸ ZIPæ ¼å¼éªŒè¯å¼‚å¸¸ï¼Œä½†ç»§ç»­å¤„ç†: {e}")


def _validate_excel_structure(filepath: str, timeout: float = 2.0) -> None:
    """
    éªŒè¯Excelæ–‡ä»¶ç»“æ„å®Œæ•´æ€§ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤2ç§’
    
    å¼‚å¸¸:
        ValueError: Excelæ–‡ä»¶ç»“æ„ä¸å®Œæ•´
    """
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    def _validate():
        """åœ¨åå°çº¿ç¨‹ä¸­éªŒè¯Excelç»“æ„"""
        try:
            with zipfile.ZipFile(filepath, 'r') as zf:
                namelist = zf.namelist()
                
                # æ£€æŸ¥å¿…éœ€çš„Excelæ–‡ä»¶
                required_files = [
                    '[Content_Types].xml',
                    'xl/workbook.xml'
                ]
                
                missing_files = []
                for req_file in required_files:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå¯èƒ½è·¯å¾„ç•¥æœ‰ä¸åŒï¼‰
                    found = False
                    for name in namelist:
                        if name == req_file or name.endswith('/' + req_file):
                            found = True
                            break
                    if not found:
                        missing_files.append(req_file)
                
                if missing_files:
                    raise ValueError(
                        f"Excelæ–‡ä»¶ç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…éœ€æ–‡ä»¶: {', '.join(missing_files)}: {filepath}"
                    )
        except zipfile.BadZipFile:
            # å¦‚æœZIPæ–‡ä»¶æœ¬èº«æœ‰é—®é¢˜ï¼Œè¿™ä¸ªåº”è¯¥åœ¨ä¹‹å‰çš„æ£€æŸ¥ä¸­å‘ç°
            raise ValueError(f"Excelæ–‡ä»¶æŸåï¼ˆZIPæ ¼å¼æ— æ•ˆï¼‰: {filepath}")
        except Exception as e:
            if isinstance(e, ValueError):
                raise
            logger.error(f"éªŒè¯Excelç»“æ„å¤±è´¥: {filepath}, é”™è¯¯: {e}")
            raise ValueError(f"éªŒè¯Excelæ–‡ä»¶ç»“æ„å¤±è´¥: {str(e)}")
    
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_validate)
            try:
                future.result(timeout=timeout)
            except FutureTimeoutError:
                logger.warning(f"âš ï¸ Excelç»“æ„éªŒè¯è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œä½†ç»§ç»­å¤„ç†: {filepath}")
                # ç»“æ„éªŒè¯è¶…æ—¶ä¸é˜»å¡ï¼Œåªè®°å½•è­¦å‘Š
                future.cancel()
    except Exception as e:
        if isinstance(e, ValueError):
            raise
        logger.warning(f"âš ï¸ Excelç»“æ„éªŒè¯å¼‚å¸¸ï¼Œä½†ç»§ç»­å¤„ç†: {e}")


def _validate_excel_row_count(filepath: str, sheet_name: str = None, max_rows: int = 10000, timeout: float = 5.0) -> None:
    """
    éªŒè¯Excelæ–‡ä»¶çš„è¡Œæ•°ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æ£€æŸ¥ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        max_rows: æœ€å¤§å…è®¸è¡Œæ•°ï¼Œé»˜è®¤10000
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
    
    å¼‚å¸¸:
        ValueError: è¡Œæ•°è¶…è¿‡é™åˆ¶
    """
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    def _check_rows():
        """åœ¨åå°çº¿ç¨‹ä¸­æ£€æŸ¥è¡Œæ•°"""
        try:
            file_ext = Path(filepath).suffix.lower()
            
            # å¦‚æœæ˜¯ .xls æ ¼å¼ï¼Œéœ€è¦å…ˆè½¬æ¢ï¼ˆä½†è¿™é‡Œåªåšå¿«é€Ÿæ£€æŸ¥ï¼Œä¸è½¬æ¢ï¼‰
            # å¯¹äº .xls æ–‡ä»¶ï¼Œè·³è¿‡è¡Œæ•°æ£€æŸ¥ï¼ˆå› ä¸ºè½¬æ¢éœ€è¦æ—¶é—´ï¼‰
            if file_ext == '.xls':
                logger.debug(f"âš ï¸ .xls æ–‡ä»¶è·³è¿‡è¡Œæ•°æ£€æŸ¥ï¼ˆéœ€è¦è½¬æ¢åæ‰èƒ½æ£€æŸ¥ï¼‰: {filepath}")
                return
            
            # ä½¿ç”¨ read_only=True æ¨¡å¼ï¼Œå¿«é€Ÿè¯»å–è¡Œæ•°
            wb = load_workbook(filepath, data_only=True, read_only=True)
            
            try:
                # é€‰æ‹©å·¥ä½œè¡¨
                if sheet_name:
                    if sheet_name not in wb.sheetnames:
                        raise ValueError(f"å·¥ä½œè¡¨ '{sheet_name}' ä¸å­˜åœ¨: {filepath}")
                    ws = wb[sheet_name]
                else:
                    if not wb.sheetnames:
                        raise ValueError(f"Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨: {filepath}")
                    ws = wb[wb.sheetnames[0]]  # æ£€æŸ¥ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                
                # è·å–æœ€å¤§è¡Œæ•°
                row_count = ws.max_row
                logger.info(f"ğŸ“Š Excelæ–‡ä»¶è¡Œæ•°æ£€æŸ¥: {row_count} è¡Œï¼ˆé™åˆ¶: {max_rows} è¡Œï¼‰")
                
                if row_count > max_rows:
                    raise ValueError(
                        f"Excelæ–‡ä»¶è¡Œæ•°è¿‡å¤š: {row_count} è¡Œï¼Œè¶…è¿‡é™åˆ¶ï¼ˆ{max_rows} è¡Œï¼‰: {filepath}"
                    )
            finally:
                wb.close()
        except Exception as e:
            if isinstance(e, ValueError):
                raise
            logger.error(f"æ£€æŸ¥Excelè¡Œæ•°å¤±è´¥: {filepath}, é”™è¯¯: {e}")
            raise ValueError(f"æ£€æŸ¥Excelæ–‡ä»¶è¡Œæ•°å¤±è´¥: {str(e)}")
    
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_check_rows)
            try:
                future.result(timeout=timeout)
            except FutureTimeoutError:
                logger.warning(f"âš ï¸ Excelè¡Œæ•°æ£€æŸ¥è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œä½†ç»§ç»­å¤„ç†: {filepath}")
                # è¡Œæ•°æ£€æŸ¥è¶…æ—¶ä¸é˜»å¡ï¼Œåªè®°å½•è­¦å‘Š
                future.cancel()
    except Exception as e:
        if isinstance(e, ValueError):
            raise
        logger.warning(f"âš ï¸ Excelè¡Œæ•°æ£€æŸ¥å¼‚å¸¸ï¼Œä½†ç»§ç»­å¤„ç†: {e}")


def print_excel_raw_data(filepath: str, sheet_name: str = None, max_rows: int = 15, max_cols: int = 25) -> None:
    """
    æ‰“å°Excelæ–‡ä»¶çš„åŸå§‹æ•°æ®ï¼ˆå‰15è¡Œï¼Œå‰25åˆ—ï¼‰
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
        max_rows: æœ€å¤§è¡Œæ•°ï¼Œé»˜è®¤15
        max_cols: æœ€å¤§åˆ—æ•°ï¼Œé»˜è®¤25
    """
    try:
        logger.info(f"ğŸ“‚ [DEBUG] print_excel_raw_data: å¼€å§‹å¤„ç†æ–‡ä»¶ {filepath}")
        from openpyxl import load_workbook
        
        # åŠ è½½å·¥ä½œç°¿
        logger.info(f"â³ [DEBUG] print_excel_raw_data: å¼€å§‹åŠ è½½å·¥ä½œç°¿ (read_only=True)...")
        wb = load_workbook(filepath, data_only=True, read_only=True)
        logger.info(f"âœ… [DEBUG] print_excel_raw_data: å·¥ä½œç°¿åŠ è½½å®Œæˆ")
        
        # é€‰æ‹©å·¥ä½œè¡¨
        logger.info(f"ğŸ“‹ [DEBUG] print_excel_raw_data: é€‰æ‹©å·¥ä½œè¡¨...")
        if sheet_name:
            ws = wb[sheet_name]
            logger.info(f"âœ… [DEBUG] print_excel_raw_data: ä½¿ç”¨æŒ‡å®šå·¥ä½œè¡¨: {sheet_name}")
        else:
            if not wb.sheetnames:
                logger.warning("âš ï¸ [DEBUG] print_excel_raw_data: Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨")
                print("âš ï¸ Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨")
                return
            ws = wb[wb.sheetnames[0]]
            logger.info(f"âœ… [DEBUG] print_excel_raw_data: ä½¿ç”¨é»˜è®¤å·¥ä½œè¡¨: {ws.title}")
        
        # ç¡®å®šå®é™…è¯»å–èŒƒå›´
        logger.info(f"ğŸ“ [DEBUG] print_excel_raw_data: å·¥ä½œè¡¨å¤§å° - æœ€å¤§è¡Œ: {ws.max_row}, æœ€å¤§åˆ—: {ws.max_column}")
        actual_max_col = min(ws.max_column, max_cols)
        actual_max_row = min(ws.max_row, max_rows)
        logger.info(f"ğŸ“ [DEBUG] print_excel_raw_data: å®é™…è¯»å–èŒƒå›´ - è¡Œ: {actual_max_row}, åˆ—: {actual_max_col}")
        
        # æ‰“å°åŸå§‹æ•°æ®
        logger.info(f"ğŸ–¨ï¸ [DEBUG] print_excel_raw_data: å¼€å§‹æ‰“å°æ•°æ®...")
        print("=" * 80)
        print(f"ã€æœ€åˆä¼ å…¥çš„ExcelåŸå§‹æ•°æ® - æ§åˆ¶å°æ‰“å°ã€‘ï¼ˆå‰{actual_max_row}è¡Œï¼Œå‰{actual_max_col}åˆ—ï¼‰")
        print(f"æ–‡ä»¶: {os.path.basename(filepath)}")
        print(f"å·¥ä½œè¡¨: {ws.title}")
        print("=" * 80)
        sys.stdout.flush()
        
        logger.info(f"ğŸ”„ [DEBUG] print_excel_raw_data: å¼€å§‹éå†å•å…ƒæ ¼...")
        for row in range(1, actual_max_row + 1):
            row_data = []
            for col in range(1, actual_max_col + 1):
                value = ws.cell(row, col).value
                row_data.append(value)
            print(f"è¡Œ{row}: {row_data}")
            sys.stdout.flush()
            if row % 5 == 0:  # æ¯5è¡Œè®°å½•ä¸€æ¬¡æ—¥å¿—
                logger.info(f"ğŸ“Š [DEBUG] print_excel_raw_data: å·²å¤„ç† {row}/{actual_max_row} è¡Œ")
        
        print("=" * 80)
        sys.stdout.flush()
        logger.info(f"âœ… [DEBUG] print_excel_raw_data: æ•°æ®æ‰“å°å®Œæˆ")
        
        # å…³é—­å·¥ä½œç°¿
        logger.info(f"ğŸ”’ [DEBUG] print_excel_raw_data: å…³é—­å·¥ä½œç°¿...")
        print("ğŸ” [DEBUG] print_excel_raw_data: å‡†å¤‡å…³é—­å·¥ä½œç°¿ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        sys.stdout.flush()
        wb.close()
        print("ğŸ” [DEBUG] print_excel_raw_data: å·¥ä½œç°¿å·²å…³é—­ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        sys.stdout.flush()
        logger.info(f"âœ… [DEBUG] print_excel_raw_data: å·¥ä½œç°¿å·²å…³é—­")
        
        # æ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå¸®åŠ©åƒåœ¾å›æ”¶
        del wb
        del ws
        import gc
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        print("ğŸ” [DEBUG] print_excel_raw_data: åƒåœ¾å›æ”¶å®Œæˆï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        sys.stdout.flush()
        
        logger.info(f"ğŸ [DEBUG] print_excel_raw_data: å‡½æ•°å³å°†è¿”å›ï¼Œæ‰€æœ‰æ“ä½œå·²å®Œæˆ")
        # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        sys.stdout.flush()
        logger.info(f"âœ… [DEBUG] print_excel_raw_data: å‡½æ•°æ‰§è¡Œå®Œæˆï¼Œå‡†å¤‡è¿”å›")
        # ä½¿ç”¨ print ç›´æ¥è¾“å‡ºï¼Œç¡®ä¿èƒ½çœ‹åˆ°
        print("ğŸ” [DEBUG] print_excel_raw_data: å‡½æ•°å³å°†è¿”å›ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        sys.stdout.flush()
        # æœ€åä¸€æ¡æ—¥å¿—
        logger.info(f"ğŸ [DEBUG] print_excel_raw_data: å‡½æ•°è¿”å›å‰æœ€åä¸€æ¡æ—¥å¿—")
        print("ğŸ” [DEBUG] print_excel_raw_data: å‡½æ•°è¿”å›ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        sys.stdout.flush()
        return  # æ˜¾å¼è¿”å›
        
    except Exception as e:
        logger.error(f"âŒ [DEBUG] print_excel_raw_data: æ‰“å°ExcelåŸå§‹æ•°æ®å¤±è´¥: {filepath}, é”™è¯¯: {e}", exc_info=True)
        print(f"âš ï¸ æ‰“å°ExcelåŸå§‹æ•°æ®å¤±è´¥: {str(e)}")
        sys.stdout.flush()


def _get_preview_data_lightweight(filepath: str, sheet_name: str = None, max_rows: int = 15, max_cols: int = 25, timeout: int = 10, max_file_size_mb: Optional[int] = None, max_excel_rows: Optional[int] = None) -> Tuple[List[List[Any]], int]:
    """
    è½»é‡çº§è·å–Excelé¢„è§ˆæ•°æ®ï¼ˆä½¿ç”¨read_onlyæ¨¡å¼ï¼Œç”¨äºè¡¨å¤´åˆ†æï¼‰
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
        max_rows: æœ€å¤§è¡Œæ•°ï¼Œé»˜è®¤15
        max_cols: æœ€å¤§åˆ—æ•°ï¼Œé»˜è®¤25
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    è¿”å›:
        (é¢„è§ˆæ•°æ®åˆ—è¡¨, æœ€å¤§åˆ—æ•°)
    """
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    def _read_preview():
        """åœ¨åå°çº¿ç¨‹ä¸­è¯»å–é¢„è§ˆæ•°æ®"""
        try:
            # æ–‡ä»¶é¢„æ£€æŸ¥ï¼ˆåŸºç¡€æ£€æŸ¥ + ZIPéªŒè¯ï¼‰
            _validate_excel_file_basic(filepath, max_file_size_mb=max_file_size_mb)
            file_ext = Path(filepath).suffix.lower()
            if file_ext == '.xlsx':
                _validate_xlsx_format(filepath, timeout=0.5)
                # è¡Œæ•°æ£€æŸ¥ï¼ˆè¶…è¿‡é…ç½®çš„æœ€å¤§è¡Œæ•°ç›´æ¥æ‹’ç»ï¼‰
                try:
                    max_rows_value = max_excel_rows if max_excel_rows is not None else 10000
                    _validate_excel_row_count(filepath, sheet_name=sheet_name, max_rows=max_rows_value, timeout=5.0)
                except ValueError as row_error:
                    # æ•è·è¡Œæ•°æ£€æŸ¥å¼‚å¸¸ï¼Œè½¬æ¢ä¸ºå¯è¯†åˆ«çš„å¼‚å¸¸
                    if "è¡Œæ•°è¿‡å¤š" in str(row_error) or "è¶…è¿‡é™åˆ¶" in str(row_error):
                        # é‡æ–°æŠ›å‡ºï¼Œä½†æ ‡è®°ä¸ºè¡Œæ•°é™åˆ¶é”™è¯¯
                        raise ValueError(f"Excelæ–‡ä»¶è¡Œæ•°è¶…è¿‡é™åˆ¶: {str(row_error)}") from row_error
                    raise
            
            # ä½¿ç”¨ read_only=True æ¨¡å¼ï¼Œæ›´å¿«ä¸”æ›´è½»é‡
            wb = load_workbook(filepath, data_only=True, read_only=True)
            
            # é€‰æ‹©å·¥ä½œè¡¨
            if sheet_name:
                ws = wb[sheet_name]
            else:
                if not wb.sheetnames:
                    raise ValueError("Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨")
                ws = wb[wb.sheetnames[0]]
            
            # ç¡®å®šå®é™…è¯»å–èŒƒå›´
            actual_max_col = min(ws.max_column, max_cols)
            actual_max_row = min(ws.max_row, max_rows)
            max_col = ws.max_column  # ä¿å­˜æ€»åˆ—æ•°
            
            # è¯»å–æ•°æ®
            data = []
            for row in range(1, actual_max_row + 1):
                row_data = []
                for col in range(1, actual_max_col + 1):
                    value = ws.cell(row, col).value
                    row_data.append(value)
                data.append(row_data)
            
            wb.close()
            return data, max_col
        except Exception as e:
            logger.error(f"è¯»å–Excelé¢„è§ˆæ•°æ®å¤±è´¥: {filepath}, é”™è¯¯: {e}")
            raise
    
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_read_preview)
            try:
                data, max_col = future.result(timeout=timeout)
                return data, max_col
            except FutureTimeoutError:
                logger.error(f"è¯»å–Excelé¢„è§ˆæ•°æ®è¶…æ—¶: {filepath} (è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
                future.cancel()
                raise TimeoutError(f"è¯»å–Excelé¢„è§ˆæ•°æ®è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰: {filepath}")
    except Exception as e:
        if isinstance(e, TimeoutError):
            raise
        # å¦‚æœæ˜¯è¡Œæ•°æ£€æŸ¥å¼‚å¸¸ï¼Œé‡æ–°æŠ›å‡ºä»¥ä¾¿ä¸Šå±‚å¤„ç†
        if isinstance(e, ValueError) and ("è¡Œæ•°è¶…è¿‡é™åˆ¶" in str(e) or "è¡Œæ•°è¿‡å¤š" in str(e)):
            raise
        logger.error(f"è¯»å–Excelé¢„è§ˆæ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {filepath}, é”™è¯¯: {e}")
        raise


def _analyze_header_with_llm_lightweight(preview_data: List[List[Any]], max_col: int,
                                         llm_api_key: Optional[str] = None,
                                         llm_base_url: Optional[str] = None,
                                         llm_model: Optional[str] = None,
                                         timeout: Optional[int] = None,
                                         thinking_callback: Optional[callable] = None) -> Tuple[HeaderAnalysis, str]:
    """
    ä½¿ç”¨LLMåˆ†æè¡¨å¤´ç»“æ„ï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼Œä¸éœ€è¦SmartHeaderProcessorï¼‰
    
    å‚æ•°:
        preview_data: é¢„è§ˆæ•°æ®åˆ—è¡¨
        max_col: æœ€å¤§åˆ—æ•°
        llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
        llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        thinking_callback: ç”¨äºæµå¼è¾“å‡º thinking å†…å®¹çš„å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        (åˆ†æç»“æœ, LLMåŸå§‹å“åº”)
    """
    # æ„å»ºæç¤ºè¯
    num_cols = len(preview_data[0]) if preview_data else 0
    num_rows = len(preview_data)
    
    # æ„å»ºç®€å•çš„è¡¨æ ¼å­—ç¬¦ä¸²
    table_str = "ã€ExcelåŸå§‹æ•°æ®ã€‘ï¼ˆå‰15è¡Œï¼Œå‰25åˆ—ï¼‰\n\n"
    table_str += "è¡Œå· | " + " | ".join([f"åˆ—{i+1}" for i in range(num_cols)]) + "\n"
    table_str += "-" * (8 + num_cols * 15) + "\n"
    
    for i, row in enumerate(preview_data, 1):
        row_str = " | ".join([str(cell) if cell is not None else "" for cell in row])
        table_str += f"  {i:2d}  | {row_str}\n"
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªExcelè¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹Excelè¡¨æ ¼çš„åŸå§‹æ•°æ®ï¼Œè¯†åˆ«è¡¨å¤´ç»“æ„ã€‚

{table_str}

ã€æ€»åˆ—æ•°ã€‘{max_col}

## åˆ†æä»»åŠ¡

è¯·åˆ†æè¡¨æ ¼ç»“æ„ï¼Œè¯†åˆ«ï¼š
1. **æ— æ•ˆè¡Œï¼ˆskip_rowsï¼‰**ï¼šè¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œï¼ˆå¦‚æ–‡æ¡£æ ‡é¢˜ã€è¯´æ˜æ–‡å­—ã€æ³¨é‡Šã€å…¬å¸åç§°ã€å¡«æŠ¥è¯´æ˜ç­‰ï¼‰
2. **è¡¨å¤´è¡Œæ•°ï¼ˆheader_rowsï¼‰**ï¼šæ‰€æœ‰è¡¨å¤´è¡Œï¼ŒåŒ…æ‹¬å¤šçº§è¡¨å¤´çš„æ‰€æœ‰å±‚çº§
3. **è¡¨å¤´ç±»å‹ï¼ˆheader_typeï¼‰**ï¼šsingleï¼ˆå•è¡¨å¤´ï¼‰æˆ– multiï¼ˆå¤šçº§è¡¨å¤´ï¼‰
4. **æ•°æ®èµ·å§‹è¡Œï¼ˆdata_start_rowï¼‰**ï¼šæ•°æ®å¼€å§‹çš„è¡Œå·ï¼Œå¿…é¡»ç­‰äº skip_rows + header_rows + 1
5. **æ•°æ®èµ·å§‹åˆ—ï¼ˆstart_colï¼‰**ï¼šç¬¬ä¸€ä¸ªè¡¨å¤´è¡Œä¸­ç¬¬ä¸€ä¸ªéç©ºè¡¨å¤´å¼€å§‹çš„åˆ—å·

## è¯†åˆ«è§„åˆ™

### æ— æ•ˆè¡Œç‰¹å¾ï¼š
- æ–‡æ¡£æ ‡é¢˜ï¼ˆå¦‚"2024å¹´åº¦æŠ¥è¡¨"ï¼‰
- å…¬å¸åç§°æˆ–éƒ¨é—¨åç§°ï¼ˆå¦‚"XXå…¬å¸"ã€"XXéƒ¨é—¨"ï¼‰
- å¡«æŠ¥è¯´æ˜ï¼ˆå¦‚"å¡«æŠ¥æœºæ„"ã€"å¡«æŠ¥æ—¥æœŸ"ã€"å¡«æŠ¥æœºæ„/æ—¥æœŸ"ç­‰ï¼Œä»»ä½•åŒ…å«"å¡«æŠ¥"å…³é”®è¯çš„è¡Œï¼‰
- åªæœ‰æ•°å­—æ²¡æœ‰æ ‡ç­¾çš„è¡Œï¼ˆå¦‚åªæœ‰"222"ã€"111"ç­‰æ•°å­—ï¼Œæ²¡æœ‰å¯¹åº”çš„åˆ—åï¼‰
- å®Œå…¨ç©ºè¡Œæˆ–åªæœ‰å°‘é‡æ–‡æœ¬çš„è¡Œ

### è¡¨å¤´è¡Œç‰¹å¾ï¼š
- åŒ…å«åˆ—åæˆ–åˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚"é”€å”®äº‹ä¸šéƒ¨"ã€"åä¸œå¤§åŒº"ã€"çº¿ä¸Šé”€å”®é¢"ç­‰ï¼‰
- æœ‰æ˜ç¡®çš„å±‚çº§ç»“æ„ï¼ˆå¤šçº§è¡¨å¤´ï¼‰
- é€šå¸¸ä¸åŒ…å«å¤§é‡æ•°å€¼æ•°æ®

### æ•°æ®è¡Œç‰¹å¾ï¼š
- åŒ…å«å¤§é‡æ•°å€¼æ•°æ®
- ä¸å†æ˜¯è¡¨å¤´æ–‡æœ¬æˆ–åˆ†ç±»æ ‡ç­¾

## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

```json
{{
    "skip_rows": <è¡¨å¤´ä¹‹å‰çš„æ— æ•ˆè¡Œæ•°ï¼Œå¦‚æœç¬¬1è¡Œå°±æ˜¯è¡¨å¤´åˆ™å¡«0>,
    "header_rows": <è¡¨å¤´å ç”¨çš„æ€»è¡Œæ•°>,
    "header_type": "<singleæˆ–multi>",
    "data_start_row": <æ•°æ®å¼€å§‹è¡Œå·ï¼ˆ1-indexedï¼‰ï¼Œå¿…é¡»ç­‰äºskip_rows+header_rows+1>,
    "start_col": <æ•°æ®èµ·å§‹åˆ—å·ï¼ˆ1-indexedï¼‰>,
    "valid_cols": null,
    "confidence": "<high/medium/low>",
    "reason": "<è¯¦ç»†è¯´æ˜è¯†åˆ«è¿‡ç¨‹>"
}}
```

## æ³¨æ„äº‹é¡¹

1. è¡Œå·å’Œåˆ—å·éƒ½ä»1å¼€å§‹è®¡æ•°
2. data_start_row å¿…é¡»ç­‰äº skip_rows + header_rows + 1
3. valid_cols å§‹ç»ˆè®¾ä¸º null
4. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹
5. å¦‚æœç¬¬1è¡Œå°±æ˜¯è¡¨å¤´ï¼Œåˆ™ skip_rows=0
6. å¤šçº§è¡¨å¤´çš„æ‰€æœ‰è¡Œéƒ½è¦è®¡å…¥ header_rows

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
    
    # è°ƒç”¨LLM
    from .config import EXCEL_LLM_API_KEY, EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL
    api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
    if not api_key:
        raise ValueError("LLM APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒExcelåˆ†æã€‚è¯·é…ç½®EXCEL_LLM_API_KEYæˆ–ä¼ å…¥llm_api_keyå‚æ•°")
    
    # è°ƒç”¨LLM API
    result = _call_llm_api(prompt, api_key, llm_base_url or EXCEL_LLM_BASE_URL, llm_model or EXCEL_LLM_MODEL, timeout=timeout, thinking_callback=thinking_callback)
    
    if not result:
        raise ValueError("LLMåˆ†æå¤±è´¥ï¼šæ— æ³•è·å–LLMå“åº”ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
    
    # è§£æLLMåˆ†æç»“æœ
    analysis = _parse_llm_analysis_response_lightweight(result)
    
    return analysis, result


def _call_llm_api(prompt: str, 
                  llm_api_key: str,
                  llm_base_url: Optional[str] = None,
                  llm_model: Optional[str] = None,
                  timeout: Optional[int] = None,
                  thinking_callback: Optional[callable] = None) -> str:
    """
    è°ƒç”¨LLM APIï¼ˆç‹¬ç«‹å‡½æ•°ï¼Œä¸ä¾èµ–SmartHeaderProcessorï¼‰
    """
    from .config import EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL
    
    base_url = llm_base_url or EXCEL_LLM_BASE_URL
    model = llm_model or EXCEL_LLM_MODEL
    
    if not base_url:
        raise ValueError("LLM APIåœ°å€æœªé…ç½®ï¼Œè¯·é…ç½®EXCEL_LLM_BASE_URLæˆ–ä¼ å…¥llm_base_urlå‚æ•°")
    
    if not model:
        raise ValueError("LLMæ¨¡å‹åç§°æœªé…ç½®ï¼Œè¯·é…ç½®EXCEL_LLM_MODELæˆ–ä¼ å…¥llm_modelå‚æ•°")
    
    # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤90ç§’
    request_timeout = timeout if timeout is not None else 90
    
    logger.info(f"â±ï¸ è¶…æ—¶è®¾ç½®: {request_timeout} ç§’")
    
    # æ„å»ºè¯·æ±‚URLå’Œå‚æ•°
    # æ³¨æ„ï¼šbase_url åº”è¯¥å·²ç»åŒ…å«å®Œæ•´çš„è·¯å¾„ï¼ˆå¦‚ /v1/chat/completionsï¼‰ï¼Œç›´æ¥ä½¿ç”¨
    url = base_url
    headers = {
        "Authorization": f"Bearer {llm_api_key}",
        "Content-Type": "application/json"
    }
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "user", "content": prompt}]
    
    # ä½¿ç”¨æµå¼è°ƒç”¨ä»¥æ”¯æŒ thinking åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    base_payload = {
        "model": model,
        "messages": messages,
        "temperature": 0.4,
        "max_tokens": 1000,
        "stream": True,  # æµå¼è°ƒç”¨ï¼ˆå¿…é¡»å¯ç”¨ä»¥æ”¯æŒ thinkingï¼‰
    }
    
    # æä¾›é»˜è®¤å›è°ƒå‡½æ•°
    if thinking_callback is None:
        def default_callback(content: str):
            pass  # ç©ºå›è°ƒï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        thinking_callback = default_callback
    
    logger.info(f"ğŸ“¡ å‘é€ LLM API è¯·æ±‚åˆ°: {url} (æµå¼è°ƒç”¨)")
    logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    
    try:
        # é»˜è®¤å¯ç”¨ thinking åŠŸèƒ½ï¼ˆæµå¼è¾“å‡ºï¼‰
        payload_with_thinking = base_payload.copy()
        payload_with_thinking["enable_thinking"] = True  # é»˜è®¤å¯ç”¨ thinking
        
        logger.info("ğŸ’­ å·²å¯ç”¨ Thinking åŠŸèƒ½ï¼Œå°†å®æ—¶æµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹")
        
        response = requests.post(
            url, 
            headers=headers, 
            json=payload_with_thinking, 
            timeout=request_timeout,
            stream=True  # å¯ç”¨æµå¼å“åº”
        )
        
        # å¦‚æœå¯ç”¨ thinking å¤±è´¥ï¼Œå›é€€åˆ°ä¸ä½¿ç”¨ thinking
        if response.status_code != 200:
            try:
                error_json = response.json()
                if "enable_thinking" in str(error_json).lower():
                    logger.warning("âš ï¸ API ä¸æ”¯æŒ enable_thinking å‚æ•°ï¼Œå°†å›é€€åˆ°ä¸ä½¿ç”¨ thinking")
                    logger.warning("ğŸ’­ æ³¨æ„ï¼šThinking æµå¼è¾“å‡ºå°†ä¸å¯ç”¨ï¼ˆAPI ä¸æ”¯æŒï¼‰")
                    payload_no_thinking = base_payload.copy()
                    response = requests.post(
                        url, 
                        headers=headers, 
                        json=payload_no_thinking, 
                        timeout=request_timeout,
                        stream=True
                    )
            except:
                pass
        
        # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¾“å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if response.status_code != 200:
            error_detail = ""
            try:
                # å¯¹äºæµå¼å“åº”ï¼Œå°è¯•è¯»å–é”™è¯¯ä¿¡æ¯
                error_text = ""
                for line in response.iter_lines():
                    if line:
                        error_text += line.decode('utf-8') + "\n"
                error_detail = error_text
            except:
                try:
                    error_json = response.json()
                    error_detail = json.dumps(error_json, ensure_ascii=False, indent=2)
                except:
                    error_detail = response.text
            
            logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            logger.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {error_detail}")
            response.raise_for_status()
        
        # å¤„ç†æµå¼å“åº”
        full_content = ""
        for line in response.iter_lines():
            if line:
                line_str = line.decode('utf-8')
                if line_str.startswith('data: '):
                    data_str = line_str[6:]
                    if data_str.strip() == '[DONE]':
                        break
                    try:
                        chunk_data = json.loads(data_str)
                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                            delta = chunk_data['choices'][0].get('delta', {})
                            # ä½¿ç”¨ get æ–¹æ³•å¹¶æä¾›é»˜è®¤å€¼ï¼Œé¿å… None å€¼
                            content = delta.get('content', '')
                            if content:
                                full_content += content
                                thinking_callback(content)
                    except json.JSONDecodeError:
                        continue
        
        if not full_content:
            logger.warning("âš ï¸ LLM æµå¼å“åº”ä¸ºç©º")
            return None
        
        logger.info("âœ… LLM API è°ƒç”¨æˆåŠŸ")
        return full_content
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥ (ç½‘ç»œé”™è¯¯): {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_json = e.response.json()
                logger.error(f"ğŸ“‹ API é”™è¯¯å“åº”: {json.dumps(error_json, ensure_ascii=False, indent=2)}")
            except:
                logger.error(f"ğŸ“‹ API é”™è¯¯å“åº” (æ–‡æœ¬): {e.response.text}")
        logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
        return None
    except Exception as e:
        logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
        return None


def _parse_llm_analysis_response_lightweight(response: str) -> HeaderAnalysis:
    """è§£æLLMåˆ†æç»“æœï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼Œä¸ä¾èµ–SmartHeaderProcessorï¼‰"""
    if not response:
        raise ValueError("LLMå“åº”ä¸ºç©º")
    
    try:
        # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
        start_idx = response.find('{')
        end_idx = response.rfind('}')
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_str = response[start_idx:end_idx + 1]
            data = json.loads(json_str)
        else:
            # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
            data = json.loads(json_match.group())
        
        # è§£ææœ‰æ•ˆåˆ—ï¼ˆå§‹ç»ˆä¸ºNoneï¼‰
        valid_cols = None
        
        # è§£æèµ·å§‹åˆ—ï¼ˆé»˜è®¤ä¸º1ï¼‰
        start_col = int(data.get('start_col', 1))
        if start_col < 1:
            start_col = 1
        
        # æ„å»ºHeaderAnalysiså¯¹è±¡
        analysis = HeaderAnalysis(
            skip_rows=int(data.get('skip_rows', 0)),
            header_rows=int(data.get('header_rows', 1)),
            header_type=data.get('header_type', 'single'),
            data_start_row=int(data.get('data_start_row', 1)),
            start_col=start_col,
            confidence=data.get('confidence', 'medium'),
            reason=f"LLMåˆ†æ: {data.get('reason', '')}",
            valid_cols=valid_cols
        )
        
        logger.info(f"âœ… LLMåˆ†æå®Œæˆ:")
        logger.info(f"  - è·³è¿‡è¡Œæ•°: {analysis.skip_rows}")
        logger.info(f"  - è¡¨å¤´è¡Œæ•°: {analysis.header_rows}")
        logger.info(f"  - è¡¨å¤´ç±»å‹: {analysis.header_type}")
        logger.info(f"  - æ•°æ®èµ·å§‹è¡Œ: {analysis.data_start_row}")
        logger.info(f"  - æ•°æ®èµ·å§‹åˆ—: {analysis.start_col}")
        logger.info(f"  - ç½®ä¿¡åº¦: {analysis.confidence}")
        
        return analysis
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        logger.error(f"âŒ è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
        logger.error(f"ğŸ“‹ å“åº”å†…å®¹: {response[:500]}")
        raise ValueError(f"è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")


def _save_csv_with_timeout(df: pd.DataFrame, csv_path: str, timeout: int = 30) -> None:
    """å¸¦è¶…æ—¶ä¿æŠ¤çš„ä¿å­˜CSVæ–‡ä»¶"""
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    def _save():
        """åœ¨åå°çº¿ç¨‹ä¸­ä¿å­˜CSV"""
        try:
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {csv_path}, é”™è¯¯: {e}")
            raise
    
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_save)
            try:
                future.result(timeout=timeout)
            except FutureTimeoutError:
                logger.error(f"ä¿å­˜CSVæ–‡ä»¶è¶…æ—¶: {csv_path} (è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
                future.cancel()
                raise TimeoutError(f"ä¿å­˜CSVæ–‡ä»¶è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰: {csv_path}")
    except Exception as e:
        if isinstance(e, TimeoutError):
            raise
        logger.error(f"ä¿å­˜CSVæ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {csv_path}, é”™è¯¯: {e}")
        raise


def process_excel_file(
    filepath: str,
    output_dir: str,
    sheet_name: str = None,
    use_llm_validate: bool = False,  # å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œç°åœ¨æ€»æ˜¯ä½¿ç”¨LLM
    output_filename: str = None,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    preprocessing_timeout: Optional[int] = None,
    excel_processing_timeout: Optional[int] = None,  # Excelå¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œåœ¨LLMåˆ†æä¹‹å‰
    debug_print_header_analysis: bool = False,  # æ˜¯å¦æµå¼æ‰“å°åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    thinking_callback: Optional[callable] = None,  # ç”¨äºæµå¼è¾“å‡º thinking å†…å®¹çš„å›è°ƒå‡½æ•°
    max_file_size_mb: Optional[int] = None,  # æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    max_rows: Optional[int] = None  # æœ€å¤§è¡Œæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼10000
) -> ExcelProcessResult:
    """
    å¤„ç†Excelæ–‡ä»¶çš„ä¸»å‡½æ•°
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        sheet_name: å·¥ä½œè¡¨åç§°
        use_llm_validate: å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ã€‚ç°åœ¨æ€»æ˜¯ä½¿ç”¨LLMè¿›è¡Œåˆ†æ
        output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼Œå¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        excel_processing_timeout: Excelå¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’ï¼ˆåŒ…æ‹¬æ–‡ä»¶åŠ è½½å’Œæ•°æ®è¯»å–ï¼‰
        debug_print_header_analysis: æ˜¯å¦æµå¼æ‰“å°åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰ï¼Œé»˜è®¤False
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    è¿”å›:
        ExcelProcessResult
    
    æ³¨æ„:
        ç°åœ¨å¿…é¡»ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼Œä¸å†æ”¯æŒè§„åˆ™åˆ†æã€‚è¯·ç¡®ä¿æä¾›llm_api_keyå‚æ•°ã€‚
    """
    try:
        logger.info(f"ğŸš€ [DEBUG] process_excel_file: å¼€å§‹å¤„ç†æ–‡ä»¶ {filepath}")
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        logger.info(f"ğŸ“ [DEBUG] process_excel_file: ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨: {output_dir}")
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"âœ… [DEBUG] process_excel_file: è¾“å‡ºç›®å½•å·²å‡†å¤‡")
        
        # è®¾ç½®è¶…æ—¶æ—¶é—´
        excel_processing_timeout_seconds = excel_processing_timeout if excel_processing_timeout is not None else 10
        logger.info(f"â±ï¸ [DEBUG] process_excel_file: Excelå¤„ç†è¶…æ—¶æ—¶é—´: {excel_processing_timeout_seconds}ç§’")
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨è½»é‡çº§æ–¹å¼è·å–é¢„è§ˆæ•°æ®å¹¶è¿›è¡ŒLLMåˆ†æï¼ˆä¸éœ€è¦åˆ›å»ºSmartHeaderProcessorï¼‰
        logger.info(f"ğŸ“‚ [DEBUG] process_excel_file: å¼€å§‹è·å–é¢„è§ˆæ•°æ®ï¼ˆè½»é‡çº§æ¨¡å¼ï¼‰")
        logger.info(f"ğŸ“‚ [DEBUG] process_excel_file: æ–‡ä»¶è·¯å¾„: {filepath}, å·¥ä½œè¡¨: {sheet_name}")
        
        try:
            preview_data, max_col = _get_preview_data_lightweight(
                filepath, 
                sheet_name, 
                max_rows=15, 
                max_cols=25, 
                timeout=excel_processing_timeout_seconds,
                max_file_size_mb=max_file_size_mb,
                max_excel_rows=max_rows
            )
            logger.info(f"âœ… [DEBUG] process_excel_file: é¢„è§ˆæ•°æ®è·å–å®Œæˆï¼Œå…± {len(preview_data)} è¡Œï¼Œ{max_col} åˆ—")
        except TimeoutError as e:
            error_msg = f"è·å–Excelé¢„è§ˆæ•°æ®è¶…æ—¶: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return ExcelProcessResult(
                success=False,
                header_analysis=None,
                processed_file_path=None,
                metadata_file_path=None,
                column_names=[],
                column_metadata={},
                row_count=0,
                error_message=error_msg
            )
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯è¡Œæ•°æ£€æŸ¥å¼‚å¸¸ï¼ˆå› ä¸ºå¯èƒ½æ˜¯ä»çº¿ç¨‹æ± æŠ›å‡ºçš„ï¼‰
            error_str = str(e)
            error_type = type(e).__name__
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¡Œæ•°ç›¸å…³çš„é”™è¯¯ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            is_row_limit_error = (
                "è¡Œæ•°è¶…è¿‡é™åˆ¶" in error_str or 
                "è¡Œæ•°è¿‡å¤š" in error_str or 
                ("è¶…è¿‡é™åˆ¶" in error_str and "è¡Œ" in error_str) or
                (isinstance(e, ValueError) and "è¡Œæ•°" in error_str and "é™åˆ¶" in error_str)
            )
            
            if is_row_limit_error:
                # æå–è¡Œæ•°ä¿¡æ¯
                import re
                match = re.search(r'(\d+)\s*è¡Œ', error_str)
                row_count = match.group(1) if match else "æœªçŸ¥"
                match_limit = re.search(r'é™åˆ¶[ï¼ˆ(](\d+)\s*è¡Œ', error_str)
                if not match_limit:
                    match_limit = re.search(r'è¶…è¿‡é™åˆ¶[ï¼ˆ(](\d+)\s*è¡Œ', error_str)
                limit = match_limit.group(1) if match_limit else (max_rows if max_rows else 10000)
                error_msg = f"Excelæ–‡ä»¶è¡Œæ•°è¿‡å¤šï¼ˆ{row_count} è¡Œï¼‰ï¼Œè¶…è¿‡é™åˆ¶ï¼ˆ{limit} è¡Œï¼‰ã€‚è¯·å‡å°‘æ–‡ä»¶è¡Œæ•°æˆ–è°ƒæ•´é…ç½®ä¸­çš„æœ€å¤§è¡Œæ•°é™åˆ¶ã€‚"
                logger.warning(f"âš ï¸ {error_msg}")
                return ExcelProcessResult(
                    success=False,
                    header_analysis=None,
                    processed_file_path=None,
                    metadata_file_path=None,
                    column_names=[],
                    column_metadata={},
                    row_count=0,
                    error_message=error_msg
                )
            # å…¶ä»–å¼‚å¸¸ç»§ç»­æŠ›å‡º
            logger.error(f"âŒ è·å–Excelé¢„è§ˆæ•°æ®æ—¶å‘ç”Ÿæœªå¤„ç†çš„å¼‚å¸¸: {error_type}: {error_str}")
            raise
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨é¢„è§ˆæ•°æ®è¿›è¡ŒLLMåˆ†æ
        logger.info("ğŸ¤– å¼€å§‹LLMè¡¨å¤´åˆ†æï¼ˆä½¿ç”¨é¢„è§ˆæ•°æ®ï¼‰...")
        try:
            analysis, llm_response = _analyze_header_with_llm_lightweight(
                preview_data,
                max_col,
                llm_api_key=llm_api_key,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
                timeout=preprocessing_timeout,
                thinking_callback=thinking_callback
            )
            logger.info("âœ… LLMè¡¨å¤´åˆ†æå®Œæˆ")
        except Exception as e:
            error_msg = f"LLMè¡¨å¤´åˆ†æå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return ExcelProcessResult(
                success=False,
                header_analysis=None,
                processed_file_path=None,
                metadata_file_path=None,
                column_names=[],
                column_metadata={},
                row_count=0,
                error_message=error_msg
            )
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºSmartHeaderProcessoræ¥è¯»å–å®Œæ•´æ•°æ®ï¼ˆåªæœ‰åœ¨éœ€è¦è¯»å–å®Œæ•´æ•°æ®æ—¶æ‰åˆ›å»ºï¼‰
        logger.info(f"ğŸ“‚ [DEBUG] process_excel_file: å¼€å§‹åˆ›å»º SmartHeaderProcessorï¼ˆç”¨äºè¯»å–å®Œæ•´æ•°æ®ï¼‰")
        load_timeout = excel_processing_timeout_seconds
        read_timeout = excel_processing_timeout_seconds
        
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
        
        def _create_processor():
            """åœ¨åå°çº¿ç¨‹ä¸­åˆ›å»º SmartHeaderProcessor"""
            try:
                return SmartHeaderProcessor(
                    filepath, 
                    sheet_name, 
                    load_timeout=load_timeout, 
                    read_timeout=read_timeout,
                    debug_print_header_analysis=debug_print_header_analysis,
                    max_file_size_mb=max_file_size_mb,
                    max_rows=max_rows
                )
            except Exception as e:
                logger.error(f"åˆ›å»º SmartHeaderProcessor å¤±è´¥: {e}")
                raise
        
        try:
            # ä½¿ç”¨æ€»è¶…æ—¶æ—¶é—´ï¼ˆexcel_processing_timeout_secondsï¼‰æ¥ä¿æŠ¤æ•´ä¸ªåˆå§‹åŒ–è¿‡ç¨‹
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_create_processor)
                try:
                    processor = future.result(timeout=excel_processing_timeout_seconds)
                    logger.info("âœ… [DEBUG] process_excel_file: SmartHeaderProcessor åˆ›å»ºå®Œæˆ")
                except FutureTimeoutError:
                    logger.error(f"åˆ›å»º SmartHeaderProcessor è¶…æ—¶: {filepath} (è¶…æ—¶æ—¶é—´: {excel_processing_timeout_seconds}ç§’)")
                    future.cancel()
                    error_msg = f"Excelæ–‡ä»¶å¤„ç†è¶…æ—¶ï¼ˆ{excel_processing_timeout_seconds}ç§’ï¼‰: {filepath}"
                    logger.error(f"âŒ {error_msg}")
                    return ExcelProcessResult(
                        success=False,
                        header_analysis=analysis,  # ä¿ç•™å·²å®Œæˆçš„LLMåˆ†æç»“æœ
                        processed_file_path=None,
                        metadata_file_path=None,
                        column_names=[],
                        column_metadata={},
                        row_count=0,
                        error_message=error_msg
                    )
        except TimeoutError as e:
            error_msg = f"Excelæ–‡ä»¶åŠ è½½è¶…æ—¶: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return ExcelProcessResult(
                success=False,
                header_analysis=analysis,  # ä¿ç•™å·²å®Œæˆçš„LLMåˆ†æç»“æœ
                processed_file_path=None,
                metadata_file_path=None,
                column_names=[],
                column_metadata={},
                row_count=0,
                error_message=error_msg
            )
        
        # ç¬¬å››æ­¥ï¼šä½¿ç”¨SmartHeaderProcessorè¯»å–å®Œæ•´æ•°æ®å¹¶è½¬æ¢ä¸ºDataFrame
        logger.info("ğŸ“Š å¼€å§‹è¯»å–å®Œæ•´æ•°æ®å¹¶è½¬æ¢ä¸ºDataFrame...")
        df, _, column_metadata, _ = processor.to_dataframe(
            analysis=analysis,  # ä½¿ç”¨å·²å®Œæˆçš„LLMåˆ†æç»“æœ
            use_llm_validate=False,  # ä¸å†éœ€è¦LLMåˆ†æï¼Œå› ä¸ºå·²ç»å®Œæˆäº†
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            preprocessing_timeout=preprocessing_timeout,
            thinking_callback=thinking_callback
        )
        logger.info("âœ… å®Œæ•´æ•°æ®è¯»å–å®Œæˆ")
        processor.close()
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_filename:
            base_name = Path(filepath).stem
            output_filename = f"{base_name}_processed"
        
        # ä¿å­˜CSVï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        csv_path = os.path.join(output_dir, f"{output_filename}.csv")
        try:
            _save_csv_with_timeout(df, csv_path, timeout=30)
        except TimeoutError as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶è¶…æ—¶: {csv_path}")
            raise ValueError(f"ä¿å­˜CSVæ–‡ä»¶è¶…æ—¶: {str(e)}")
        
        # æå–å­—æ®µå€¼æ ·æœ¬ï¼ˆåˆ†ç»„èšåˆåçš„å¸¸è§å€¼ï¼‰
        logger.info("ğŸ“Š æå–å­—æ®µå€¼æ ·æœ¬...")
        column_value_samples = extract_column_value_samples(df, max_samples_per_column=10)
        
        # å°†å€¼æ ·æœ¬ä¿¡æ¯åˆå¹¶åˆ°åˆ—å…ƒæ•°æ®ä¸­
        for col_name, samples in column_value_samples.items():
            if col_name in column_metadata:
                column_metadata[col_name]["value_samples"] = samples
            else:
                # å¦‚æœåˆ—ä¸åœ¨å…ƒæ•°æ®ä¸­ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œåˆ›å»ºæ–°çš„å…ƒæ•°æ®é¡¹
                column_metadata[col_name] = {"value_samples": samples}
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "header_analysis": analysis.to_dict(),
            "column_metadata": column_metadata,
            "column_names": list(df.columns),
            "row_count": len(df),
            "original_file": os.path.basename(filepath)
        }
        metadata_path = os.path.join(output_dir, f"{output_filename}_metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å¤„ç†åçš„JSONå…ƒæ•°æ®ï¼ˆæš‚æ—¶æ³¨é‡Šï¼‰
        # logger.info("=" * 80)
        # logger.info("ğŸ“„ å¤„ç†åçš„JSONå…ƒæ•°æ®:")
        # logger.info("=" * 80)
        # logger.info(json.dumps(metadata, ensure_ascii=False, indent=2))
        # logger.info("=" * 80)
        
        return ExcelProcessResult(
            success=True,
            header_analysis=analysis,
            processed_file_path=csv_path,
            metadata_file_path=metadata_path,
            column_names=list(df.columns),
            column_metadata=column_metadata,
            row_count=len(df),
            error_message=None,
            llm_analysis_response=llm_response
        )
        
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return ExcelProcessResult(
            success=False,
            header_analysis=None,
            processed_file_path=None,
            metadata_file_path=None,
            column_names=[],
            column_metadata={},
            row_count=0,
            error_message=error_msg
        )


def get_sheet_names(filepath: str, timeout: int = 10, max_file_size_mb: Optional[int] = None, max_rows: Optional[int] = None) -> List[str]:
    """è·å–Excelæ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œè¡¨åç§°ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
    
    Args:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    Returns:
        å·¥ä½œè¡¨åç§°åˆ—è¡¨ï¼Œå¦‚æœè¶…æ—¶æˆ–å‡ºé”™åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    import threading
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    logger.info(f"ğŸ“‹ [DEBUG] get_sheet_names: å¼€å§‹è·å–å·¥ä½œè¡¨åç§°ï¼Œæ–‡ä»¶: {os.path.basename(filepath)}, è¶…æ—¶: {timeout}ç§’")
    print(f"ğŸ” [DEBUG] get_sheet_names: å¼€å§‹è·å–å·¥ä½œè¡¨åç§°ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
    sys.stdout.flush()
    
    def _load_sheets():
        """åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½å·¥ä½œè¡¨åç§°"""
        try:
            # æ–‡ä»¶é¢„æ£€æŸ¥ï¼ˆåŸºç¡€æ£€æŸ¥ + ZIPéªŒè¯ï¼‰
            _validate_excel_file_basic(filepath, max_file_size_mb=max_file_size_mb)
            file_ext = Path(filepath).suffix.lower()
            if file_ext == '.xlsx':
                _validate_xlsx_format(filepath, timeout=0.5)
                # è¡Œæ•°æ£€æŸ¥ï¼ˆè¶…è¿‡é…ç½®çš„æœ€å¤§è¡Œæ•°ç›´æ¥æ‹’ç»ï¼‰
                try:
                    max_rows_value = max_rows if max_rows is not None else 10000
                    _validate_excel_row_count(filepath, sheet_name=None, max_rows=max_rows_value, timeout=5.0)
                except ValueError as row_error:
                    # æ•è·è¡Œæ•°æ£€æŸ¥å¼‚å¸¸ï¼Œè®°å½•è­¦å‘Šä½†ä¸æŠ›å‡ºï¼ˆget_sheet_names è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºå¤±è´¥ï¼‰
                    if "è¡Œæ•°è¿‡å¤š" in str(row_error) or "è¶…è¿‡é™åˆ¶" in str(row_error):
                        logger.warning(f"âš ï¸ Excelæ–‡ä»¶è¡Œæ•°è¶…è¿‡é™åˆ¶ï¼Œæ— æ³•è·å–å·¥ä½œè¡¨åˆ—è¡¨: {str(row_error)}")
                        return []  # è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºå¤±è´¥
                    raise
            
            logger.info(f"ğŸ“‚ [DEBUG] get_sheet_names: å¼€å§‹åŠ è½½å·¥ä½œç°¿...")
            print(f"ğŸ” [DEBUG] get_sheet_names: å¼€å§‹åŠ è½½å·¥ä½œç°¿ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
            sys.stdout.flush()
            wb = load_workbook(filepath)
            logger.info(f"âœ… [DEBUG] get_sheet_names: å·¥ä½œç°¿åŠ è½½å®Œæˆ")
            print(f"ğŸ” [DEBUG] get_sheet_names: å·¥ä½œç°¿åŠ è½½å®Œæˆï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
            sys.stdout.flush()
            sheets = wb.sheetnames
            logger.info(f"ğŸ“‹ [DEBUG] get_sheet_names: è·å–åˆ°å·¥ä½œè¡¨: {sheets}")
            wb.close()
            logger.info(f"âœ… [DEBUG] get_sheet_names: å·¥ä½œç°¿å·²å…³é—­")
            return sheets
        except Exception as e:
            logger.warning(f"âŒ [DEBUG] get_sheet_names: è¯»å–Excelå·¥ä½œè¡¨å¤±è´¥: {filepath}, é”™è¯¯: {e}", exc_info=True)
            return []
    
    try:
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œï¼Œå¸¦è¶…æ—¶ä¿æŠ¤
        logger.info(f"ğŸ”„ [DEBUG] get_sheet_names: å‡†å¤‡åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ...")
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_load_sheets)
            try:
                logger.info(f"â³ [DEBUG] get_sheet_names: ç­‰å¾…ç»“æœï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’...")
                sheets = future.result(timeout=timeout)
                logger.info(f"âœ… [DEBUG] get_sheet_names: è·å–å·¥ä½œè¡¨åç§°æˆåŠŸ: {sheets}")
                return sheets if sheets else []
            except FutureTimeoutError:
                logger.error(f"è·å–Excelå·¥ä½œè¡¨åç§°è¶…æ—¶: {filepath} (è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
                # å°è¯•å–æ¶ˆä»»åŠ¡ï¼ˆä½†å¯èƒ½å·²ç»æ‰§è¡Œäº†ï¼‰
                future.cancel()
                return []
    except Exception as e:
        logger.error(f"è·å–Excelå·¥ä½œè¡¨åç§°æ—¶å‘ç”Ÿå¼‚å¸¸: {filepath}, é”™è¯¯: {e}")
        return []


def extract_column_value_samples(
    df: pd.DataFrame,
    max_samples_per_column: int = 10,
    max_unique_ratio: float = 0.5
) -> Dict[str, Dict[str, Any]]:
    """
    æå–æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼æ ·æœ¬ï¼ˆé€šè¿‡åˆ†ç»„èšåˆï¼‰
    
    å‚æ•°:
        df: æ•°æ®æ¡†
        max_samples_per_column: æ¯ä¸ªå­—æ®µæœ€å¤šä¿ç•™çš„æ ·æœ¬æ•°é‡
        max_unique_ratio: å¦‚æœå”¯ä¸€å€¼å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹ï¼Œåˆ™åªæä¾›ç»Ÿè®¡ä¿¡æ¯è€Œä¸ç»Ÿè®¡é¢‘ç‡
    
    è¿”å›:
        å­—å…¸ï¼Œkeyä¸ºåˆ—åï¼Œvalueä¸ºåŒ…å«å¸¸è§å€¼å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    column_samples = {}
    
    for col_name in df.columns:
        col_data = df[col_name]
        
        # è·³è¿‡å®Œå…¨ä¸ºç©ºçš„åˆ—
        if col_data.isna().all():
            continue
        
        # è®¡ç®—éç©ºå€¼æ•°é‡
        non_null_count = col_data.notna().sum()
        if non_null_count == 0:
            continue
        
        # è®¡ç®—å”¯ä¸€å€¼æ•°é‡
        unique_count = col_data.nunique()
        unique_ratio = unique_count / non_null_count if non_null_count > 0 else 1.0
        
        sample_info = {
            "total_count": len(col_data),
            "non_null_count": int(non_null_count),
            "null_count": int(col_data.isna().sum()),
            "unique_count": int(unique_count),
            "data_type": str(col_data.dtype)
        }
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
        is_numeric = pd.api.types.is_numeric_dtype(col_data)
        
        if is_numeric:
            # æ•°å€¼ç±»å‹ï¼šæä¾›ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼ï¼ˆå¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼‰
            sample_info["is_numeric"] = True
            non_null_data = col_data.dropna()
            if len(non_null_data) > 0:
                sample_info["min"] = float(non_null_data.min())
                sample_info["max"] = float(non_null_data.max())
                sample_info["mean"] = float(non_null_data.mean())
                sample_info["median"] = float(non_null_data.median())
            else:
                sample_info["min"] = None
                sample_info["max"] = None
                sample_info["mean"] = None
                sample_info["median"] = None
            
            # å¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œä¹Ÿç»Ÿè®¡é¢‘ç‡
            if unique_ratio <= max_unique_ratio and unique_count <= 100:
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
            elif unique_count <= max_samples_per_column:
                # å³ä½¿å”¯ä¸€å€¼æ¯”ä¾‹é«˜ï¼Œä½†å¦‚æœæ€»æ•°ä¸å¤šï¼Œä¹Ÿå±•ç¤ºæ‰€æœ‰å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œå±•ç¤ºæ‰€æœ‰å€¼"
        else:
            # éæ•°å€¼ç±»å‹ï¼šç»Ÿè®¡é¢‘ç‡
            sample_info["is_numeric"] = False
            
            # å¦‚æœå”¯ä¸€å€¼å¤ªå¤šï¼Œåªæä¾›ç»Ÿè®¡ä¿¡æ¯
            if unique_ratio > max_unique_ratio:
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œä»…å±•ç¤ºéƒ¨åˆ†å¸¸è§å€¼"
                # ä»ç„¶å±•ç¤ºå‰Nä¸ªæœ€å¸¸è§çš„å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
            else:
                # å”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œç»Ÿè®¡æ‰€æœ‰å€¼çš„é¢‘ç‡
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
        
        column_samples[col_name] = sample_info
    
    return column_samples


def _build_column_hierarchy_tree(column_metadata: Dict[str, Dict]) -> str:
    """
    æ„å»ºåˆ—å±‚çº§ç»“æ„çš„æ ‘å½¢å±•ç¤º
    
    å‚æ•°:
        column_metadata: åˆ—å…ƒæ•°æ®å­—å…¸
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æ ‘å½¢ç»“æ„å­—ç¬¦ä¸²
    """
    if not column_metadata:
        return ""
    
    # æ„å»ºæ ‘å½¢ç»“æ„
    tree = {}
    
    for col_name, meta in column_metadata.items():
        # è·å–æ‰€æœ‰å±‚çº§
        levels = []
        level_keys = sorted([k for k in meta.keys() if k.startswith('level')], 
                          key=lambda x: int(x.replace('level', '')))
        for level_key in level_keys:
            value = meta.get(level_key)
            if value and str(value).strip():
                levels.append(str(value).strip())
        
        # å¦‚æœæ²¡æœ‰å±‚çº§ä¿¡æ¯ï¼Œä½¿ç”¨åˆ—åæœ¬èº«
        if not levels:
            levels = [col_name]
        
        # æ„å»ºæ ‘
        current = tree
        for i, level_value in enumerate(levels):
            if level_value not in current:
                current[level_value] = {}
            current = current[level_value]
    
    # é€’å½’ç”Ÿæˆæ ‘å½¢å­—ç¬¦ä¸²
    def _format_tree(node: Dict, prefix: str = "", is_last: bool = True, depth: int = 0) -> List[str]:
        lines = []
        items = list(node.items())
        
        for idx, (key, children) in enumerate(items):
            is_last_item = (idx == len(items) - 1)
            current_prefix = "â””â”€ " if is_last_item else "â”œâ”€ "
            
            if children:
                # æœ‰å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
                next_prefix = prefix + ("   " if is_last_item else "â”‚  ")
                child_lines = _format_tree(children, next_prefix, is_last_item, depth + 1)
                lines.extend(child_lines)
            else:
                # å¶å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
        
        return lines
    
    tree_lines = _format_tree(tree)
    return "\n".join(tree_lines)


def generate_analysis_prompt(
    process_result: ExcelProcessResult,
    custom_prompt: str = None,
    include_metadata: bool = True
) -> str:
    """
    æ ¹æ®Excelå¤„ç†ç»“æœç”Ÿæˆæ•°æ®åˆ†ææç¤ºè¯
    
    å‚æ•°:
        process_result: Excelå¤„ç†ç»“æœ
        custom_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        include_metadata: æ˜¯å¦åŒ…å«åˆ—ç»“æ„å…ƒæ•°æ®
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æç¤ºè¯
    """
    if not process_result.success:
        return ""
    
    # åŸºç¡€ä¿¡æ¯
    prompt_parts = []
    
    # æ·»åŠ è¯­è¨€è¦æ±‚ï¼ˆå¿…é¡»åœ¨æœ€å‰é¢ï¼‰
    prompt_parts.append("**é‡è¦è¦æ±‚ï¼šè¯·ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æå’Œå›ç­”ï¼ŒåŒ…æ‹¬ä»£ç æ³¨é‡Šã€åˆ†ææŠ¥å‘Šç­‰æ‰€æœ‰å†…å®¹ã€‚**")
    prompt_parts.append("")
    prompt_parts.append("**ç¦æ­¢è¦æ±‚ï¼šè¯·ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š**")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ matplotlibã€plotlyã€seaborn ç­‰ç»˜å›¾åº“")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ plt.figure()ã€plt.plot()ã€plt.savefig() ç­‰ç»˜å›¾å‡½æ•°")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ .plot()ã€.hist() ç­‰ pandas ç»˜å›¾æ–¹æ³•")
    prompt_parts.append("- ä¸è¦ä¿å­˜ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼ˆ.pngã€.jpgã€.svg ç­‰ï¼‰")
    prompt_parts.append("**è¯·ä¸“æ³¨äºæ•°æ®åˆ†æå’Œç»Ÿè®¡è®¡ç®—ï¼Œä¸è¦ç”Ÿæˆå¯è§†åŒ–ä»£ç ã€‚**")
    prompt_parts.append("")
    
    if custom_prompt:
        prompt_parts.append(custom_prompt)
    else:
        prompt_parts.append("è¯·å¯¹ä¸Šä¼ çš„æ•°æ®è¿›è¡Œå…¨é¢åˆ†æï¼Œç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šã€‚")
    
    # æ·»åŠ æ•°æ®æ–‡ä»¶ä¿¡æ¯ï¼ˆé‡è¦ï¼šå‘Šè¯‰AIéœ€è¦è¯»å–CSVæ–‡ä»¶ï¼‰
    if process_result.processed_file_path:
        csv_filename = os.path.basename(process_result.processed_file_path)
        prompt_parts.append(f"\n\n## æ•°æ®æ–‡ä»¶")
        prompt_parts.append(f"**é‡è¦ï¼šå·¥ä½œç©ºé—´ä¸­å·²å‡†å¤‡å¥½å¤„ç†åçš„CSVæ•°æ®æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºï¼š`{csv_filename}`**")
        prompt_parts.append(f"")
        prompt_parts.append(f"**è¯·åŠ¡å¿…ä½¿ç”¨ä»¥ä¸‹ä»£ç è¯»å–æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æï¼š**")
        prompt_parts.append(f"```python")
        prompt_parts.append(f"import pandas as pd")
        prompt_parts.append(f"")
        prompt_parts.append(f"# è¯»å–å¤„ç†åçš„CSVæ–‡ä»¶")
        prompt_parts.append(f"df = pd.read_csv('{csv_filename}')")
        prompt_parts.append(f"print(f'æ•°æ®å½¢çŠ¶: {{df.shape}}')")
        prompt_parts.append(f"print(f'åˆ—å: {{list(df.columns)}}')")
        prompt_parts.append(f"```")
        prompt_parts.append(f"")
        prompt_parts.append(f"**æ³¨æ„ï¼š**")
        prompt_parts.append(f"- CSVæ–‡ä»¶å·²ä¿å­˜åœ¨å½“å‰å·¥ä½œç©ºé—´ç›®å½•ä¸­")
        prompt_parts.append(f"- è¯·ä½¿ç”¨ `pd.read_csv('{csv_filename}')` è¯»å–æ•°æ®")
        prompt_parts.append(f"- ä¸è¦ä»…æ ¹æ®å…ƒæ•°æ®è¿›è¡Œåˆ†æï¼Œå¿…é¡»è¯»å–å®é™…æ•°æ®æ–‡ä»¶è¿›è¡Œè®¡ç®—")
        prompt_parts.append(f"")
    
    # æ·»åŠ æ•°æ®æ¦‚å†µ
    prompt_parts.append(f"\n## æ•°æ®æ¦‚å†µ")
    prompt_parts.append(f"- æ•°æ®è¡Œæ•°: {process_result.row_count}")
    prompt_parts.append(f"- åˆ—æ•°: {len(process_result.column_names)}")
    
    # æ·»åŠ è¡¨å¤´ç±»å‹ä¿¡æ¯ï¼ˆä»…ä¿ç•™å¯¹åˆ†ææœ‰ç”¨çš„ä¿¡æ¯ï¼‰
    if process_result.header_analysis:
        ha = process_result.header_analysis
        if ha.header_type == 'multi':
            prompt_parts.append(f"\n## è¡¨å¤´ç»“æ„")
            prompt_parts.append(f"- è¡¨å¤´ç±»å‹: å¤šçº§è¡¨å¤´ï¼ˆ{ha.header_rows}å±‚ï¼‰")
    
    # æ·»åŠ åˆ—ç»“æ„å…ƒæ•°æ®ï¼ˆå¸®åŠ©AIç†è§£åˆ—ä¹‹é—´çš„å…³ç³»ï¼‰
    if include_metadata and process_result.column_metadata:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šçº§ç»“æ„
        has_multi_level = any(
            len(meta) > 1 
            for meta in process_result.column_metadata.values()
        )
        
        if has_multi_level:
            prompt_parts.append(f"\n## åˆ—å±‚çº§ç»“æ„ï¼ˆå¤šçº§è¡¨å¤´è¯­ä¹‰å…³ç³»ï¼‰")
            prompt_parts.append("ä»¥ä¸‹æ ‘å½¢ç»“æ„å±•ç¤ºäº†åˆ—ä¹‹é—´çš„å±‚çº§åˆ†ç»„å…³ç³»ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„ä¸šåŠ¡å«ä¹‰ï¼š")
            prompt_parts.append("")
            hierarchy_tree = _build_column_hierarchy_tree(process_result.column_metadata)
            if hierarchy_tree:
                prompt_parts.append(hierarchy_tree)
            else:
                # å¦‚æœæ ‘å½¢æ„å»ºå¤±è´¥ï¼Œä½¿ç”¨åˆ†ç»„å±•ç¤º
                groups = defaultdict(list)
                for col_name, meta in process_result.column_metadata.items():
                    level1 = meta.get('level1', col_name)
                    groups[level1].append(col_name)
                
                for group, cols in groups.items():
                    if len(cols) > 1:
                        prompt_parts.append(f"- {group}: {', '.join(cols)}")
    
    # æ·»åŠ å®Œæ•´çš„åˆ—ååˆ—è¡¨
    prompt_parts.append(f"\n## å®Œæ•´åˆ—ååˆ—è¡¨")
    if len(process_result.column_names) <= 30:
        # å¦‚æœåˆ—æ•°ä¸å¤šï¼Œå…¨éƒ¨å±•ç¤º
        for idx, col_name in enumerate(process_result.column_names, 1):
            prompt_parts.append(f"{idx}. {col_name}")
    else:
        # å¦‚æœåˆ—æ•°å¾ˆå¤šï¼Œå±•ç¤ºå‰20ä¸ªå’Œå10ä¸ª
        for idx, col_name in enumerate(process_result.column_names[:20], 1):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"... (çœç•¥ä¸­é—´ {len(process_result.column_names) - 30} åˆ—) ...")
        for idx, col_name in enumerate(process_result.column_names[-10:], len(process_result.column_names) - 9):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"\n(å…± {len(process_result.column_names)} åˆ—)")
    
    # æ·»åŠ å­—æ®µå€¼æ ·æœ¬ä¿¡æ¯ï¼ˆä»¥JSONæ ¼å¼æä¾›ï¼Œæ›´ç»“æ„åŒ–ï¼‰
    if include_metadata and process_result.column_metadata:
        prompt_parts.append(f"\n## å­—æ®µå€¼æ ·æœ¬ï¼ˆå¸¸è§å€¼ç»Ÿè®¡ï¼‰")
        prompt_parts.append("ä»¥ä¸‹JSONæ ¼å¼å±•ç¤ºäº†æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼åŠå…¶å‡ºç°é¢‘ç‡ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„å®é™…å†…å®¹ï¼š")
        prompt_parts.append("")
        
        # æ„å»ºåŒ…å«å€¼æ ·æœ¬çš„column_metadata JSON
        column_metadata_with_samples = {}
        for col_name in process_result.column_names:
            if col_name in process_result.column_metadata:
                column_metadata_with_samples[col_name] = process_result.column_metadata[col_name]
        
        # å°†column_metadataè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
        prompt_parts.append("```json")
        prompt_parts.append(json.dumps(column_metadata_with_samples, ensure_ascii=False, indent=2))
        prompt_parts.append("```")
        prompt_parts.append("")
        
        prompt_parts.append("**è¯´æ˜ï¼š**")
        prompt_parts.append("- æ¯ä¸ªå­—æ®µçš„å…ƒæ•°æ®åŒ…å« `value_samples` å­—æ®µï¼Œå…¶ä¸­åŒ…å«è¯¥å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼")
        prompt_parts.append("- `value_samples.top_values` æ•°ç»„å±•ç¤ºäº†å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼åŠå…¶å‡ºç°æ¬¡æ•°")
        prompt_parts.append("- å¯¹äºæ•°å€¼ç±»å‹å­—æ®µï¼Œè¿˜åŒ…å« `min`ã€`max`ã€`mean`ã€`median` ç­‰ç»Ÿè®¡ä¿¡æ¯")
    
    # åœ¨æœ«å°¾å†æ¬¡å¼ºè°ƒè¦æ±‚
    prompt_parts.append("\n\n**å†æ¬¡æé†’ï¼šè¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æã€ä»£ç æ³¨é‡Šå’ŒæŠ¥å‘Šæ’°å†™ï¼Œä¸”ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ã€‚**")
    
    full_prompt = '\n'.join(prompt_parts)
    
    # æ‰“å°ç”Ÿæˆçš„æç¤ºè¯
    logger.info("=" * 80)
    logger.info("ğŸ“ ç”Ÿæˆçš„AIåˆ†ææç¤ºè¯:")
    logger.info("=" * 80)
    logger.info(full_prompt)
    logger.info("=" * 80)
    
    return full_prompt

