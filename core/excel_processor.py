"""
Excelæ™ºèƒ½å¤„ç†æ¨¡å—
æ”¯æŒï¼š
1. è‡ªåŠ¨è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ³¨é‡Šã€æ ‡é¢˜ç­‰ï¼‰
2. å•è¡¨å¤´/å¤šè¡¨å¤´è‡ªåŠ¨è¯†åˆ«
3. å¯é€‰è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
4. åˆå¹¶å•å…ƒæ ¼å¤„ç†
5. åˆ—ç»“æ„å…ƒæ•°æ®ç”Ÿæˆ
"""

import pandas as pd
import json
import re
import os
import requests
import logging
import tempfile
import shutil
from openpyxl import load_workbook
from typing import Tuple, List, Dict, Optional, Any
from collections import defaultdict
from dataclasses import dataclass, asdict, field
from pathlib import Path

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥é…ç½®ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨å»¶è¿Ÿå¯¼å…¥ï¼‰

from .config import EXCEL_LLM_API_KEY, EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL



@dataclass
class HeaderAnalysis:
    """è¡¨å¤´åˆ†æç»“æœ"""
    skip_rows: int          # éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œæ•°
    header_rows: int        # è¡¨å¤´å ç”¨çš„è¡Œæ•°
    header_type: str        # 'single' æˆ– 'multi'
    data_start_row: int     # æ•°æ®å¼€å§‹è¡Œï¼ˆ1-indexedï¼‰
    confidence: str         # ç½®ä¿¡åº¦: high/medium/low
    reason: str             # åˆ†æåŸå› è¯´æ˜
    valid_cols: Optional[List[int]] = None  # æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        if result.get('valid_cols') is None:
            result['valid_cols'] = None
        return result


@dataclass
class ExcelProcessResult:
    """Excelå¤„ç†ç»“æœ"""
    success: bool
    header_analysis: Optional[HeaderAnalysis]
    processed_file_path: Optional[str]      # å¤„ç†åçš„CSVæ–‡ä»¶è·¯å¾„
    metadata_file_path: Optional[str]       # å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„
    column_names: List[str]                 # åˆ—ååˆ—è¡¨
    column_metadata: Dict[str, Dict]        # åˆ—ç»“æ„å…ƒæ•°æ®
    row_count: int                          # æ•°æ®è¡Œæ•°
    error_message: Optional[str]            # é”™è¯¯ä¿¡æ¯
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "success": self.success,
            "header_analysis": self.header_analysis.to_dict() if self.header_analysis else None,
            "processed_file_path": self.processed_file_path,
            "metadata_file_path": self.metadata_file_path,
            "column_names": self.column_names,
            "column_metadata": self.column_metadata,
            "row_count": self.row_count,
            "error_message": self.error_message
        }


class SmartHeaderProcessor:
    """æ™ºèƒ½è¡¨å¤´å¤„ç†å™¨"""
    
    def __init__(self, filepath: str, sheet_name: str = None):
        self.filepath = filepath
        self.sheet_name = sheet_name
        self.file_ext = Path(filepath).suffix.lower()
        self._temp_xlsx_path = None  # ç”¨äºå­˜å‚¨ä¸´æ—¶è½¬æ¢çš„ .xlsx æ–‡ä»¶è·¯å¾„
        
        # å¦‚æœæ˜¯ .xls æ ¼å¼ï¼Œå…ˆè½¬æ¢ä¸º .xlsx
        if self.file_ext == '.xls':
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ° .xls æ ¼å¼æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸º .xlsx...")
            self._temp_xlsx_path = self._convert_xls_to_xlsx(filepath)
            actual_filepath = self._temp_xlsx_path
            logger.info(f"âœ… è½¬æ¢å®Œæˆ: {self._temp_xlsx_path}")
        else:
            actual_filepath = filepath
        
        # ç»Ÿä¸€ä½¿ç”¨ openpyxl è¯»å–
        self.wb = load_workbook(actual_filepath, data_only=True)
        # ä¿®å¤ï¼šæ˜ç¡®ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼Œè€Œä¸æ˜¯ä¾èµ– wb.activeï¼ˆactiveå¯èƒ½æ˜¯ç”¨æˆ·æœ€åæŸ¥çœ‹çš„å·¥ä½œè¡¨ï¼‰
        if sheet_name:
            self.ws = self.wb[sheet_name]
        else:
            # æ˜ç¡®ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼ˆç´¢å¼•0ï¼‰ï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´
            if not self.wb.sheetnames:
                raise ValueError("Excelæ–‡ä»¶ä¸åŒ…å«ä»»ä½•å·¥ä½œè¡¨")
            self.ws = self.wb[self.wb.sheetnames[0]]
        self.merged_cells_map = self._build_merged_cells_map()
    
    def _convert_xls_to_xlsx(self, xls_path: str) -> str:
        """
        å°† .xls æ–‡ä»¶è½¬æ¢ä¸º .xlsx æ ¼å¼
        
        å‚æ•°:
            xls_path: .xls æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
            ä¸´æ—¶ .xlsx æ–‡ä»¶è·¯å¾„
        """
        try:
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(xls_path, engine='xlrd')
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_dir = os.path.dirname(xls_path)
            temp_xlsx_path = os.path.join(
                temp_dir, 
                f"{Path(xls_path).stem}_converted_{os.getpid()}.xlsx"
            )
            
            # ä½¿ç”¨ ExcelWriter å†™å…¥æ‰€æœ‰å·¥ä½œè¡¨
            with pd.ExcelWriter(temp_xlsx_path, engine='openpyxl') as writer:
                for sheet_name in excel_file.sheet_names:
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine='xlrd')
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            logger.info(f"âœ… .xls æ–‡ä»¶å·²è½¬æ¢ä¸º .xlsx: {temp_xlsx_path}")
            return temp_xlsx_path
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢ .xls æ–‡ä»¶å¤±è´¥: {e}")
            raise ValueError(
                f"æ— æ³•è½¬æ¢ .xls æ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²å®‰è£… xlrd åº“: pip install xlrdã€‚é”™è¯¯: {str(e)}"
            )
    
    def _build_merged_cells_map(self) -> Dict[Tuple[int, int], str]:
        """æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„"""
        merged_map = {}
        for merged_range in self.ws.merged_cells.ranges:
            min_row, min_col = merged_range.min_row, merged_range.min_col
            value = self.ws.cell(min_row, min_col).value
            for row in range(merged_range.min_row, merged_range.max_row + 1):
                for col in range(merged_range.min_col, merged_range.max_col + 1):
                    merged_map[(row, col)] = value
        return merged_map
    
    def get_cell_value(self, row: int, col: int) -> Any:
        """è·å–å•å…ƒæ ¼å€¼ï¼Œå¤„ç†åˆå¹¶å•å…ƒæ ¼"""
        if (row, col) in self.merged_cells_map:
            return self.merged_cells_map[(row, col)]
        return self.ws.cell(row, col).value
    
    def get_preview_data(self, max_rows: int = 15, max_cols: int = 10) -> List[List[Any]]:
        """è·å–é¢„è§ˆæ•°æ®ç”¨äºåˆ†æ"""
        actual_max_col = min(self.ws.max_column, max_cols)
        actual_max_row = min(self.ws.max_row, max_rows)
        
        data = []
        for row in range(1, actual_max_row + 1):
            row_data = []
            for col in range(1, actual_max_col + 1):
                value = self.get_cell_value(row, col)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä¾¿äºåˆ†æ
                if value is None:
                    row_data.append("")
                elif isinstance(value, (int, float)):
                    row_data.append(f"[æ•°å€¼:{value}]")
                else:
                    row_data.append(str(value)[:50])  # æˆªæ–­è¿‡é•¿å†…å®¹
            data.append(row_data)
        return data
    
    def get_merged_info(self) -> List[Dict]:
        """è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯"""
        merged_info = []
        for merged_range in self.ws.merged_cells.ranges:
            if merged_range.min_row <= 10:  # åªå…³æ³¨å‰10è¡Œ
                merged_info.append({
                    'range': str(merged_range),
                    'rows': f"{merged_range.min_row}-{merged_range.max_row}",
                    'cols': f"{merged_range.min_col}-{merged_range.max_col}",
                    'value': str(self.ws.cell(merged_range.min_row, merged_range.min_col).value)[:30]
                })
        return merged_info
    
    def analyze_with_llm(self, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         timeout: Optional[int] = None) -> HeaderAnalysis:
        """
        ä½¿ç”¨LLMç›´æ¥åˆ†æExcelè¡¨æ ¼çš„è¡Œå’Œåˆ—ç»“æ„
        
        å‚æ•°:
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        
        è¿”å›:
            åˆ†æç»“æœï¼ˆå¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼‰
        """
        preview_data = self.get_preview_data(max_rows=20, max_cols=15)
        merged_info = self.get_merged_info()
        max_col = self.ws.max_column
        
        # æ„å»ºåˆ†ææç¤ºè¯
        prompt = self._build_llm_analysis_prompt(preview_data, merged_info, max_col)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model, timeout=timeout)
        
        if not result:
            raise ValueError("LLMåˆ†æå¤±è´¥ï¼šæ— æ³•è·å–LLMå“åº”ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        
        # è§£æLLMåˆ†æç»“æœ
        analysis = self._parse_llm_analysis_response(result)
        
        return analysis
    
    def validate_with_llm(self, rule_analysis: HeaderAnalysis, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         timeout: Optional[int] = None) -> HeaderAnalysis:
        """
        ä½¿ç”¨LLMéªŒè¯è§„åˆ™åˆ†æçš„ç»“æœï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
        
        å‚æ•°:
            rule_analysis: è§„åˆ™åˆ†æçš„ç»“æœ
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        
        è¿”å›:
            éªŒè¯åçš„åˆ†æç»“æœï¼ˆå¦‚æœLLMéªŒè¯å¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœï¼‰
        """
        preview_data = self.get_preview_data()
        merged_info = self.get_merged_info()
        
        # æ„å»ºéªŒè¯æç¤ºè¯
        prompt = self._build_validation_prompt(preview_data, merged_info, rule_analysis)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model, timeout=timeout)
        
        # è§£æLLMéªŒè¯ç»“æœ
        validated = self._parse_validation_response(result, rule_analysis)
        
        return validated
    
    def _build_llm_analysis_prompt(self, preview_data: List[List], merged_info: List[Dict], 
                                   max_col: int) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯ï¼ˆåŒæ—¶åˆ†æè¡Œå’Œåˆ—ï¼‰"""
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºè¡¨æ ¼å½¢å¼
        table_str = "è¡Œå· | åˆ—1 | åˆ—2 | åˆ—3 | åˆ—4 | åˆ—5 | åˆ—6 | åˆ—7 | åˆ—8\n" + "-" * 80 + "\n"
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join(str(cell)[:15] for cell in row[:8])
            table_str += f"  {i:2d}  | {row_str}\n"
        
        # æ ¼å¼åŒ–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯
        merged_str = "æ— " if not merged_info else "\n".join(
            f"  - {m['range']}: '{m['value']}'" for m in merged_info[:10]
        )
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹Excelè¡¨æ ¼çš„ç»“æ„ï¼Œè¯†åˆ«è¡¨å¤´ã€æ•°æ®è¡Œå’Œæœ‰æ•ˆåˆ—ã€‚

ã€è¡¨æ ¼é¢„è§ˆã€‘ï¼ˆå‰20è¡Œï¼Œ[æ•°å€¼:xxx]è¡¨ç¤ºæ•°å€¼ç±»å‹ï¼‰
{table_str}

ã€åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ã€‘
{merged_str}

ã€æ€»åˆ—æ•°ã€‘{max_col}

è¯·ä»”ç»†åˆ†æè¡¨æ ¼ç»“æ„ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "skip_rows": <éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œæ•°ï¼ˆæ ‡é¢˜ã€æ³¨é‡Šç­‰ï¼Œä»ç¬¬1è¡Œå¼€å§‹è®¡æ•°ï¼‰>,
    "header_rows": <è¡¨å¤´å ç”¨çš„è¡Œæ•°ï¼ˆ1è¡¨ç¤ºå•è¡¨å¤´ï¼Œ>1è¡¨ç¤ºå¤šçº§è¡¨å¤´ï¼‰>,
    "header_type": "<singleæˆ–multi>",
    "data_start_row": <æ•°æ®å¼€å§‹è¡Œå·ï¼ˆ1-indexedï¼‰>,
    "valid_cols": [<æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼Œ1-indexedï¼Œä¾‹å¦‚[1,2,3,5,6]è¡¨ç¤ºç¬¬1,2,3,5,6åˆ—æœ‰æ•ˆï¼Œå…¶ä»–åˆ—æ— æ•ˆ>],
    "confidence": "<high/medium/low>",
    "reason": "<åˆ†æè¯´æ˜ï¼šè¯´æ˜å¦‚ä½•è¯†åˆ«è¡¨å¤´ã€æ•°æ®è¡Œå’Œæœ‰æ•ˆåˆ—>"
}}

åˆ†æè¦ç‚¹ï¼š
1. **è·³è¿‡è¡Œè¯†åˆ«**ï¼šè¯†åˆ«è¡¨æ ¼å¼€å¤´çš„æ ‡é¢˜è¡Œã€æ³¨é‡Šè¡Œç­‰æ— æ•ˆè¡Œï¼ˆé€šå¸¸åªæœ‰å°‘é‡éç©ºå•å…ƒæ ¼æˆ–å…¨æ˜¯æ–‡æœ¬ï¼‰
2. **è¡¨å¤´è¯†åˆ«**ï¼š
   - å•è¡¨å¤´ï¼šåªæœ‰ä¸€è¡Œè¡¨å¤´
   - å¤šçº§è¡¨å¤´ï¼šæœ‰å¤šè¡Œè¡¨å¤´ï¼ˆæ³¨æ„åˆå¹¶å•å…ƒæ ¼å¯èƒ½è¡¨ç¤ºå¤šçº§è¡¨å¤´ï¼‰
   - è¡¨å¤´é€šå¸¸åŒ…å«åˆ—åã€åˆ†ç±»æ ‡ç­¾ç­‰æ–‡æœ¬ä¿¡æ¯
3. **æ•°æ®èµ·å§‹è¡Œ**ï¼šè¯†åˆ«æ•°æ®å†…å®¹å¼€å§‹çš„è¡Œï¼ˆé€šå¸¸åŒ…å«æ•°å€¼æ•°æ®ï¼‰
4. **æœ‰æ•ˆåˆ—è¯†åˆ«**ï¼š
   - è¡¨å¤´åŒºåŸŸå®Œå…¨ä¸ºç©ºä¸”æ•°æ®åŒºåŸŸå®Œå…¨ä¸ºç©ºæˆ–æ— æ•°å€¼çš„åˆ—åº”æ ‡è®°ä¸ºæ— æ•ˆ
   - å¦‚æœåˆ—ç´¢å¼•ä¸åœ¨valid_colsä¸­ï¼Œè¡¨ç¤ºè¯¥åˆ—æ— æ•ˆï¼Œåº”è¢«è¿‡æ»¤
   - å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆï¼Œvalid_colsåº”ä¸ºnullæˆ–åŒ…å«æ‰€æœ‰åˆ—ç´¢å¼•
5. **åˆå¹¶å•å…ƒæ ¼**ï¼šæ³¨æ„åˆå¹¶å•å…ƒæ ¼å¯èƒ½å½±å“è¡¨å¤´è¡Œæ•°çš„åˆ¤æ–­

é‡è¦ï¼š
- è¡Œå·å’Œåˆ—å·éƒ½ä»1å¼€å§‹è®¡æ•°
- valid_colså¿…é¡»æ˜¯åˆ—ç´¢å¼•çš„æ•°ç»„ï¼ˆ1-indexedï¼‰ï¼Œä¾‹å¦‚[1,2,3,5,6]è¡¨ç¤ºä¿ç•™ç¬¬1,2,3,5,6åˆ—
- å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆï¼Œvalid_colså¯ä»¥è®¾ä¸ºnullæˆ–åŒ…å«æ‰€æœ‰åˆ—ç´¢å¼•[1,2,3,...,{max_col}]
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""
        
        return prompt
    
    def _build_validation_prompt(self, preview_data: List[List], merged_info: List[Dict], 
                                rule_analysis: HeaderAnalysis) -> str:
        """æ„å»ºLLMéªŒè¯æç¤ºè¯"""
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºè¡¨æ ¼å½¢å¼
        table_str = "è¡Œå· | å†…å®¹\n" + "-" * 50 + "\n"
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join(str(cell)[:20] for cell in row[:8])
            table_str += f"  {i}  | {row_str}\n"
        
        # æ ¼å¼åŒ–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯
        merged_str = "æ— " if not merged_info else "\n".join(
            f"  - {m['range']}: '{m['value']}'" for m in merged_info[:5]
        )
        
        prompt = f"""è¯·éªŒè¯ä»¥ä¸‹Excelè¡¨æ ¼çš„è§„åˆ™åˆ†æç»“æœæ˜¯å¦æ­£ç¡®ã€‚

ã€è¡¨æ ¼é¢„è§ˆã€‘ï¼ˆå‰15è¡Œï¼Œ[æ•°å€¼:xxx]è¡¨ç¤ºæ•°å€¼ç±»å‹ï¼‰
{table_str}

ã€åˆå¹¶å•å…ƒæ ¼ã€‘
{merged_str}

ã€è§„åˆ™åˆ†æç»“æœã€‘
- è·³è¿‡è¡Œæ•°: {rule_analysis.skip_rows}
- è¡¨å¤´è¡Œæ•°: {rule_analysis.header_rows}
- è¡¨å¤´ç±»å‹: {rule_analysis.header_type}
- æ•°æ®èµ·å§‹è¡Œ: {rule_analysis.data_start_row}
- åˆ†æåŸå› : {rule_analysis.reason}

è¯·éªŒè¯è¿™ä¸ªç»“æœæ˜¯å¦åˆç†ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "is_valid": <trueæˆ–falseï¼Œè¡¨ç¤ºç»“æœæ˜¯å¦åˆç†>,
    "confidence": "<high/medium/low>",
    "suggestions": {{
        "skip_rows": <å»ºè®®çš„è·³è¿‡è¡Œæ•°ï¼Œå¦‚æœåˆç†åˆ™ä¸è§„åˆ™åˆ†æç›¸åŒ>,
        "header_rows": <å»ºè®®çš„è¡¨å¤´è¡Œæ•°ï¼Œå¦‚æœåˆç†åˆ™ä¸è§„åˆ™åˆ†æç›¸åŒ>,
        "header_type": "<singleæˆ–multi>",
        "data_start_row": <å»ºè®®çš„æ•°æ®èµ·å§‹è¡Œï¼Œå¦‚æœåˆç†åˆ™ä¸è§„åˆ™åˆ†æç›¸åŒ>
    }},
    "reason": "<éªŒè¯è¯´æ˜ï¼šå¦‚æœåˆç†ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆï¼›å¦‚æœä¸åˆç†ï¼ŒæŒ‡å‡ºé—®é¢˜å¹¶ç»™å‡ºå»ºè®®>"
}}

éªŒè¯è¦ç‚¹ï¼š
- æ£€æŸ¥è·³è¿‡çš„è¡Œæ˜¯å¦çœŸçš„æ˜¯æ— æ•ˆè¡Œï¼ˆæ ‡é¢˜ã€æ³¨é‡Šç­‰ï¼‰
- æ£€æŸ¥è¡¨å¤´è¡Œæ•°æ˜¯å¦æ­£ç¡®ï¼ˆæ˜¯å¦é—æ¼äº†å¤šçº§è¡¨å¤´ï¼‰
- æ£€æŸ¥æ•°æ®èµ·å§‹è¡Œæ˜¯å¦å‡†ç¡®ï¼ˆæ˜¯å¦æŠŠè¡¨å¤´è¡Œè¯¯åˆ¤ä¸ºæ•°æ®è¡Œï¼‰
- å¦‚æœè§„åˆ™åˆ†æç»“æœåˆç†ï¼Œä¿æŒåŸç»“æœï¼›å¦‚æœä¸åˆç†ï¼Œç»™å‡ºä¿®æ­£å»ºè®®
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""
        
        return prompt
    
    def _call_llm(self, prompt: str, llm_api_key: Optional[str] = None, 
                  llm_base_url: Optional[str] = None, llm_model: Optional[str] = None,
                  timeout: Optional[int] = None) -> str:
        """è°ƒç”¨LLM APIï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰
        
        å‚æ•°:
            prompt: æç¤ºè¯
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»é…ç½®è¯»å–
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        base_url = llm_base_url if llm_base_url is not None else EXCEL_LLM_BASE_URL
        model = llm_model if llm_model is not None else EXCEL_LLM_MODEL
        
        logger.info("=" * 60)
        logger.info("ğŸ¤– è°ƒç”¨ LLM API è¿›è¡ŒExcelè¡¨æ ¼åˆ†æ")
        logger.info(f"ğŸ”— EXCEL_LLM_BASE_URL: {base_url}")
        logger.info(f"ğŸ“Œ æ¨¡å‹: {model}")
        logger.info(f"ğŸ”‘ API Key: {'å·²é…ç½®' if api_key else 'æœªé…ç½®'}")
        
        if not api_key:
            logger.error("âŒ æœªé…ç½® LLM API Keyï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return None
            
        url = base_url
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # ä½¿ç”¨æµå¼è°ƒç”¨ä»¥æ”¯æŒ thinking åŠŸèƒ½
        base_payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.4,
            "max_tokens": 1000,  # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒæ›´è¯¦ç»†çš„åˆ†æ
            "stream": True,  # æµå¼è°ƒç”¨
        }
        
        # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤90ç§’
        request_timeout = timeout if timeout is not None else 90
        
        logger.info(f"ğŸ“¡ å‘é€ LLM API è¯·æ±‚åˆ°: {url} (æµå¼è°ƒç”¨)")
        logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.info(f"â±ï¸ è¶…æ—¶è®¾ç½®: {request_timeout} ç§’")
        
        try:
            # ä¼˜å…ˆå°è¯•å¯ç”¨ thinking åŠŸèƒ½
            payload_with_thinking = base_payload.copy()
            payload_with_thinking["enable_thinking"] = True
            
            logger.debug(f"ğŸ“¦ è¯·æ±‚ payload (å¯ç”¨ thinking): {json.dumps(payload_with_thinking, ensure_ascii=False, indent=2)}")
            
            response = requests.post(
                url, 
                headers=headers, 
                json=payload_with_thinking, 
                timeout=request_timeout,
                stream=True  # å¯ç”¨æµå¼å“åº”
            )
            
            # å¦‚æœå¯ç”¨ thinking å¤±è´¥ï¼Œå›é€€åˆ°ä¸ä½¿ç”¨ thinking
            if response.status_code != 200:
                try:
                    error_json = response.json()
                    if "enable_thinking" in str(error_json).lower():
                        logger.warning("âš ï¸ å¯ç”¨ thinking å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ thinking")
                        payload_no_thinking = base_payload.copy()
                        logger.debug(f"ğŸ“¦ è¯·æ±‚ payload (ä¸ä½¿ç”¨ thinking): {json.dumps(payload_no_thinking, ensure_ascii=False, indent=2)}")
                        response = requests.post(
                            url, 
                            headers=headers, 
                            json=payload_no_thinking, 
                            timeout=request_timeout,
                            stream=True
                        )
                except:
                    pass
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¾“å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if response.status_code != 200:
                error_detail = ""
                try:
                    # å¯¹äºæµå¼å“åº”ï¼Œå°è¯•è¯»å–é”™è¯¯ä¿¡æ¯
                    error_text = ""
                    for line in response.iter_lines():
                        if line:
                            line_str = line.decode('utf-8')
                            if line_str.startswith('data: '):
                                line_str = line_str[6:]
                            try:
                                error_json = json.loads(line_str)
                                error_detail = json.dumps(error_json, ensure_ascii=False, indent=2)
                                break
                            except:
                                error_text += line_str + "\n"
                    if not error_detail:
                        error_detail = error_text or response.text
                except:
                    try:
                        error_detail = response.text
                    except:
                        error_detail = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ… (çŠ¶æ€ç : {response.status_code})"
                
                logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                logger.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…:\n{error_detail}")
                logger.error(f"ğŸ”— è¯·æ±‚ URL: {url}")
                logger.error(f"ğŸ“¦ è¯·æ±‚ payload: {json.dumps(base_payload, ensure_ascii=False, indent=2)}")
                return None
            
            # å¤„ç†æµå¼å“åº”
            full_content = ""
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    # è·³è¿‡ SSE æ ¼å¼çš„å‰ç¼€ "data: "
                    if line_str.startswith('data: '):
                        line_str = line_str[6:]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                    if line_str.strip() == '[DONE]':
                        break
                    
                    # è§£æ JSON
                    try:
                        chunk_data = json.loads(line_str)
                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                            delta = chunk_data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                full_content += content
                    except json.JSONDecodeError:
                        # å¿½ç•¥æ— æ³•è§£æçš„è¡Œï¼ˆå¯èƒ½æ˜¯ç©ºè¡Œæˆ–å…¶ä»–æ ¼å¼ï¼‰
                        continue
            
            if not full_content:
                logger.warning("âš ï¸ LLM æµå¼å“åº”ä¸ºç©º")
                return None
            
            logger.info("âœ… LLM API è°ƒç”¨æˆåŠŸ")
            logger.info("=" * 60)
            logger.info("ğŸ“ LLM å“åº”å†…å®¹:")
            logger.info("=" * 60)
            logger.info(full_content)
            logger.info("=" * 60)
            
            return full_content
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥ (ç½‘ç»œé”™è¯¯): {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_json = e.response.json()
                    logger.error(f"ğŸ“‹ API é”™è¯¯å“åº”: {json.dumps(error_json, ensure_ascii=False, indent=2)}")
                except:
                    logger.error(f"ğŸ“‹ API é”™è¯¯å“åº” (æ–‡æœ¬): {e.response.text}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
    
    def _parse_llm_analysis_response(self, response: str) -> HeaderAnalysis:
        """è§£æLLMåˆ†æç»“æœï¼ˆåŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰"""
        if not response:
            raise ValueError("LLMå“åº”ä¸ºç©º")
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            # è§£ææœ‰æ•ˆåˆ—
            valid_cols = data.get('valid_cols')
            if valid_cols is None:
                # å¦‚æœä¸ºnullï¼Œè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
                valid_cols = None
            elif isinstance(valid_cols, list):
                # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                valid_cols = [int(col) for col in valid_cols if isinstance(col, (int, str)) and str(col).isdigit()]
                # å¦‚æœåˆ—è¡¨ä¸ºç©ºæˆ–åŒ…å«æ‰€æœ‰åˆ—ï¼Œè®¾ä¸ºNone
                max_col = self.ws.max_column
                if not valid_cols or set(valid_cols) == set(range(1, max_col + 1)):
                    valid_cols = None
            else:
                valid_cols = None
            
            # æ„å»ºHeaderAnalysiså¯¹è±¡
            analysis = HeaderAnalysis(
                skip_rows=int(data.get('skip_rows', 0)),
                header_rows=int(data.get('header_rows', 1)),
                header_type=data.get('header_type', 'single'),
                data_start_row=int(data.get('data_start_row', 1)),
                confidence=data.get('confidence', 'medium'),
                reason=f"LLMåˆ†æ: {data.get('reason', '')}",
                valid_cols=valid_cols
            )
            
            logger.info(f"âœ… LLMåˆ†æå®Œæˆ:")
            logger.info(f"  - è·³è¿‡è¡Œæ•°: {analysis.skip_rows}")
            logger.info(f"  - è¡¨å¤´è¡Œæ•°: {analysis.header_rows}")
            logger.info(f"  - è¡¨å¤´ç±»å‹: {analysis.header_type}")
            logger.info(f"  - æ•°æ®èµ·å§‹è¡Œ: {analysis.data_start_row}")
            logger.info(f"  - æœ‰æ•ˆåˆ—æ•°: {len(analysis.valid_cols) if analysis.valid_cols else 'å…¨éƒ¨'}")
            logger.info(f"  - ç½®ä¿¡åº¦: {analysis.confidence}")
            
            return analysis
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"âŒ è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
            logger.error(f"ğŸ“‹ å“åº”å†…å®¹: {response[:500]}")
            raise ValueError(f"è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
    
    def _parse_validation_response(self, response: str, rule_analysis: HeaderAnalysis) -> HeaderAnalysis:
        """è§£æLLMéªŒè¯ç»“æœï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
        if not response:
            # LLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœ
            return rule_analysis
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            is_valid = data.get('is_valid', True)
            suggestions = data.get('suggestions', {})
            
            if is_valid:
                # LLMè®¤ä¸ºè§„åˆ™åˆ†æç»“æœåˆç†ï¼Œä¿æŒåŸç»“æœä½†æ›´æ–°ç½®ä¿¡åº¦å’ŒåŸå› 
                return HeaderAnalysis(
                    skip_rows=rule_analysis.skip_rows,
                    header_rows=rule_analysis.header_rows,
                    header_type=rule_analysis.header_type,
                    data_start_row=rule_analysis.data_start_row,
                    confidence=data.get('confidence', 'high'),  # LLMéªŒè¯é€šè¿‡ï¼Œç½®ä¿¡åº¦æå‡
                    reason=f"è§„åˆ™åˆ†æ+LLMéªŒè¯: {data.get('reason', 'éªŒè¯é€šè¿‡')}",
                    valid_cols=rule_analysis.valid_cols  # ä¿æŒåŸæœ‰çš„åˆ—è¿‡æ»¤ç»“æœ
                )
            else:
                # LLMè®¤ä¸ºä¸åˆç†ï¼Œä½¿ç”¨LLMçš„å»ºè®®
                # æ³¨æ„ï¼šLLMå¯èƒ½å»ºè®®ä¿®æ”¹è¡¨å¤´è¡Œæ•°ï¼Œä½†åˆ—è¿‡æ»¤ç»“æœä»ç„¶ä¿ç•™
                return HeaderAnalysis(
                    skip_rows=suggestions.get('skip_rows', rule_analysis.skip_rows),
                    header_rows=suggestions.get('header_rows', rule_analysis.header_rows),
                    header_type=suggestions.get('header_type', rule_analysis.header_type),
                    data_start_row=suggestions.get('data_start_row', rule_analysis.data_start_row),
                    confidence=data.get('confidence', 'medium'),
                    reason=f"è§„åˆ™åˆ†æ+LLMä¿®æ­£: {data.get('reason', 'LLMå»ºè®®ä¿®æ­£')}",
                    valid_cols=rule_analysis.valid_cols  # ä¿æŒåŸæœ‰çš„åˆ—è¿‡æ»¤ç»“æœ
                )
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"è§£æLLMéªŒè¯å“åº”å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸè§„åˆ™åˆ†æç»“æœ")
        
        # è§£æå¤±è´¥ï¼Œè¿”å›åŸè§„åˆ™åˆ†æç»“æœ
        return rule_analysis
    
    # å·²åºŸå¼ƒï¼šè§„åˆ™åˆ†ææ–¹æ³•ï¼Œç°åœ¨å¿…é¡»ä½¿ç”¨LLMåˆ†æ
    # def analyze_with_rules(self) -> HeaderAnalysis:
    #     """åŸºäºè§„åˆ™çš„åˆ†æï¼ˆå·²åºŸå¼ƒï¼Œç°åœ¨å¿…é¡»ä½¿ç”¨LLMåˆ†æï¼‰"""
    #     max_col = self.ws.max_column
    #     skip_rows = 0
    #     header_rows = 1
    #     
    #     # æ£€æµ‹éœ€è¦è·³è¿‡çš„è¡Œ
    #     for row in range(1, min(6, self.ws.max_row + 1)):
    #         row_values = [self.get_cell_value(row, col) for col in range(1, max_col + 1)]
    #         non_empty = sum(1 for v in row_values if v is not None)
    #         
    #         # å¦‚æœåªæœ‰å¾ˆå°‘çš„éç©ºå•å…ƒæ ¼ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜è¡Œ
    #         if non_empty <= 2 and non_empty < max_col * 0.3:
    #             skip_rows = row
    #         else:
    #             break
    #     
    #     # æ£€æµ‹è¡¨å¤´è¡Œæ•°
    #     header_start = skip_rows + 1
    #     
    #     # æ£€æŸ¥åˆå¹¶å•å…ƒæ ¼
    #     max_merged_row = 0
    #     for merged_range in self.ws.merged_cells.ranges:
    #         if merged_range.min_row > skip_rows:
    #             if merged_range.max_row > max_merged_row:
    #                 max_merged_row = merged_range.max_row
    #     
    #     if max_merged_row > header_start:
    #         header_rows = max_merged_row - skip_rows
    #     
    #     # æ£€æµ‹æ•°æ®è¡Œå¼€å§‹ä½ç½®
    #     data_start = skip_rows + header_rows + 1
    #     for row in range(header_start, min(skip_rows + 10, self.ws.max_row + 1)):
    #         row_values = [self.get_cell_value(row, col) for col in range(1, max_col + 1)]
    #         non_empty = sum(1 for v in row_values if v is not None)
    #         numeric = sum(1 for v in row_values if isinstance(v, (int, float)) and not isinstance(v, bool))
    #         
    #         if non_empty > 0 and numeric / max(non_empty, 1) > 0.4:
    #             data_start = row
    #             header_rows = row - skip_rows - 1
    #             break
    #     
    #     header_type = 'multi' if header_rows > 1 else 'single'
    #     
    #     return HeaderAnalysis(
    #         skip_rows=skip_rows,
    #         header_rows=max(1, header_rows),
    #         header_type=header_type,
    #         data_start_row=data_start,
    #         confidence='medium',
    #         reason='åŸºäºè§„åˆ™åˆ†æ',
    #         valid_cols=None
    #     )
    
    def _detect_valid_columns(self, skip_rows: int, header_rows: int, data_start_row: int) -> List[int]:
        """
        æ£€æµ‹æœ‰æ•ˆåˆ—ï¼ˆè¿‡æ»¤æ— æ•ˆåˆ—ï¼‰
        
        æ— æ•ˆåˆ—çš„åˆ¤æ–­æ ‡å‡†ï¼š
        1. è¡¨å¤´åŒºåŸŸå®Œå…¨ä¸ºç©º
        2. æ•°æ®åŒºåŸŸå®Œå…¨ä¸ºç©ºæˆ–æ²¡æœ‰æ•°å€¼æ•°æ®
        
        è¿”å›: æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰
        """
        max_col = self.ws.max_column
        header_start = skip_rows + 1
        header_end = skip_rows + header_rows
        valid_cols = []
        
        logger.info("ğŸ” å¼€å§‹æ£€æµ‹æ— æ•ˆåˆ—...")
        
        for col in range(1, max_col + 1):
            # æ£€æŸ¥è¡¨å¤´åŒºåŸŸæ˜¯å¦æœ‰å†…å®¹
            has_header = False
            for row in range(header_start, header_end + 1):
                value = self.get_cell_value(row, col)
                if value is not None and str(value).strip():
                    has_header = True
                    break
            
            # æ£€æŸ¥æ•°æ®åŒºåŸŸæ˜¯å¦æœ‰æ•°å€¼æ•°æ®
            has_data = False
            numeric_count = 0
            total_count = 0
            for row in range(data_start_row, min(data_start_row + 10, self.ws.max_row + 1)):
                value = self.ws.cell(row, col).value
                if value is not None:
                    total_count += 1
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        numeric_count += 1
                        has_data = True
            
            # å¦‚æœè¡¨å¤´æœ‰å†…å®¹æˆ–æ•°æ®åŒºåŸŸæœ‰æ•°å€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆåˆ—
            if has_header or has_data:
                valid_cols.append(col)
                logger.debug(f"âœ… åˆ— {col}: æœ‰æ•ˆ (è¡¨å¤´: {has_header}, æ•°æ®: {has_data}, æ•°å€¼: {numeric_count}/{total_count})")
            else:
                logger.info(f"âŒ åˆ— {col}: æ— æ•ˆ (è¡¨å¤´ä¸ºç©ºä¸”æ•°æ®ä¸ºç©º)")
        
        logger.info(f"ğŸ“Š åˆ—è¿‡æ»¤ç»“æœ: æ€»åˆ—æ•° {max_col}, æœ‰æ•ˆåˆ—æ•° {len(valid_cols)}, æ— æ•ˆåˆ—æ•° {max_col - len(valid_cols)}")
        
        # å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆï¼Œè¿”å›Noneï¼ˆè¡¨ç¤ºä¸éœ€è¦è¿‡æ»¤ï¼‰
        if len(valid_cols) == max_col:
            return None
        
        return valid_cols
    
    def extract_headers(self, analysis: HeaderAnalysis) -> Tuple[List[str], Dict[str, Dict]]:
        """
        æ ¹æ®åˆ†æç»“æœæå–è¡¨å¤´
        è¿”å›: (åˆ—ååˆ—è¡¨, åˆ—ç»“æ„å…ƒæ•°æ®)
        """
        max_col = self.ws.max_column
        header_start = analysis.skip_rows + 1
        header_end = analysis.skip_rows + analysis.header_rows
        
        # ç¡®å®šè¦å¤„ç†çš„åˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåªå¤„ç†æœ‰æ•ˆåˆ—ï¼‰
        cols_to_process = analysis.valid_cols if analysis.valid_cols is not None else list(range(1, max_col + 1))
        
        logger.info(f"ğŸ“‹ æå–è¡¨å¤´: å¤„ç† {len(cols_to_process)} åˆ—")
        
        column_metadata = {}
        
        if analysis.header_type == 'single':
            # å•è¡¨å¤´
            headers = []
            for col in cols_to_process:
                value = self.get_cell_value(header_start, col)
                col_name = str(value) if value else f'Column_{col}'
                headers.append(col_name)
                column_metadata[col_name] = {"level1": col_name}
            
            headers = self._handle_duplicate_names(headers)
            # æ›´æ–°å…ƒæ•°æ®çš„key
            column_metadata = {h: {"level1": h} for h in headers}
            return headers, column_metadata
        
        else:
            # å¤šè¡¨å¤´ï¼šå±•å¹³
            column_headers = []
            original_metadata_list = []  # ä¿å­˜åŸå§‹å…ƒæ•°æ®åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºå¯¹åº”
            
            for col in cols_to_process:
                parts = []
                levels = {}
                for row_idx, row in enumerate(range(header_start, header_end + 1), 1):
                    value = self.get_cell_value(row, col)
                    if value is not None:
                        part = str(value).strip()
                        parts.append(part)
                        levels[f"level{row_idx}"] = part
                
                # å»é‡è¿ç»­ç›¸åŒå€¼
                unique_parts = []
                for p in parts:
                    if not unique_parts or p != unique_parts[-1]:
                        unique_parts.append(p)
                
                col_name = '_'.join(unique_parts) if unique_parts else f'Column_{col}'
                column_headers.append(col_name)
                original_metadata_list.append(levels)  # æŒ‰é¡ºåºä¿å­˜å…ƒæ•°æ®
            
            # å¤„ç†é‡å¤åˆ—å
            column_headers = self._handle_duplicate_names(column_headers)
            
            # é‡æ–°æ˜ å°„å…ƒæ•°æ®ï¼šä½¿ç”¨ç´¢å¼•å¯¹åº”å…³ç³»
            new_metadata = {}
            for i, header in enumerate(column_headers):
                # ä½¿ç”¨ç´¢å¼•ç›´æ¥è·å–å¯¹åº”çš„å…ƒæ•°æ®
                if i < len(original_metadata_list):
                    new_metadata[header] = original_metadata_list[i]
                else:
                    # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œåˆ›å»ºé»˜è®¤å…ƒæ•°æ®
                    logger.warning(f"âš ï¸ ç´¢å¼•è¶…å‡ºèŒƒå›´: i={i}, headersé•¿åº¦={len(column_headers)}, metadataé•¿åº¦={len(original_metadata_list)}")
                    new_metadata[header] = {"level1": header}
            
            return column_headers, new_metadata
    
    def _handle_duplicate_names(self, names: List[str]) -> List[str]:
        """å¤„ç†é‡å¤åˆ—å"""
        counts = defaultdict(int)
        result = []
        for name in names:
            if counts[name] > 0:
                result.append(f"{name}_{counts[name]}")
            else:
                result.append(name)
            counts[name] += 1
        return result
    
    def to_dataframe(self, analysis: HeaderAnalysis = None, use_llm_validate: bool = False,
                    llm_api_key: Optional[str] = None,
                    llm_base_url: Optional[str] = None,
                    llm_model: Optional[str] = None,
                    preprocessing_timeout: Optional[int] = None) -> Tuple[pd.DataFrame, HeaderAnalysis, Dict[str, Dict]]:
        """
        è½¬æ¢ä¸ºDataFrame
        
        å‚æ•°:
            analysis: é¢„å…ˆçš„åˆ†æç»“æœï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨LLMè‡ªåŠ¨åˆ†æ
            use_llm_validate: å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
        
        è¿”å›:
            (DataFrame, åˆ†æç»“æœ, åˆ—ç»“æ„å…ƒæ•°æ®)
        """
        if analysis is None:
            # å¿…é¡»ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼ˆåŒæ—¶åˆ†æè¡Œå’Œåˆ—ï¼‰
            logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨LLMåˆ†æExcelè¡¨æ ¼ç»“æ„ï¼ˆè¡Œå’Œåˆ—ï¼‰...")
            
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
            api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
            if not api_key:
                raise ValueError("LLM APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒExcelåˆ†æã€‚è¯·é…ç½®EXCEL_LLM_API_KEYæˆ–ä¼ å…¥llm_api_keyå‚æ•°")
            
            # ä½¿ç”¨LLMç›´æ¥åˆ†æï¼ˆåŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰
            analysis = self.analyze_with_llm(
                llm_api_key=llm_api_key,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
                timeout=preprocessing_timeout
            )
            logger.info("âœ… LLMåˆ†æå®Œæˆï¼ˆå·²åŒ…å«è¡Œå’Œåˆ—ä¿¡æ¯ï¼‰")
        
        headers, column_metadata = self.extract_headers(analysis)
        
        # ç¡®å®šè¦è¯»å–çš„åˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåªè¯»å–æœ‰æ•ˆåˆ—ï¼‰
        cols_to_read = analysis.valid_cols if analysis.valid_cols is not None else list(range(1, self.ws.max_column + 1))
        
        logger.info(f"ğŸ“Š è¯»å–æ•°æ®: ä» {len(cols_to_read)} åˆ—è¯»å–æ•°æ®")
        
        # è¯»å–æ•°æ®
        data = []
        for row in range(analysis.data_start_row, self.ws.max_row + 1):
            row_data = []
            for col in cols_to_read:
                row_data.append(self.ws.cell(row, col).value)
            if any(v is not None for v in row_data):
                data.append(row_data)
        
        df = pd.DataFrame(data, columns=headers)
        
        # æ™ºèƒ½ç±»å‹è½¬æ¢ï¼šå°è¯•å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—
        logger.info("ğŸ”„ å¼€å§‹æ™ºèƒ½ç±»å‹è½¬æ¢...")
        def smart_convert_value(value):
            """æ™ºèƒ½è½¬æ¢å€¼ï¼šå°è¯•å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—"""
            if value is None:
                return value
            if isinstance(value, (int, float)):
                return value
            if isinstance(value, str):
                # å»é™¤å‰åç©ºæ ¼
                value = value.strip()
                if not value:  # ç©ºå­—ç¬¦ä¸²
                    return None
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                try:
                    # å°è¯•æ•´æ•°ï¼ˆæ”¯æŒè´Ÿæ•°ï¼‰
                    if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                        return int(value)
                    # å°è¯•æµ®ç‚¹æ•°ï¼ˆæ”¯æŒç§‘å­¦è®¡æ•°æ³•ï¼‰
                    return float(value)
                except (ValueError, AttributeError):
                    # è½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå­—ç¬¦ä¸²
                    return value
            return value
        
        # å¯¹æ¯åˆ—åº”ç”¨æ™ºèƒ½è½¬æ¢
        for col in df.columns:
            original_type = df[col].dtype
            df[col] = df[col].apply(smart_convert_value)
            new_type = df[col].dtype
            if original_type != new_type:
                logger.debug(f"  åˆ— '{col}': {original_type} â†’ {new_type}")
        
        # ä½¿ç”¨ pandas çš„ convert_dtypes è¿›ä¸€æ­¥ä¼˜åŒ–ç±»å‹æ¨æ–­
        df = df.convert_dtypes()
        
        logger.info(f"âœ… DataFrame åˆ›å»ºå®Œæˆ: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        logger.info(f"ğŸ“Š æ•°æ®ç±»å‹ä¼˜åŒ–å®Œæˆ")
        return df, analysis, column_metadata
    
    def close(self):
        """å…³é—­å·¥ä½œç°¿å¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            self.wb.close()
        except Exception:
            pass
        
        # åˆ é™¤ä¸´æ—¶è½¬æ¢çš„ .xlsx æ–‡ä»¶
        if self._temp_xlsx_path and os.path.exists(self._temp_xlsx_path):
            try:
                os.remove(self._temp_xlsx_path)
                logger.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self._temp_xlsx_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {self._temp_xlsx_path}, é”™è¯¯: {e}")


def process_excel_file(
    filepath: str,
    output_dir: str,
    sheet_name: str = None,
    use_llm_validate: bool = False,  # å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œç°åœ¨æ€»æ˜¯ä½¿ç”¨LLM
    output_filename: str = None,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    preprocessing_timeout: Optional[int] = None
) -> ExcelProcessResult:
    """
    å¤„ç†Excelæ–‡ä»¶çš„ä¸»å‡½æ•°
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        sheet_name: å·¥ä½œè¡¨åç§°
        use_llm_validate: å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ã€‚ç°åœ¨æ€»æ˜¯ä½¿ç”¨LLMè¿›è¡Œåˆ†æ
        output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼Œå¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤90ç§’
    
    è¿”å›:
        ExcelProcessResult
    
    æ³¨æ„:
        ç°åœ¨å¿…é¡»ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼Œä¸å†æ”¯æŒè§„åˆ™åˆ†æã€‚è¯·ç¡®ä¿æä¾›llm_api_keyå‚æ•°ã€‚
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤„ç†Excelï¼ˆç°åœ¨æ€»æ˜¯ä½¿ç”¨LLMåˆ†æï¼‰
        processor = SmartHeaderProcessor(filepath, sheet_name)
        df, analysis, column_metadata = processor.to_dataframe(
            use_llm_validate=True,  # æ€»æ˜¯ä½¿ç”¨LLMï¼Œå¿½ç•¥ä¼ å…¥çš„use_llm_validateå‚æ•°
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            preprocessing_timeout=preprocessing_timeout
        )
        processor.close()
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_filename:
            base_name = Path(filepath).stem
            output_filename = f"{base_name}_processed"
        
        # ä¿å­˜CSV
        csv_path = os.path.join(output_dir, f"{output_filename}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # æå–å­—æ®µå€¼æ ·æœ¬ï¼ˆåˆ†ç»„èšåˆåçš„å¸¸è§å€¼ï¼‰
        logger.info("ğŸ“Š æå–å­—æ®µå€¼æ ·æœ¬...")
        column_value_samples = extract_column_value_samples(df, max_samples_per_column=10)
        
        # å°†å€¼æ ·æœ¬ä¿¡æ¯åˆå¹¶åˆ°åˆ—å…ƒæ•°æ®ä¸­
        for col_name, samples in column_value_samples.items():
            if col_name in column_metadata:
                column_metadata[col_name]["value_samples"] = samples
            else:
                # å¦‚æœåˆ—ä¸åœ¨å…ƒæ•°æ®ä¸­ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œåˆ›å»ºæ–°çš„å…ƒæ•°æ®é¡¹
                column_metadata[col_name] = {"value_samples": samples}
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "header_analysis": analysis.to_dict(),
            "column_metadata": column_metadata,
            "column_names": list(df.columns),
            "row_count": len(df),
            "original_file": os.path.basename(filepath)
        }
        metadata_path = os.path.join(output_dir, f"{output_filename}_metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å¤„ç†åçš„JSONå…ƒæ•°æ®ï¼ˆæš‚æ—¶æ³¨é‡Šï¼‰
        # logger.info("=" * 80)
        # logger.info("ğŸ“„ å¤„ç†åçš„JSONå…ƒæ•°æ®:")
        # logger.info("=" * 80)
        # logger.info(json.dumps(metadata, ensure_ascii=False, indent=2))
        # logger.info("=" * 80)
        
        return ExcelProcessResult(
            success=True,
            header_analysis=analysis,
            processed_file_path=csv_path,
            metadata_file_path=metadata_path,
            column_names=list(df.columns),
            column_metadata=column_metadata,
            row_count=len(df),
            error_message=None
        )
        
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return ExcelProcessResult(
            success=False,
            header_analysis=None,
            processed_file_path=None,
            metadata_file_path=None,
            column_names=[],
            column_metadata={},
            row_count=0,
            error_message=error_msg
        )


def get_sheet_names(filepath: str) -> List[str]:
    """è·å–Excelæ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œè¡¨åç§°"""
    try:
        wb = load_workbook(filepath, read_only=True)
        sheets = wb.sheetnames
        wb.close()
        return sheets
    except Exception as e:
        return []


def extract_column_value_samples(
    df: pd.DataFrame,
    max_samples_per_column: int = 10,
    max_unique_ratio: float = 0.5
) -> Dict[str, Dict[str, Any]]:
    """
    æå–æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼æ ·æœ¬ï¼ˆé€šè¿‡åˆ†ç»„èšåˆï¼‰
    
    å‚æ•°:
        df: æ•°æ®æ¡†
        max_samples_per_column: æ¯ä¸ªå­—æ®µæœ€å¤šä¿ç•™çš„æ ·æœ¬æ•°é‡
        max_unique_ratio: å¦‚æœå”¯ä¸€å€¼å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹ï¼Œåˆ™åªæä¾›ç»Ÿè®¡ä¿¡æ¯è€Œä¸ç»Ÿè®¡é¢‘ç‡
    
    è¿”å›:
        å­—å…¸ï¼Œkeyä¸ºåˆ—åï¼Œvalueä¸ºåŒ…å«å¸¸è§å€¼å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    column_samples = {}
    
    for col_name in df.columns:
        col_data = df[col_name]
        
        # è·³è¿‡å®Œå…¨ä¸ºç©ºçš„åˆ—
        if col_data.isna().all():
            continue
        
        # è®¡ç®—éç©ºå€¼æ•°é‡
        non_null_count = col_data.notna().sum()
        if non_null_count == 0:
            continue
        
        # è®¡ç®—å”¯ä¸€å€¼æ•°é‡
        unique_count = col_data.nunique()
        unique_ratio = unique_count / non_null_count if non_null_count > 0 else 1.0
        
        sample_info = {
            "total_count": len(col_data),
            "non_null_count": int(non_null_count),
            "null_count": int(col_data.isna().sum()),
            "unique_count": int(unique_count),
            "data_type": str(col_data.dtype)
        }
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
        is_numeric = pd.api.types.is_numeric_dtype(col_data)
        
        if is_numeric:
            # æ•°å€¼ç±»å‹ï¼šæä¾›ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼ï¼ˆå¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼‰
            sample_info["is_numeric"] = True
            non_null_data = col_data.dropna()
            if len(non_null_data) > 0:
                sample_info["min"] = float(non_null_data.min())
                sample_info["max"] = float(non_null_data.max())
                sample_info["mean"] = float(non_null_data.mean())
                sample_info["median"] = float(non_null_data.median())
            else:
                sample_info["min"] = None
                sample_info["max"] = None
                sample_info["mean"] = None
                sample_info["median"] = None
            
            # å¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œä¹Ÿç»Ÿè®¡é¢‘ç‡
            if unique_ratio <= max_unique_ratio and unique_count <= 100:
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
            elif unique_count <= max_samples_per_column:
                # å³ä½¿å”¯ä¸€å€¼æ¯”ä¾‹é«˜ï¼Œä½†å¦‚æœæ€»æ•°ä¸å¤šï¼Œä¹Ÿå±•ç¤ºæ‰€æœ‰å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œå±•ç¤ºæ‰€æœ‰å€¼"
        else:
            # éæ•°å€¼ç±»å‹ï¼šç»Ÿè®¡é¢‘ç‡
            sample_info["is_numeric"] = False
            
            # å¦‚æœå”¯ä¸€å€¼å¤ªå¤šï¼Œåªæä¾›ç»Ÿè®¡ä¿¡æ¯
            if unique_ratio > max_unique_ratio:
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œä»…å±•ç¤ºéƒ¨åˆ†å¸¸è§å€¼"
                # ä»ç„¶å±•ç¤ºå‰Nä¸ªæœ€å¸¸è§çš„å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
            else:
                # å”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œç»Ÿè®¡æ‰€æœ‰å€¼çš„é¢‘ç‡
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
        
        column_samples[col_name] = sample_info
    
    return column_samples


def _build_column_hierarchy_tree(column_metadata: Dict[str, Dict]) -> str:
    """
    æ„å»ºåˆ—å±‚çº§ç»“æ„çš„æ ‘å½¢å±•ç¤º
    
    å‚æ•°:
        column_metadata: åˆ—å…ƒæ•°æ®å­—å…¸
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æ ‘å½¢ç»“æ„å­—ç¬¦ä¸²
    """
    if not column_metadata:
        return ""
    
    # æ„å»ºæ ‘å½¢ç»“æ„
    tree = {}
    
    for col_name, meta in column_metadata.items():
        # è·å–æ‰€æœ‰å±‚çº§
        levels = []
        level_keys = sorted([k for k in meta.keys() if k.startswith('level')], 
                          key=lambda x: int(x.replace('level', '')))
        for level_key in level_keys:
            value = meta.get(level_key)
            if value and str(value).strip():
                levels.append(str(value).strip())
        
        # å¦‚æœæ²¡æœ‰å±‚çº§ä¿¡æ¯ï¼Œä½¿ç”¨åˆ—åæœ¬èº«
        if not levels:
            levels = [col_name]
        
        # æ„å»ºæ ‘
        current = tree
        for i, level_value in enumerate(levels):
            if level_value not in current:
                current[level_value] = {}
            current = current[level_value]
    
    # é€’å½’ç”Ÿæˆæ ‘å½¢å­—ç¬¦ä¸²
    def _format_tree(node: Dict, prefix: str = "", is_last: bool = True, depth: int = 0) -> List[str]:
        lines = []
        items = list(node.items())
        
        for idx, (key, children) in enumerate(items):
            is_last_item = (idx == len(items) - 1)
            current_prefix = "â””â”€ " if is_last_item else "â”œâ”€ "
            
            if children:
                # æœ‰å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
                next_prefix = prefix + ("   " if is_last_item else "â”‚  ")
                child_lines = _format_tree(children, next_prefix, is_last_item, depth + 1)
                lines.extend(child_lines)
            else:
                # å¶å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
        
        return lines
    
    tree_lines = _format_tree(tree)
    return "\n".join(tree_lines)


def generate_analysis_prompt(
    process_result: ExcelProcessResult,
    custom_prompt: str = None,
    include_metadata: bool = True
) -> str:
    """
    æ ¹æ®Excelå¤„ç†ç»“æœç”Ÿæˆæ•°æ®åˆ†ææç¤ºè¯
    
    å‚æ•°:
        process_result: Excelå¤„ç†ç»“æœ
        custom_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        include_metadata: æ˜¯å¦åŒ…å«åˆ—ç»“æ„å…ƒæ•°æ®
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æç¤ºè¯
    """
    if not process_result.success:
        return ""
    
    # åŸºç¡€ä¿¡æ¯
    prompt_parts = []
    
    # æ·»åŠ è¯­è¨€è¦æ±‚ï¼ˆå¿…é¡»åœ¨æœ€å‰é¢ï¼‰
    prompt_parts.append("**é‡è¦è¦æ±‚ï¼šè¯·ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æå’Œå›ç­”ï¼ŒåŒ…æ‹¬ä»£ç æ³¨é‡Šã€åˆ†ææŠ¥å‘Šç­‰æ‰€æœ‰å†…å®¹ã€‚**")
    prompt_parts.append("")
    prompt_parts.append("**ç¦æ­¢è¦æ±‚ï¼šè¯·ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š**")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ matplotlibã€plotlyã€seaborn ç­‰ç»˜å›¾åº“")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ plt.figure()ã€plt.plot()ã€plt.savefig() ç­‰ç»˜å›¾å‡½æ•°")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ .plot()ã€.hist() ç­‰ pandas ç»˜å›¾æ–¹æ³•")
    prompt_parts.append("- ä¸è¦ä¿å­˜ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼ˆ.pngã€.jpgã€.svg ç­‰ï¼‰")
    prompt_parts.append("**è¯·ä¸“æ³¨äºæ•°æ®åˆ†æå’Œç»Ÿè®¡è®¡ç®—ï¼Œä¸è¦ç”Ÿæˆå¯è§†åŒ–ä»£ç ã€‚**")
    prompt_parts.append("")
    
    if custom_prompt:
        prompt_parts.append(custom_prompt)
    else:
        prompt_parts.append("è¯·å¯¹ä¸Šä¼ çš„æ•°æ®è¿›è¡Œå…¨é¢åˆ†æï¼Œç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šã€‚")
    
    # æ·»åŠ æ•°æ®æ–‡ä»¶ä¿¡æ¯ï¼ˆé‡è¦ï¼šå‘Šè¯‰AIéœ€è¦è¯»å–CSVæ–‡ä»¶ï¼‰
    if process_result.processed_file_path:
        csv_filename = os.path.basename(process_result.processed_file_path)
        prompt_parts.append(f"\n\n## æ•°æ®æ–‡ä»¶")
        prompt_parts.append(f"**é‡è¦ï¼šå·¥ä½œç©ºé—´ä¸­å·²å‡†å¤‡å¥½å¤„ç†åçš„CSVæ•°æ®æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºï¼š`{csv_filename}`**")
        prompt_parts.append(f"")
        prompt_parts.append(f"**è¯·åŠ¡å¿…ä½¿ç”¨ä»¥ä¸‹ä»£ç è¯»å–æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æï¼š**")
        prompt_parts.append(f"```python")
        prompt_parts.append(f"import pandas as pd")
        prompt_parts.append(f"")
        prompt_parts.append(f"# è¯»å–å¤„ç†åçš„CSVæ–‡ä»¶")
        prompt_parts.append(f"df = pd.read_csv('{csv_filename}')")
        prompt_parts.append(f"print(f'æ•°æ®å½¢çŠ¶: {{df.shape}}')")
        prompt_parts.append(f"print(f'åˆ—å: {{list(df.columns)}}')")
        prompt_parts.append(f"```")
        prompt_parts.append(f"")
        prompt_parts.append(f"**æ³¨æ„ï¼š**")
        prompt_parts.append(f"- CSVæ–‡ä»¶å·²ä¿å­˜åœ¨å½“å‰å·¥ä½œç©ºé—´ç›®å½•ä¸­")
        prompt_parts.append(f"- è¯·ä½¿ç”¨ `pd.read_csv('{csv_filename}')` è¯»å–æ•°æ®")
        prompt_parts.append(f"- ä¸è¦ä»…æ ¹æ®å…ƒæ•°æ®è¿›è¡Œåˆ†æï¼Œå¿…é¡»è¯»å–å®é™…æ•°æ®æ–‡ä»¶è¿›è¡Œè®¡ç®—")
        prompt_parts.append(f"")
    
    # æ·»åŠ æ•°æ®æ¦‚å†µ
    prompt_parts.append(f"\n## æ•°æ®æ¦‚å†µ")
    prompt_parts.append(f"- æ•°æ®è¡Œæ•°: {process_result.row_count}")
    prompt_parts.append(f"- åˆ—æ•°: {len(process_result.column_names)}")
    
    # æ·»åŠ è¡¨å¤´ç±»å‹ä¿¡æ¯ï¼ˆä»…ä¿ç•™å¯¹åˆ†ææœ‰ç”¨çš„ä¿¡æ¯ï¼‰
    if process_result.header_analysis:
        ha = process_result.header_analysis
        if ha.header_type == 'multi':
            prompt_parts.append(f"\n## è¡¨å¤´ç»“æ„")
            prompt_parts.append(f"- è¡¨å¤´ç±»å‹: å¤šçº§è¡¨å¤´ï¼ˆ{ha.header_rows}å±‚ï¼‰")
    
    # æ·»åŠ åˆ—ç»“æ„å…ƒæ•°æ®ï¼ˆå¸®åŠ©AIç†è§£åˆ—ä¹‹é—´çš„å…³ç³»ï¼‰
    if include_metadata and process_result.column_metadata:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šçº§ç»“æ„
        has_multi_level = any(
            len(meta) > 1 
            for meta in process_result.column_metadata.values()
        )
        
        if has_multi_level:
            prompt_parts.append(f"\n## åˆ—å±‚çº§ç»“æ„ï¼ˆå¤šçº§è¡¨å¤´è¯­ä¹‰å…³ç³»ï¼‰")
            prompt_parts.append("ä»¥ä¸‹æ ‘å½¢ç»“æ„å±•ç¤ºäº†åˆ—ä¹‹é—´çš„å±‚çº§åˆ†ç»„å…³ç³»ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„ä¸šåŠ¡å«ä¹‰ï¼š")
            prompt_parts.append("")
            hierarchy_tree = _build_column_hierarchy_tree(process_result.column_metadata)
            if hierarchy_tree:
                prompt_parts.append(hierarchy_tree)
            else:
                # å¦‚æœæ ‘å½¢æ„å»ºå¤±è´¥ï¼Œä½¿ç”¨åˆ†ç»„å±•ç¤º
                groups = defaultdict(list)
                for col_name, meta in process_result.column_metadata.items():
                    level1 = meta.get('level1', col_name)
                    groups[level1].append(col_name)
                
                for group, cols in groups.items():
                    if len(cols) > 1:
                        prompt_parts.append(f"- {group}: {', '.join(cols)}")
    
    # æ·»åŠ å®Œæ•´çš„åˆ—ååˆ—è¡¨
    prompt_parts.append(f"\n## å®Œæ•´åˆ—ååˆ—è¡¨")
    if len(process_result.column_names) <= 30:
        # å¦‚æœåˆ—æ•°ä¸å¤šï¼Œå…¨éƒ¨å±•ç¤º
        for idx, col_name in enumerate(process_result.column_names, 1):
            prompt_parts.append(f"{idx}. {col_name}")
    else:
        # å¦‚æœåˆ—æ•°å¾ˆå¤šï¼Œå±•ç¤ºå‰20ä¸ªå’Œå10ä¸ª
        for idx, col_name in enumerate(process_result.column_names[:20], 1):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"... (çœç•¥ä¸­é—´ {len(process_result.column_names) - 30} åˆ—) ...")
        for idx, col_name in enumerate(process_result.column_names[-10:], len(process_result.column_names) - 9):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"\n(å…± {len(process_result.column_names)} åˆ—)")
    
    # æ·»åŠ å­—æ®µå€¼æ ·æœ¬ä¿¡æ¯ï¼ˆä»¥JSONæ ¼å¼æä¾›ï¼Œæ›´ç»“æ„åŒ–ï¼‰
    if include_metadata and process_result.column_metadata:
        prompt_parts.append(f"\n## å­—æ®µå€¼æ ·æœ¬ï¼ˆå¸¸è§å€¼ç»Ÿè®¡ï¼‰")
        prompt_parts.append("ä»¥ä¸‹JSONæ ¼å¼å±•ç¤ºäº†æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼åŠå…¶å‡ºç°é¢‘ç‡ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„å®é™…å†…å®¹ï¼š")
        prompt_parts.append("")
        
        # æ„å»ºåŒ…å«å€¼æ ·æœ¬çš„column_metadata JSON
        column_metadata_with_samples = {}
        for col_name in process_result.column_names:
            if col_name in process_result.column_metadata:
                column_metadata_with_samples[col_name] = process_result.column_metadata[col_name]
        
        # å°†column_metadataè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
        prompt_parts.append("```json")
        prompt_parts.append(json.dumps(column_metadata_with_samples, ensure_ascii=False, indent=2))
        prompt_parts.append("```")
        prompt_parts.append("")
        
        prompt_parts.append("**è¯´æ˜ï¼š**")
        prompt_parts.append("- æ¯ä¸ªå­—æ®µçš„å…ƒæ•°æ®åŒ…å« `value_samples` å­—æ®µï¼Œå…¶ä¸­åŒ…å«è¯¥å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼")
        prompt_parts.append("- `value_samples.top_values` æ•°ç»„å±•ç¤ºäº†å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼åŠå…¶å‡ºç°æ¬¡æ•°")
        prompt_parts.append("- å¯¹äºæ•°å€¼ç±»å‹å­—æ®µï¼Œè¿˜åŒ…å« `min`ã€`max`ã€`mean`ã€`median` ç­‰ç»Ÿè®¡ä¿¡æ¯")
    
    # åœ¨æœ«å°¾å†æ¬¡å¼ºè°ƒè¦æ±‚
    prompt_parts.append("\n\n**å†æ¬¡æé†’ï¼šè¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æã€ä»£ç æ³¨é‡Šå’ŒæŠ¥å‘Šæ’°å†™ï¼Œä¸”ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ã€‚**")
    
    full_prompt = '\n'.join(prompt_parts)
    
    # æ‰“å°ç”Ÿæˆçš„æç¤ºè¯
    logger.info("=" * 80)
    logger.info("ğŸ“ ç”Ÿæˆçš„AIåˆ†ææç¤ºè¯:")
    logger.info("=" * 80)
    logger.info(full_prompt)
    logger.info("=" * 80)
    
    return full_prompt

