"""
LangGraph Data Analysis Workflow

åŸºäº LangGraph 1.0.0+ å®ç°çš„æ•°æ®åˆ†æå·¥ä½œæµå›¾
æ”¯æŒï¼šä»£ç ç”Ÿæˆ â†’ æ‰§è¡Œ â†’ é”™è¯¯ä¿®å¤ â†’ æŠ¥å‘Šç”Ÿæˆ
"""

import re
import os
import shutil
import logging
import threading
import queue
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Generator, Literal

from langgraph.graph import StateGraph, START, END

from .state import AnalysisState, AnalysisPhase, CodeExecution, create_initial_state, AnalysisResult
from .prompts import PromptTemplates

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ============================================================================
# è¯·æ±‚çº§åˆ«çš„é˜Ÿåˆ—ç®¡ç†ï¼ˆè§£å†³å¤šçº¿ç¨‹å¹¶å‘é—®é¢˜ï¼‰
# ============================================================================
# ä½¿ç”¨å­—å…¸å­˜å‚¨æ¯ä¸ªè¯·æ±‚çš„ç‹¬ç«‹é˜Ÿåˆ—ï¼Œé¿å…å…¨å±€é˜Ÿåˆ—è¢«å¤šä¸ªè¯·æ±‚å…±äº«å¯¼è‡´çš„ç«æ€æ¡ä»¶
_request_queues: Dict[str, queue.Queue] = {}
_queues_lock = threading.Lock()


def _create_request_queue(request_id: str) -> queue.Queue:
    """ä¸ºè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„é˜Ÿåˆ—"""
    with _queues_lock:
        q = queue.Queue(maxsize=1000)
        _request_queues[request_id] = q
        logger.debug(f"ğŸ”§ åˆ›å»ºè¯·æ±‚é˜Ÿåˆ—: {request_id}")
        return q


def _get_request_queue(request_id: str) -> Optional[queue.Queue]:
    """è·å–è¯·æ±‚çš„é˜Ÿåˆ—"""
    with _queues_lock:
        return _request_queues.get(request_id)


def _remove_request_queue(request_id: str):
    """ç§»é™¤è¯·æ±‚çš„é˜Ÿåˆ—"""
    with _queues_lock:
        if request_id in _request_queues:
            # æ¸…ç©ºé˜Ÿåˆ—
            q = _request_queues[request_id]
            while not q.empty():
                try:
                    q.get_nowait()
                except queue.Empty:
                    break
            del _request_queues[request_id]
            logger.debug(f"ğŸ§¹ ç§»é™¤è¯·æ±‚é˜Ÿåˆ—: {request_id}")


def _push_to_request_queue(request_id: str, chunk: Optional[str]):
    """æ¨é€åˆ°æŒ‡å®šè¯·æ±‚çš„é˜Ÿåˆ—ï¼ˆchunk ä¸º None è¡¨ç¤ºç»“æŸæ ‡è®°ï¼‰"""
    q = _get_request_queue(request_id)
    if q is not None:
        try:
            q.put(chunk, timeout=0.1)
        except queue.Full:
            # é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡ï¼ˆé¿å…é˜»å¡ï¼‰
            if chunk is not None:
                logger.warning(f"âš ï¸ è¯·æ±‚ {request_id} çš„é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡ chunk")
            pass


# ============================================================================
# LLM å®¢æˆ·ç«¯è¾…åŠ©å‡½æ•°
# ============================================================================

def extract_api_base(api_url: str) -> str:
    """ä»å®Œæ•´çš„API URLä¸­æå–base URL"""
    if api_url.endswith("/chat/completions"):
        return api_url.rsplit("/chat/completions", 1)[0]
    elif "/v1" in api_url:
        return api_url.rsplit("/v1", 1)[0] + "/v1"
    else:
        return api_url


def create_llm_client(api_url: str, api_key: Optional[str] = None):
    """åˆ›å»º OpenAI å…¼å®¹çš„ LLM å®¢æˆ·ç«¯"""
    import openai
    
    api_base = extract_api_base(api_url)
    return openai.OpenAI(
        base_url=api_base,
        api_key=api_key or "dummy",
        timeout=120.0,
    )


def call_llm(
    client,
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.4,
    stream: bool = False,
    stream_callback: Optional[callable] = None,
    push_to_queue: bool = True,
    request_id: Optional[str] = None,  # æ–°å¢ï¼šè¯·æ±‚IDï¼Œç”¨äºå®šä½ç‹¬ç«‹é˜Ÿåˆ—
) -> str:
    """
    è°ƒç”¨ LLM å¹¶è¿”å›å“åº”å†…å®¹
    
    Args:
        client: LLM å®¢æˆ·ç«¯
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        temperature: ç”Ÿæˆæ¸©åº¦
        stream: æ˜¯å¦æµå¼è¾“å‡º
        stream_callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æ¯ä¸ª token (chunk: str) -> None
        push_to_queue: æ˜¯å¦æ¨é€åˆ°æµå¼è¾“å‡ºé˜Ÿåˆ—ï¼ˆé»˜è®¤Trueï¼‰
        request_id: è¯·æ±‚IDï¼Œç”¨äºå®šä½è¯¥è¯·æ±‚çš„ç‹¬ç«‹é˜Ÿåˆ—ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰
    
    Returns:
        å®Œæ•´çš„å“åº”å†…å®¹
    """
    if stream:
        # æµå¼è°ƒç”¨ï¼Œå®æ—¶å›è°ƒ
        # æµå¼è°ƒç”¨æ—¶å¯ç”¨ thinking åŠŸèƒ½ï¼Œä½¿ç”¨æµå¼è°ƒç”¨é¿å…é˜»å¡
        # ä¼˜å…ˆå°è¯• extra_body æ–¹å¼ï¼ˆå…¼å®¹æ›´å¤š APIï¼‰
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                stream=True,  # å¿…é¡»ä½¿ç”¨æµå¼è°ƒç”¨
                extra_body={"enable_thinking": True},  # æµå¼è°ƒç”¨æ—¶å¯ç”¨ thinking
            )
        except Exception:
            # å¦‚æœ extra_body æ–¹å¼å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¼ é€’å‚æ•°
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    stream=True,  # å¿…é¡»ä½¿ç”¨æµå¼è°ƒç”¨
                    enable_thinking=True,  # å°è¯•ç›´æ¥ä¼ é€’å‚æ•°
                )
            except Exception:
                # å¦‚æœå¯ç”¨ thinking å¤±è´¥ï¼Œä»ç„¶ä½¿ç”¨æµå¼è°ƒç”¨ï¼ˆä¸å¯ç”¨ thinkingï¼‰
                # è¿™æ ·å¯ä»¥é¿å…é˜»å¡ï¼Œä¿è¯ç³»ç»Ÿæ­£å¸¸è¿è¡Œ
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    stream=True,  # å¿…é¡»ä¿æŒæµå¼è°ƒç”¨
                )
        
        full_content = ""
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                delta = chunk.choices[0].delta.content
                full_content += delta
                
                # å®æ—¶å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                if stream_callback:
                    stream_callback(delta)
                
                # æ¨é€åˆ°è¯·æ±‚ç‹¬ç«‹çš„é˜Ÿåˆ—ï¼ˆå¦‚æœå¯ç”¨ä¸”æä¾›äº† request_idï¼‰
                if push_to_queue and request_id:
                    _push_to_request_queue(request_id, delta)
        
        return full_content
    else:
        # éæµå¼è°ƒç”¨å·²è¢«ç¦ç”¨ï¼Œå› ä¸ºå¿…é¡»å¯ç”¨ thinking åŠŸèƒ½
        # thinking åŠŸèƒ½åªèƒ½åœ¨æµå¼è°ƒç”¨æ—¶å¯ç”¨ï¼Œéæµå¼è°ƒç”¨ä¸æ”¯æŒ
        # å¼ºåˆ¶ä½¿ç”¨æµå¼è°ƒç”¨ä»¥ç¡®ä¿ thinking åŠŸèƒ½å¯ç”¨
        raise ValueError(
            "éæµå¼è°ƒç”¨å·²è¢«ç¦ç”¨ã€‚å¿…é¡»ä½¿ç”¨æµå¼è°ƒç”¨ï¼ˆstream=Trueï¼‰ä»¥å¯ç”¨ thinking åŠŸèƒ½ã€‚"
            "è¯·ç¡®ä¿æ‰€æœ‰ call_llm è°ƒç”¨éƒ½ä½¿ç”¨ stream=True å‚æ•°ã€‚"
        )


def call_llm_stream(
    client,
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.4,
) -> Generator[str, None, str]:
    """æµå¼è°ƒç”¨ LLMï¼Œyield æ¯ä¸ª tokenï¼Œæœ€åè¿”å›å®Œæ•´å†…å®¹"""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        stream=True,
    )
    
    full_content = ""
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            delta = chunk.choices[0].delta.content
            full_content += delta
            yield delta
    
    return full_content


# ============================================================================
# ä»£ç æå–è¾…åŠ©å‡½æ•°
# ============================================================================

def extract_python_code(text: str) -> Optional[str]:
    """ä» LLM å“åº”ä¸­æå– Python ä»£ç å—"""
    # åŒ¹é… ```python ... ``` æ ¼å¼
    pattern = r"```python\s*(.*?)```"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # åŒ¹é… ``` ... ``` æ ¼å¼ï¼ˆæ— è¯­è¨€æ ‡è®°ï¼‰
    pattern2 = r"```\s*(.*?)```"
    match2 = re.search(pattern2, text, re.DOTALL)
    if match2:
        code = match2.group(1).strip()
        # ç®€å•åˆ¤æ–­æ˜¯å¦åƒ Python ä»£ç 
        if "import " in code or "print(" in code or "def " in code:
            return code
    
    return None


def has_python_code(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å« Python ä»£ç å—"""
    return extract_python_code(text) is not None


def is_execution_error(output: str) -> bool:
    """æ£€æŸ¥æ‰§è¡Œè¾“å‡ºæ˜¯å¦åŒ…å«é”™è¯¯"""
    error_indicators = [
        "[Error]",
        "[Timeout]",
        "Traceback (most recent call last)",
        "Error:",
        "Exception:",
        "SyntaxError:",
        "NameError:",
        "TypeError:",
        "ValueError:",
        "KeyError:",
        "IndexError:",
        "FileNotFoundError:",
        "ModuleNotFoundError:",
    ]
    return any(indicator in output for indicator in error_indicators)


# ============================================================================
# å·¥ä½œæµèŠ‚ç‚¹å‡½æ•°
# ============================================================================

def analyze_intent_node(state: AnalysisState) -> Dict[str, Any]:
    """
    æ„å›¾åˆ†æå’Œç­–ç•¥åˆ¶å®šèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼š
    1. åˆ¤æ–­ç”¨æˆ·è¾“å…¥ä¸æ•°æ®çš„ç›¸å…³æ€§
    2. å¦‚æœæ— å…³ï¼Œè¿”å›æ¾„æ¸…æ¶ˆæ¯
    3. å¦‚æœç›¸å…³ï¼Œé‡å†™ç”¨æˆ·éœ€æ±‚å¹¶åˆ¶å®šåˆ†æç­–ç•¥
    """
    logger.info("ğŸ” [Node] æ„å›¾åˆ†æèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # æ„å»ºæ„å›¾åˆ†æ prompt
    messages = PromptTemplates.format_intent_analysis_prompt(
        csv_path=state["csv_path"],
        row_count=state["row_count"],
        column_names=state["column_names"],
        column_metadata=state["column_metadata"],
        data_preview=state["data_preview"],
        user_prompt=state["user_prompt"],
    )
    
    # æ”¶é›†æµå¼è¾“å‡ºçš„åˆ—è¡¨ï¼ˆç”¨äºåç»­æ ¼å¼åŒ–ï¼‰
    stream_chunks = []
    
    def stream_callback(chunk: str):
        """æµå¼è¾“å‡ºå›è°ƒï¼Œåªæ”¶é›† tokenï¼Œä¸æ¨é€åˆ°é˜Ÿåˆ—ï¼ˆé¿å…è¾“å‡ºJSONï¼‰"""
        stream_chunks.append(chunk)
        # æ³¨æ„ï¼šä¸æ¨é€åˆ°é˜Ÿåˆ—ï¼Œé¿å…ç›´æ¥è¾“å‡ºJSONå†…å®¹
    
    # æµå¼è°ƒç”¨ LLMï¼Œæ”¶é›†è¾“å‡ºä½†ä¸å®æ—¶æ¨é€ï¼ˆé¿å…è¾“å‡ºJSONï¼‰
    # ä½¿ç”¨æµå¼è°ƒç”¨ä»¥æ”¯æŒ think åŠŸèƒ½ï¼Œä½†ä¸ç›´æ¥è¾“å‡ºå†…å®¹
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        stream_callback=stream_callback,
        push_to_queue=False,  # ä¸æ¨é€åˆ°é˜Ÿåˆ—ï¼Œé¿å…è¾“å‡ºJSON
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info("ğŸ” [æ„å›¾åˆ†æ] LLM å®Œæ•´å“åº”:")
    logger.info("=" * 80)
    logger.info(response)
    logger.info("=" * 80)
    
    # è§£æ JSON å“åº”
    import json
    try:
        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç å—ï¼‰
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            json_str = response
        
        intent_result = json.loads(json_str.strip())
    except (json.JSONDecodeError, AttributeError) as e:
        logger.warning(f"âš ï¸ [Node] æ— æ³•è§£ææ„å›¾åˆ†æç»“æœ: {e}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤ç»§ç»­åˆ†æ
        intent_result = {
            "is_relevant": True,
            "needs_clarification": False,
            "refined_prompt": state["user_prompt"],
            "analysis_strategy": "åŸºäºç”¨æˆ·éœ€æ±‚è¿›è¡Œæ•°æ®åˆ†æ",
            "research_directions": ["æ•°æ®æ¦‚è§ˆ", "ç»Ÿè®¡åˆ†æ"],
        }
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ç”¨æˆ·æ¾„æ¸…
    is_relevant = intent_result.get("is_relevant", True)
    needs_clarification = intent_result.get("needs_clarification", False)
    
    if not is_relevant:
        # æ•°æ®ä¸ç”¨æˆ·è¾“å…¥æ— å…³
        clarification_msg = intent_result.get(
            "clarification_message",
            "æ‚¨çš„é—®é¢˜ä¸å½“å‰æ•°æ®æ–‡ä»¶ä¸ç›¸å…³ã€‚è¯·æä¾›ä¸æ•°æ®ç›¸å…³çš„åˆ†æéœ€æ±‚ï¼Œæˆ–ä¸Šä¼ æ­£ç¡®çš„æ•°æ®æ–‡ä»¶ã€‚"
        )
        logger.warning(f"âš ï¸ [Node] ç”¨æˆ·è¾“å…¥ä¸æ•°æ®æ— å…³: {clarification_msg}")
        _push_to_request_queue(request_id, f"\n\nâš ï¸ **éœ€è¦æ¾„æ¸…**\n\n{clarification_msg}\n\n")
        # æ³¨æ„ï¼šæ¾„æ¸…æ¶ˆæ¯å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶é€šè¿‡é˜Ÿåˆ—å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        return {
            "phase": AnalysisPhase.USER_CLARIFICATION_NEEDED.value,
            "needs_clarification": True,
            "clarification_message": clarification_msg,
            "intent_analysis_result": response,
            "stream_output": [],  # é¿å…é‡å¤æ¨é€
        }
    
    if needs_clarification:
        # éœ€è¦ç”¨æˆ·æ¾„æ¸…
        clarification_msg = intent_result.get(
            "clarification_message",
            "æ‚¨çš„åˆ†æéœ€æ±‚ä¸å¤Ÿæ˜ç¡®ï¼Œè¯·æä¾›æ›´å…·ä½“çš„åˆ†æè¦æ±‚ã€‚"
        )
        logger.info(f"â„¹ï¸ [Node] éœ€è¦ç”¨æˆ·æ¾„æ¸…: {clarification_msg}")
        _push_to_request_queue(request_id, f"\n\nâ“ **éœ€è¦æ¾„æ¸…**\n\n{clarification_msg}\n\n")
        # æ³¨æ„ï¼šæ¾„æ¸…æ¶ˆæ¯å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶é€šè¿‡é˜Ÿåˆ—å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        return {
            "phase": AnalysisPhase.USER_CLARIFICATION_NEEDED.value,
            "needs_clarification": True,
            "clarification_message": clarification_msg,
            "intent_analysis_result": response,
            "stream_output": [],  # é¿å…é‡å¤æ¨é€
        }
    
    # å¯ä»¥ç»§ç»­åˆ†æ
    refined_prompt = intent_result.get("refined_prompt", state["user_prompt"])
    analysis_strategy = intent_result.get("analysis_strategy", "")
    research_directions = intent_result.get("research_directions", [])
    
    logger.info(f"âœ… [Node] æ„å›¾åˆ†æå®Œæˆ")
    logger.info(f"   - é‡å†™åçš„éœ€æ±‚: {refined_prompt[:100]}...")
    logger.info(f"   - åˆ†æç­–ç•¥: {analysis_strategy[:100]}...")
    logger.info(f"   - ç ”ç©¶æ–¹å‘: {research_directions}")
    
    # åªè¾“å‡ºåˆ†æç­–ç•¥å’Œç ”ç©¶æ–¹å‘ï¼Œä¸è¾“å‡ºæ ‡é¢˜å’Œé‡å†™åçš„éœ€æ±‚
    if analysis_strategy:
        _push_to_request_queue(request_id, f"**åˆ†æç­–ç•¥ï¼š**\n{analysis_strategy}\n\n")
    
    if research_directions:
        _push_to_request_queue(request_id, f"**ç ”ç©¶æ–¹å‘ï¼š**\n")
        for i, direction in enumerate(research_directions, 1):
            _push_to_request_queue(request_id, f"{i}. {direction}\n")
        _push_to_request_queue(request_id, "\n")
    
    # æ„å»ºæµå¼è¾“å‡ºï¼ˆç”¨äºçŠ¶æ€è®°å½•ï¼‰
    # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹ï¼ˆæ ‡é¢˜ã€æµå¼tokenã€æ ¼å¼åŒ–ç»“æœï¼‰éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
    # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
    stream_output = []
    
    return {
        "phase": AnalysisPhase.CODE_GENERATION.value,
        "refined_prompt": refined_prompt,
        "analysis_strategy": analysis_strategy,
        "research_directions": research_directions,
        "intent_analysis_result": response,
        "needs_clarification": False,
        "messages": messages + [{"role": "assistant", "content": response}],
        "stream_output": stream_output,  # æµå¼è¾“å‡ºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½ä¼šå®æ—¶ä¼ é€’
    }


def generate_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç ç”ŸæˆèŠ‚ç‚¹
    
    æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œæ•°æ®ä¿¡æ¯ï¼Œè°ƒç”¨ LLM ç”Ÿæˆ Python åˆ†æä»£ç 
    """
    logger.info("ğŸ“ [Node] ä»£ç ç”ŸæˆèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # ä½¿ç”¨é‡å†™åçš„ç”¨æˆ·éœ€æ±‚ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹éœ€æ±‚
    user_prompt = state.get("refined_prompt") or state["user_prompt"]
    
    # æ„å»º prompt
    messages = PromptTemplates.format_code_generation_prompt(
        csv_path=state["csv_path"],
        row_count=state["row_count"],
        column_names=state["column_names"],
        column_metadata=state["column_metadata"],
        data_preview=state["data_preview"],
        user_prompt=user_prompt,
    )
    
    # æ”¶é›†æµå¼è¾“å‡ºçš„åˆ—è¡¨ï¼ˆç”¨äºåç»­æ ¼å¼åŒ–ï¼‰
    stream_chunks = []
    
    def stream_callback(chunk: str):
        """æµå¼è¾“å‡ºå›è°ƒï¼Œæ”¶é›† tokenï¼ˆåŒæ—¶ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰"""
        stream_chunks.append(chunk)
    
    # å…ˆè¾“å‡ºæ ‡é¢˜ï¼ˆå®æ—¶ä¼ é€’ï¼‰
    _push_to_request_queue(request_id, "\nğŸ“ **æ­£åœ¨ç”Ÿæˆåˆ†æä»£ç ...**\n\n")
    
    # æµå¼è°ƒç”¨ LLMï¼Œå®æ—¶æ”¶é›†è¾“å‡ºï¼ˆæ¯ä¸ª token ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        stream_callback=stream_callback,
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info("ğŸ“ [ä»£ç ç”Ÿæˆ] LLM å®Œæ•´å“åº”:")
    logger.info("=" * 80)
    logger.info(response)
    logger.info("=" * 80)
    
    # æå–ä»£ç 
    code = extract_python_code(response)
    
    if code:
        logger.info(f"âœ… [Node] æˆåŠŸç”Ÿæˆä»£ç ï¼Œé•¿åº¦: {len(code)} å­—ç¬¦")
        # æ³¨æ„ï¼šä»£ç å·²ç»åœ¨æµå¼è°ƒç”¨æ—¶å®æ—¶æ¨é€è¿‡äº†ï¼Œä¸éœ€è¦å†æ¬¡æ¨é€æ ¼å¼åŒ–ä»£ç 
        
        # æ„å»ºæµå¼è¾“å‡ºï¼ˆç”¨äºçŠ¶æ€è®°å½•ï¼‰
        # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹ï¼ˆæ ‡é¢˜ã€æµå¼tokenï¼‰éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        stream_output = []
        
        return {
            "phase": AnalysisPhase.CODE_EXECUTION.value,
            "current_code": code,
            "code_history": [code],
            "messages": messages + [{"role": "assistant", "content": response}],
            "stream_output": stream_output,
        }
    else:
        logger.warning("âš ï¸ [Node] æœªèƒ½ä» LLM å“åº”ä¸­æå–ä»£ç ")
        _push_to_request_queue(request_id, f"\n\nâš ï¸ æœªç”Ÿæˆä»£ç ï¼ŒLLM ç›´æ¥è¿”å›ï¼š\n\n{response}\n\n")
        
        # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        stream_output = []
        
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "current_output": response,
            "messages": messages + [{"role": "assistant", "content": response}],
            "stream_output": stream_output,
        }


def execute_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç æ‰§è¡ŒèŠ‚ç‚¹
    
    åœ¨æœ¬åœ°å®‰å…¨ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆçš„ Python ä»£ç 
    """
    logger.info("â–¶ï¸ [Node] ä»£ç æ‰§è¡ŒèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    # å¯¼å…¥æ‰§è¡Œå‡½æ•°
    from ..utils import execute_code_safe
    
    code = state["current_code"]
    workspace_dir = state["workspace_dir"]
    
    # æ·»åŠ  matplotlib ä¸­æ–‡æ”¯æŒ
    chinese_matplot_setup = '''
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import warnings

chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC']
available_fonts = [f.name for f in fm.fontManager.ttflist]
chinese_font = next((f for f in chinese_fonts if f in available_fonts), None)

if chinese_font:
    plt.rcParams['font.sans-serif'] = [chinese_font] + plt.rcParams['font.sans-serif']
else:
    warnings.filterwarnings('ignore', category=UserWarning, message='.*Glyph.*missing.*')
plt.rcParams['axes.unicode_minus'] = False
'''
    
    full_code = chinese_matplot_setup + "\n" + code
    
    # åœ¨æ‰§è¡Œä»£ç å‰ï¼Œè®°å½•å·²æœ‰çš„CSVæ–‡ä»¶ï¼ˆç”¨äºæ£€æµ‹æ–°ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
    workspace_path = Path(workspace_dir)
    existing_csv_files = set()
    if workspace_path.exists():
        for csv_file in workspace_path.rglob("*.csv"):
            existing_csv_files.add(csv_file.resolve())
    
    # æ‰§è¡Œä»£ç 
    logger.info(f"â³ æ‰§è¡Œä»£ç ï¼Œå·¥ä½œç›®å½•: {workspace_dir}")
    output = execute_code_safe(full_code, workspace_dir)
    logger.info(f"ğŸ“Š ä»£ç æ‰§è¡Œå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
    
    # æ£€æŸ¥æ‰§è¡Œç»“æœ
    success = not is_execution_error(output)
    
    # åˆ›å»ºæ‰§è¡Œè®°å½•
    execution = CodeExecution(
        code=code,
        output=output,
        success=success,
        error_message=output if not success else None,
        attempt=state.get("retry_count", 0) + 1,
    )
    
    if success:
        logger.info("âœ… [Node] ä»£ç æ‰§è¡ŒæˆåŠŸ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç”Ÿæˆçš„CSVæ–‡ä»¶ï¼Œå¹¶å¤åˆ¶åˆ°/tmpï¼ˆç‰¹åˆ«æ˜¯ä¿®å¤åçš„ä»£ç æ‰§è¡Œï¼‰
        retry_count = state.get("retry_count", 0)
        if retry_count > 0:  # å¦‚æœæ˜¯ä¿®å¤åçš„ä»£ç æ‰§è¡Œ
            try:
                new_csv_files = []
                if workspace_path.exists():
                    for csv_file in workspace_path.rglob("*.csv"):
                        csv_resolved = csv_file.resolve()
                        if csv_resolved not in existing_csv_files:
                            new_csv_files.append(csv_resolved)
                
                if new_csv_files:
                    tmp_dir = Path("/tmp")
                    tmp_dir.mkdir(exist_ok=True)
                    logger.info(f"ğŸ“ æ£€æµ‹åˆ° {len(new_csv_files)} ä¸ªæ–°ç”Ÿæˆçš„CSVæ–‡ä»¶ï¼Œå¤åˆ¶åˆ° /tmp ç›®å½•...")
                    
                    for csv_file in new_csv_files:
                        try:
                            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’ŒåŸå§‹æ–‡ä»¶åï¼‰
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            base_name = csv_file.stem
                            dest_name = f"{base_name}_{timestamp}.csv"
                            dest_path = tmp_dir / dest_name
                            
                            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ åºå·
                            counter = 1
                            while dest_path.exists():
                                dest_name = f"{base_name}_{timestamp}_{counter}.csv"
                                dest_path = tmp_dir / dest_name
                                counter += 1
                            
                            shutil.copy2(str(csv_file), str(dest_path))
                            logger.info(f"   âœ… å·²å¤åˆ¶: {csv_file.name} â†’ /tmp/{dest_name}")
                        except Exception as e:
                            logger.warning(f"   âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {csv_file.name}: {e}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥å¹¶å¤åˆ¶CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¾“å‡ºæ‰§è¡Œç»“æœ
        debug_print = state.get("debug_print_execution_output", False)
        if debug_print:
            _push_to_request_queue(request_id, "\nâœ… **ä»£ç æ‰§è¡Œå®Œæ¯•**\n\n")
            _push_to_request_queue(request_id, "ğŸ“Š **æ‰§è¡Œç»“æœï¼š**\n\n")
            _push_to_request_queue(request_id, f"```\n{output}\n```\n\n")
            _push_to_request_queue(request_id, "æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...\n\n")
        else:
            # é»˜è®¤ä¸æ˜¾ç¤ºå…·ä½“æ‰§è¡Œç»“æœ
            _push_to_request_queue(request_id, "\nâœ… **ä»£ç æ‰§è¡Œå®Œæ¯•ï¼Œæ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...**\n\n")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "current_output": output,
            "execution_success": True,
            "execution_history": [execution],
            "round_count": state.get("round_count", 0) + 1,
            "stream_output": [],
        }
    else:
        logger.warning(f"âŒ [Node] ä»£ç æ‰§è¡Œå¤±è´¥: {output[:200]}...")
        return {
            "phase": AnalysisPhase.ERROR_FIXING.value,
            "current_output": output,
            "execution_success": False,
            "error_message": output,
            "execution_history": [execution],
            "stream_output": [f"\nâŒ **æ‰§è¡Œå‡ºé”™ï¼š**\n\n```\n{output}\n```\n\n"],
        }


def fix_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç ä¿®å¤èŠ‚ç‚¹
    
    å½“ä»£ç æ‰§è¡Œå¤±è´¥æ—¶ï¼Œè°ƒç”¨ LLM ä¿®å¤ä»£ç 
    """
    logger.info("ğŸ”§ [Node] ä»£ç ä¿®å¤èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    retry_count = state.get("retry_count", 0) + 1
    max_retries = 3
    
    if retry_count > max_retries:
        logger.error(f"âŒ [Node] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "retry_count": retry_count,
            "stream_output": [f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œè·³è¿‡ä»£ç æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š\n\n"],
        }
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # æ„å»ºä¿®å¤ prompt
    messages = PromptTemplates.format_code_fix_prompt(
        original_code=state["current_code"],
        error_message=state.get("error_message", "æœªçŸ¥é”™è¯¯"),
        csv_path=state["csv_path"],
        column_names=state["column_names"],
    )
    
    # æ”¶é›†æµå¼è¾“å‡ºçš„åˆ—è¡¨ï¼ˆç”¨äºåç»­æ ¼å¼åŒ–ï¼‰
    stream_chunks = []
    
    def stream_callback(chunk: str):
        """æµå¼è¾“å‡ºå›è°ƒï¼Œæ”¶é›† tokenï¼ˆåŒæ—¶ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰"""
        stream_chunks.append(chunk)
    
    # å…ˆè¾“å‡ºæ ‡é¢˜ï¼ˆå®æ—¶ä¼ é€’ï¼‰
    _push_to_request_queue(request_id, f"\nğŸ”§ **æ­£åœ¨ä¿®å¤ä»£ç ï¼ˆå°è¯• {retry_count}/{max_retries}ï¼‰...**\n\n")
    
    # æµå¼è°ƒç”¨ LLM ä¿®å¤ï¼ˆæ¯ä¸ª token ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        stream_callback=stream_callback,
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info(f"ğŸ”§ [ä»£ç ä¿®å¤] LLM å®Œæ•´å“åº” (å°è¯• {retry_count}/{max_retries}):")
    logger.info("=" * 80)
    logger.info(response)
    logger.info("=" * 80)
    
    # æå–ä¿®å¤åçš„ä»£ç 
    fixed_code = extract_python_code(response)
    
    if fixed_code:
        logger.info(f"âœ… [Node] æˆåŠŸè·å–ä¿®å¤ä»£ç ï¼Œé‡è¯•æ¬¡æ•°: {retry_count}")
        # æ³¨æ„ï¼šä»£ç å·²ç»åœ¨æµå¼è°ƒç”¨æ—¶å®æ—¶æ¨é€è¿‡äº†ï¼Œä¸éœ€è¦å†æ¬¡æ¨é€æ ¼å¼åŒ–ä»£ç 
        
        # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹ï¼ˆæ ‡é¢˜ã€æµå¼tokenï¼‰éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        stream_output = []
        
        return {
            "phase": AnalysisPhase.CODE_EXECUTION.value,
            "current_code": fixed_code,
            "code_history": [fixed_code],
            "retry_count": retry_count,
            "stream_output": stream_output,
        }
    else:
        logger.warning("âš ï¸ [Node] æœªèƒ½ä»ä¿®å¤å“åº”ä¸­æå–ä»£ç ")
        _push_to_request_queue(request_id, f"\n\nâš ï¸ æ— æ³•ä¿®å¤ä»£ç ï¼Œè·³è¿‡æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š\n\n")
        
        # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        stream_output = []
        
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "retry_count": retry_count,
            "stream_output": stream_output,
        }


def generate_report_node(state: AnalysisState) -> Dict[str, Any]:
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    
    æ ¹æ®ä»£ç æ‰§è¡Œç»“æœï¼Œè°ƒç”¨ LLM ç”Ÿæˆåˆ†ææŠ¥å‘Š
    """
    logger.info("ğŸ“„ [Node] æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # è·å–æœ€åæ‰§è¡Œçš„ä»£ç å’Œè¾“å‡º
    code = state.get("current_code", "")
    output = state.get("current_output", "")
    
    # å¦‚æœæ²¡æœ‰æ‰§è¡Œè¾“å‡ºï¼Œä½¿ç”¨ä»£ç å†å²ä¸­çš„æœ€åä¸€ä¸ª
    if not output and state.get("execution_history"):
        last_execution = state["execution_history"][-1]
        code = last_execution.code
        output = last_execution.output
    
    # æ„å»ºæŠ¥å‘Š promptï¼ˆåŒ…å«è¡¨å¤´å…ƒæ•°æ®ï¼‰
    messages = PromptTemplates.format_report_generation_prompt(
        user_prompt=state["user_prompt"],
        code=code,
        execution_output=output,
        column_names=state.get("column_names", []),
        column_metadata=state.get("column_metadata", {}),
    )
    
    # æ”¶é›†æµå¼è¾“å‡ºçš„åˆ—è¡¨ï¼ˆç”¨äºåç»­æ ¼å¼åŒ–ï¼‰
    stream_chunks = []
    
    def stream_callback(chunk: str):
        """æµå¼è¾“å‡ºå›è°ƒï¼Œæ”¶é›† tokenï¼ˆåŒæ—¶ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰"""
        stream_chunks.append(chunk)
    
    # å…ˆè¾“å‡ºæ ‡é¢˜ï¼ˆå®æ—¶ä¼ é€’ï¼‰
    _push_to_request_queue(request_id, "\nğŸ“„ **æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...**\n\n")
    
    # æµå¼è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Šï¼ˆæ¯ä¸ª token ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰
    report = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        stream_callback=stream_callback,
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info("ğŸ“„ [æŠ¥å‘Šç”Ÿæˆ] LLM å®Œæ•´å“åº”:")
    logger.info("=" * 80)
    logger.info(report)
    logger.info("=" * 80)
    
    logger.info(f"âœ… [Node] æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼Œé•¿åº¦: {len(report)} å­—ç¬¦")
    
    # æ„å»ºæµå¼è¾“å‡ºï¼ˆç”¨äºçŠ¶æ€è®°å½•ï¼‰
    # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹ï¼ˆæ ‡é¢˜ã€æµå¼tokenï¼‰éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
    # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
    stream_output = []
    
    return {
        "phase": AnalysisPhase.COMPLETED.value,
        "report": report,
        "stream_output": stream_output,
    }


# ============================================================================
# æ¡ä»¶è·¯ç”±å‡½æ•°
# ============================================================================

def route_after_execution(state: AnalysisState) -> Literal["fix_code", "generate_report"]:
    """
    æ‰§è¡Œåè·¯ç”±å†³ç­–
    
    æ ¹æ®æ‰§è¡Œç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - æ‰§è¡ŒæˆåŠŸ â†’ ç”ŸæˆæŠ¥å‘Š
    - æ‰§è¡Œå¤±è´¥ â†’ ä¿®å¤ä»£ç 
    """
    if state.get("execution_success", False):
        return "generate_report"
    else:
        return "fix_code"


def route_after_fix(state: AnalysisState) -> Literal["execute_code", "generate_report"]:
    """
    ä¿®å¤åè·¯ç”±å†³ç­–
    
    æ ¹æ®ä¿®å¤ç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - æœ‰æ–°ä»£ç  â†’ é‡æ–°æ‰§è¡Œ
    - æ— æ³•ä¿®å¤ â†’ ç”ŸæˆæŠ¥å‘Š
    """
    if state.get("phase") == AnalysisPhase.CODE_EXECUTION.value:
        return "execute_code"
    else:
        return "generate_report"


# ============================================================================
# å·¥ä½œæµå›¾æ„å»º
# ============================================================================

def create_analysis_graph() -> StateGraph:
    """
    åˆ›å»ºæ•°æ®åˆ†æå·¥ä½œæµå›¾
    
    å·¥ä½œæµç»“æ„ï¼š
    
    START â†’ analyze_intent â”€â”¬â”€(éœ€è¦æ¾„æ¸…)â”€â†’ END
                            â”‚
                            â””â”€(å¯ä»¥åˆ†æ)â”€â†’ generate_code â†’ execute_code â”€â”¬â”€(æˆåŠŸ)â”€â†’ generate_report â†’ END
                                                                          â”‚
                                                                          â””â”€(å¤±è´¥)â”€â†’ fix_code â”€â”¬â”€(æœ‰ä¿®å¤)â”€â†’ execute_code
                                                                                              â”‚
                                                                                              â””â”€(æ— æ³•ä¿®å¤)â”€â†’ generate_report
    
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AnalysisState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("analyze_intent", analyze_intent_node)
    workflow.add_node("generate_code", generate_code_node)
    workflow.add_node("execute_code", execute_code_node)
    workflow.add_node("fix_code", fix_code_node)
    workflow.add_node("generate_report", generate_report_node)
    
    # æ·»åŠ è¾¹
    # START â†’ analyze_intent
    workflow.add_edge(START, "analyze_intent")
    
    # analyze_intent â†’ generate_code æˆ– ENDï¼ˆéœ€è¦æ¾„æ¸…ï¼‰
    def route_after_intent(state: AnalysisState) -> Literal["generate_code", "end"]:
        """æ„å›¾åˆ†æåçš„è·¯ç”±å†³ç­–"""
        phase = state.get("phase", "")
        if phase == AnalysisPhase.CODE_GENERATION.value:
            return "generate_code"
        else:
            # éœ€è¦æ¾„æ¸…æˆ–å…¶ä»–æƒ…å†µï¼Œç›´æ¥ç»“æŸ
            return "end"
    
    workflow.add_conditional_edges(
        "analyze_intent",
        route_after_intent,
        {
            "generate_code": "generate_code",
            "end": END,
        }
    )
    
    # generate_code â†’ execute_code (å¦‚æœç”Ÿæˆäº†ä»£ç )
    workflow.add_conditional_edges(
        "generate_code",
        lambda s: "execute_code" if s.get("phase") == AnalysisPhase.CODE_EXECUTION.value else "generate_report",
        {
            "execute_code": "execute_code",
            "generate_report": "generate_report",
        }
    )
    
    # å¤„ç†éœ€è¦æ¾„æ¸…çš„æƒ…å†µï¼ˆç›´æ¥ç»“æŸï¼‰
    # æ³¨æ„ï¼šanalyze_intent èŠ‚ç‚¹å¦‚æœè¿”å› USER_CLARIFICATION_NEEDEDï¼Œä¼šé€šè¿‡æ¡ä»¶è¾¹è·¯ç”±åˆ° END
    
    # execute_code â†’ fix_code æˆ– generate_report
    workflow.add_conditional_edges(
        "execute_code",
        route_after_execution,
        {
            "fix_code": "fix_code",
            "generate_report": "generate_report",
        }
    )
    
    # fix_code â†’ execute_code æˆ– generate_report
    workflow.add_conditional_edges(
        "fix_code",
        route_after_fix,
        {
            "execute_code": "execute_code",
            "generate_report": "generate_report",
        }
    )
    
    # generate_report â†’ END
    workflow.add_edge("generate_report", END)
    
    return workflow


# ============================================================================
# é«˜çº§å°è£…ç±»
# ============================================================================

class DataAnalysisGraph:
    """
    æ•°æ®åˆ†æå›¾å°è£…ç±»
    
    æä¾›ç®€åŒ–çš„ API ç”¨äºæ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµå›¾"""
        self._workflow = create_analysis_graph()
        self._graph = self._workflow.compile()
    
    def analyze(
        self,
        workspace_dir: str,
        thread_id: str,
        csv_path: str,
        column_names: List[str],
        column_metadata: Dict[str, Any],
        row_count: int,
        data_preview: str,
        user_prompt: str,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        temperature: float = 0.4,
    ) -> AnalysisResult:
        """
        æ‰§è¡Œæ•°æ®åˆ†æï¼ˆéæµå¼ï¼‰
        
        Args:
            workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
            thread_id: ä¼šè¯ID
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            column_names: åˆ—ååˆ—è¡¨
            column_metadata: åˆ—å…ƒæ•°æ®
            row_count: æ•°æ®è¡Œæ•°
            data_preview: æ•°æ®é¢„è§ˆ
            user_prompt: ç”¨æˆ·åˆ†æéœ€æ±‚
            api_url: LLM API åœ°å€
            model: æ¨¡å‹åç§°
            api_key: LLM API å¯†é’¥
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            AnalysisResult åˆ†æç»“æœ
        """
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = create_initial_state(
            workspace_dir=workspace_dir,
            thread_id=thread_id,
            csv_path=csv_path,
            column_names=column_names,
            column_metadata=column_metadata,
            row_count=row_count,
            data_preview=data_preview,
            user_prompt=user_prompt,
            api_url=api_url,
            model=model,
            api_key=api_key,
            temperature=temperature,
        )
        
        # æ‰§è¡Œå·¥ä½œæµ
        final_state = self._graph.invoke(initial_state)
        
        # æ„å»ºç»“æœ
        return AnalysisResult(
            success=final_state.get("phase") == AnalysisPhase.COMPLETED.value,
            report=final_state.get("report", ""),
            code_history=final_state.get("code_history", []),
            execution_outputs=[
                e.output for e in final_state.get("execution_history", [])
            ],
            generated_files=final_state.get("generated_files", []),
            error_message=final_state.get("error_message"),
            total_rounds=final_state.get("round_count", 0),
        )
    
    def analyze_stream(
        self,
        workspace_dir: str,
        thread_id: str,
        csv_path: str,
        column_names: List[str],
        column_metadata: Dict[str, Any],
        row_count: int,
        data_preview: str,
        user_prompt: str,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        temperature: float = 0.4,
        analysis_timeout: Optional[int] = None,
        debug_print_execution_output: bool = False,
    ) -> Generator[str, None, AnalysisResult]:
        """
        æ‰§è¡Œæ•°æ®åˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰
        
        ä½¿ç”¨ LangGraph çš„ stream æ¨¡å¼ + çº¿ç¨‹é˜Ÿåˆ—å®ç°çœŸæ­£çš„å®æ—¶æµå¼è¾“å‡º
        åœ¨èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼ŒLLM çš„æ¯ä¸ª token éƒ½ä¼šå®æ—¶ä¼ é€’
        
        æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„é˜Ÿåˆ—ï¼Œç¡®ä¿å¤šçº¿ç¨‹å®‰å…¨ã€‚
        
        Yields:
            str: æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
            
        Returns:
            AnalysisResult åˆ†æç»“æœ
        """
        import uuid
        
        # ä¸ºæ¯ä¸ªè¯·æ±‚ç”Ÿæˆå”¯ä¸€çš„ request_idï¼ˆç”¨äºé˜Ÿåˆ—éš”ç¦»ï¼‰
        request_id = f"req-{uuid.uuid4().hex[:16]}"
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æè¯·æ±‚: {request_id}")
        
        # ä¸ºè¯¥è¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„é˜Ÿåˆ—ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰
        request_queue = _create_request_queue(request_id)
        
        try:
            # åˆ›å»ºåˆå§‹çŠ¶æ€ï¼ˆåŒ…å« request_idï¼‰
            initial_state = create_initial_state(
                workspace_dir=workspace_dir,
                thread_id=thread_id,
                csv_path=csv_path,
                column_names=column_names,
                column_metadata=column_metadata,
                row_count=row_count,
                data_preview=data_preview,
                user_prompt=user_prompt,
                api_url=api_url,
                model=model,
                api_key=api_key,
                temperature=temperature,
                request_id=request_id,  # ä¼ é€’è¯·æ±‚ID
                debug_print_execution_output=debug_print_execution_output,  # ä¼ é€’è°ƒè¯•é…ç½®
            )
            
            # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå·¥ä½œæµ
            final_state = None
            execution_done = threading.Event()
            execution_error = [None]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨çº¿ç¨‹é—´å…±äº«
            
            def run_graph():
                """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå·¥ä½œæµ"""
                nonlocal final_state
                try:
                    for state_update in self._graph.stream(initial_state):
                        # state_update æ˜¯ {node_name: node_output} çš„å­—å…¸
                        for node_name, node_output in state_update.items():
                            logger.debug(f"ğŸ“Š èŠ‚ç‚¹ {node_name} å®Œæˆï¼Œè¾“å‡ºçŠ¶æ€æ›´æ–° (request_id={request_id})")
                            
                            # è¾“å‡ºèŠ‚ç‚¹å®Œæˆåçš„æ ¼å¼åŒ–å†…å®¹
                            if "stream_output" in node_output:
                                stream_output_list = node_output["stream_output"]
                                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°†æ ¼å¼åŒ–å†…å®¹æ¨é€åˆ°é˜Ÿåˆ—
                                if isinstance(stream_output_list, list):
                                    for chunk in stream_output_list:
                                        if chunk and chunk.strip():
                                            _push_to_request_queue(request_id, chunk)
                                elif stream_output_list:
                                    _push_to_request_queue(request_id, stream_output_list)
                            
                            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
                            final_state = node_output
                except Exception as e:
                    execution_error[0] = e
                    logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå‡ºé”™ (request_id={request_id}): {e}", exc_info=True)
                finally:
                    execution_done.set()
                    # å‘é€ç»“æŸæ ‡è®°åˆ°è¯¥è¯·æ±‚çš„é˜Ÿåˆ—
                    _push_to_request_queue(request_id, None)
            
            # å¯åŠ¨åå°çº¿ç¨‹æ‰§è¡Œå·¥ä½œæµ
            graph_thread = threading.Thread(target=run_graph, daemon=True)
            graph_thread.start()
            
            # å®æ—¶ä»è¯¥è¯·æ±‚çš„é˜Ÿåˆ—ä¸­è¯»å–å¹¶ yield token
            while True:
                try:
                    # ä»è¯¥è¯·æ±‚çš„é˜Ÿåˆ—ä¸­è·å– tokenï¼ˆè¶…æ—¶0.1ç§’ï¼Œé¿å…é˜»å¡å¤ªä¹…ï¼‰
                    chunk = request_queue.get(timeout=0.1)
                    
                    # None è¡¨ç¤ºç»“æŸ
                    if chunk is None:
                        break
                    
                    # å®æ—¶ yield token
                    yield chunk
                    
                except queue.Empty:
                    # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å·²å®Œæˆ
                    if execution_done.is_set():
                        # æ¸…ç©ºé˜Ÿåˆ—ä¸­å‰©ä½™çš„å†…å®¹
                        while True:
                            try:
                                chunk = request_queue.get_nowait()
                                if chunk is None:
                                    break
                                yield chunk
                            except queue.Empty:
                                break
                        break
                    # ç»§ç»­ç­‰å¾…
                    continue
            
            # ç­‰å¾…å·¥ä½œæµçº¿ç¨‹å®Œæˆ
            # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤360ç§’
            timeout_seconds = analysis_timeout if analysis_timeout is not None else 360
            graph_thread.join(timeout=timeout_seconds)
            
            # å¦‚æœçº¿ç¨‹ä»åœ¨è¿è¡Œï¼Œè¯´æ˜è¶…æ—¶äº†
            if graph_thread.is_alive():
                logger.warning(f"âš ï¸ åˆ†æè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰ï¼Œå¼ºåˆ¶ç»“æŸ (request_id={request_id})")
                yield f"\n\nâš ï¸ **åˆ†æè¶…æ—¶**\n\nåˆ†æè¿‡ç¨‹è¶…è¿‡ {timeout_seconds} ç§’ï¼Œå·²è‡ªåŠ¨ç»ˆæ­¢ã€‚\n\n"
                # æ³¨æ„ï¼šdaemon çº¿ç¨‹ä¼šåœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if execution_error[0]:
                raise execution_error[0]
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            if final_state:
                return AnalysisResult(
                    success=final_state.get("phase") == AnalysisPhase.COMPLETED.value,
                    report=final_state.get("report", ""),
                    code_history=final_state.get("code_history", []),
                    execution_outputs=[
                        e.output for e in final_state.get("execution_history", [])
                    ] if final_state.get("execution_history") else [],
                    generated_files=final_state.get("generated_files", []),
                    error_message=final_state.get("error_message"),
                    total_rounds=final_state.get("round_count", 0),
                )
            else:
                return AnalysisResult(
                    success=False,
                    error_message="å·¥ä½œæµæ‰§è¡Œå¤±è´¥",
                )
        finally:
            # æ¸…ç†è¯¥è¯·æ±‚çš„é˜Ÿåˆ—ï¼ˆä¸å½±å“å…¶ä»–è¯·æ±‚ï¼‰
            _remove_request_queue(request_id)
            logger.info(f"ğŸ åˆ†æè¯·æ±‚å®Œæˆ: {request_id}")

