"""
LangGraph Data Analysis Workflow

åŸºäº LangGraph 1.0.0+ å®ç°çš„æ•°æ®åˆ†æå·¥ä½œæµå›¾
æ”¯æŒï¼šä»£ç ç”Ÿæˆ â†’ æ‰§è¡Œ â†’ é”™è¯¯ä¿®å¤ â†’ æŠ¥å‘Šç”Ÿæˆ
"""

import re
import os
import shutil
import logging
import threading
import queue
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Generator, Literal

from langgraph.graph import StateGraph, START, END

from .state import AnalysisState, AnalysisPhase, CodeExecution, create_initial_state, AnalysisResult
from .prompts import PromptTemplates

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ============================================================================
# è¯·æ±‚çº§åˆ«çš„é˜Ÿåˆ—ç®¡ç†ï¼ˆè§£å†³å¤šçº¿ç¨‹å¹¶å‘é—®é¢˜ï¼‰
# ============================================================================
# ä½¿ç”¨å­—å…¸å­˜å‚¨æ¯ä¸ªè¯·æ±‚çš„ç‹¬ç«‹é˜Ÿåˆ—ï¼Œé¿å…å…¨å±€é˜Ÿåˆ—è¢«å¤šä¸ªè¯·æ±‚å…±äº«å¯¼è‡´çš„ç«æ€æ¡ä»¶
_request_queues: Dict[str, queue.Queue] = {}
_queues_lock = threading.Lock()
# é˜Ÿåˆ—çŠ¶æ€æ ‡è®°ï¼šè®°å½•æ¶ˆè´¹æ–¹æ˜¯å¦æ–­å¼€è¿æ¥
_queue_disconnected: Dict[str, bool] = {}


def _create_request_queue(request_id: str) -> queue.Queue:
    """ä¸ºè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„é˜Ÿåˆ—"""
    with _queues_lock:
        q = queue.Queue(maxsize=1000)
        _request_queues[request_id] = q
        _queue_disconnected[request_id] = False  # åˆå§‹åŒ–ä¸ºæœªæ–­å¼€
        logger.debug(f"ğŸ”§ åˆ›å»ºè¯·æ±‚é˜Ÿåˆ—: {request_id}")
        return q


def _get_request_queue(request_id: str) -> Optional[queue.Queue]:
    """è·å–è¯·æ±‚çš„é˜Ÿåˆ—"""
    with _queues_lock:
        return _request_queues.get(request_id)


def _remove_request_queue(request_id: str):
    """ç§»é™¤è¯·æ±‚çš„é˜Ÿåˆ—"""
    with _queues_lock:
        if request_id in _request_queues:
            # æ¸…ç©ºé˜Ÿåˆ—
            q = _request_queues[request_id]
            while not q.empty():
                try:
                    q.get_nowait()
                except queue.Empty:
                    break
            del _request_queues[request_id]
        # ç§»é™¤æ–­å¼€æ ‡è®°
        if request_id in _queue_disconnected:
            del _queue_disconnected[request_id]
        logger.debug(f"ğŸ§¹ ç§»é™¤è¯·æ±‚é˜Ÿåˆ—: {request_id}")


def _mark_queue_disconnected(request_id: str):
    """æ ‡è®°é˜Ÿåˆ—çš„æ¶ˆè´¹æ–¹å·²æ–­å¼€è¿æ¥"""
    with _queues_lock:
        _queue_disconnected[request_id] = True
        logger.warning(f"âš ï¸ æ ‡è®°é˜Ÿåˆ— {request_id} ä¸ºå·²æ–­å¼€ï¼Œå°†åœæ­¢æ¨é€æ•°æ®")


def _is_queue_disconnected(request_id: str) -> bool:
    """æ£€æŸ¥é˜Ÿåˆ—çš„æ¶ˆè´¹æ–¹æ˜¯å¦å·²æ–­å¼€è¿æ¥"""
    with _queues_lock:
        return _queue_disconnected.get(request_id, False)


def _push_to_request_queue(request_id: str, chunk: Optional[str]):
    """æ¨é€åˆ°æŒ‡å®šè¯·æ±‚çš„é˜Ÿåˆ—ï¼ˆchunk ä¸º None è¡¨ç¤ºç»“æŸæ ‡è®°ï¼‰
    
    å¦‚æœæ¶ˆè´¹æ–¹å·²æ–­å¼€æˆ–é˜Ÿåˆ—æ¥è¿‘æ»¡ï¼Œå°†åœæ­¢æ¨é€ä»¥é¿å…èµ„æºæµªè´¹
    """
    # æ£€æŸ¥æ¶ˆè´¹æ–¹æ˜¯å¦å·²æ–­å¼€
    if _is_queue_disconnected(request_id):
        # ç»“æŸæ ‡è®°ä»ç„¶æ¨é€ï¼Œä»¥ä¾¿åå°çº¿ç¨‹çŸ¥é“å¯ä»¥ç»“æŸ
        if chunk is None:
            # ç»“æŸæ ‡è®°ä»ç„¶å°è¯•æ¨é€ï¼ˆä½¿ç”¨éé˜»å¡æ–¹å¼ï¼‰
            q = _get_request_queue(request_id)
            if q is not None:
                try:
                    q.put_nowait(chunk)
                except queue.Full:
                    pass
        # éç»“æŸæ ‡è®°ç›´æ¥è¿”å›ï¼Œä¸æ¨é€
        return
    
    q = _get_request_queue(request_id)
    if q is not None:
        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°ï¼Œå¦‚æœæ¥è¿‘æ»¡ï¼ˆ>90%ï¼‰ï¼Œåœæ­¢æ¨é€éå…³é”®æ•°æ®
        queue_size = q.qsize()
        queue_maxsize = q.maxsize
        if queue_size > queue_maxsize * 0.9:
            # é˜Ÿåˆ—æ¥è¿‘æ»¡ï¼Œåªæ¨é€ç»“æŸæ ‡è®°ï¼Œå…¶ä»–æ•°æ®è·³è¿‡
            if chunk is None:
                # ç»“æŸæ ‡è®°ä»ç„¶å°è¯•æ¨é€
                try:
                    q.put_nowait(chunk)
                except queue.Full:
                    pass
            else:
                # éç»“æŸæ ‡è®°è·³è¿‡ï¼Œé¿å…é˜Ÿåˆ—æ»¡
                logger.warning(f"âš ï¸ è¯·æ±‚ {request_id} çš„é˜Ÿåˆ—æ¥è¿‘æ»¡ ({queue_size}/{queue_maxsize})ï¼Œè·³è¿‡ chunk")
            return
        
        try:
            q.put(chunk, timeout=0.1)
        except queue.Full:
            # é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡ï¼ˆé¿å…é˜»å¡ï¼‰
            if chunk is not None:
                logger.warning(f"âš ï¸ è¯·æ±‚ {request_id} çš„é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡ chunk")
            pass


# ============================================================================
# LLM å®¢æˆ·ç«¯è¾…åŠ©å‡½æ•°
# ============================================================================

def extract_api_base(api_url: str) -> str:
    """ä»å®Œæ•´çš„API URLä¸­æå–base URL"""
    if api_url.endswith("/chat/completions"):
        return api_url.rsplit("/chat/completions", 1)[0]
    elif "/v1" in api_url:
        return api_url.rsplit("/v1", 1)[0] + "/v1"
    else:
        return api_url


def create_llm_client(api_url: str, api_key: Optional[str] = None):
    """åˆ›å»º OpenAI å…¼å®¹çš„ LLM å®¢æˆ·ç«¯"""
    import openai
    
    api_base = extract_api_base(api_url)
    return openai.OpenAI(
        base_url=api_base,
        api_key=api_key or "dummy",
        timeout=120.0,
    )


def call_llm(
    client,
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.4,
    stream: bool = False,
    stream_callback: Optional[callable] = None,
    push_to_queue: bool = True,
    request_id: Optional[str] = None,  # æ–°å¢ï¼šè¯·æ±‚IDï¼Œç”¨äºå®šä½ç‹¬ç«‹é˜Ÿåˆ—
) -> str:
    """
    è°ƒç”¨ LLM å¹¶è¿”å›å“åº”å†…å®¹
    
    Args:
        client: LLM å®¢æˆ·ç«¯
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        temperature: ç”Ÿæˆæ¸©åº¦
        stream: æ˜¯å¦æµå¼è¾“å‡º
        stream_callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æ¯ä¸ª token (chunk: str) -> None
        push_to_queue: æ˜¯å¦æ¨é€åˆ°æµå¼è¾“å‡ºé˜Ÿåˆ—ï¼ˆé»˜è®¤Trueï¼‰
        request_id: è¯·æ±‚IDï¼Œç”¨äºå®šä½è¯¥è¯·æ±‚çš„ç‹¬ç«‹é˜Ÿåˆ—ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰
    
    Returns:
        å®Œæ•´çš„å“åº”å†…å®¹
    """
    if stream:
        # æµå¼è°ƒç”¨ï¼Œå®æ—¶å›è°ƒ
        # æµå¼è°ƒç”¨æ—¶å¯ç”¨ thinking åŠŸèƒ½ï¼Œä½¿ç”¨æµå¼è°ƒç”¨é¿å…é˜»å¡
        # ä¼˜å…ˆå°è¯• extra_body æ–¹å¼ï¼ˆå…¼å®¹æ›´å¤š APIï¼‰
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                stream=True,  # å¿…é¡»ä½¿ç”¨æµå¼è°ƒç”¨
                extra_body={"enable_thinking": True},  # æµå¼è°ƒç”¨æ—¶å¯ç”¨ thinking
            )
        except Exception:
            # å¦‚æœ extra_body æ–¹å¼å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¼ é€’å‚æ•°
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    stream=True,  # å¿…é¡»ä½¿ç”¨æµå¼è°ƒç”¨
                    enable_thinking=True,  # å°è¯•ç›´æ¥ä¼ é€’å‚æ•°
                )
            except Exception:
                # å¦‚æœå¯ç”¨ thinking å¤±è´¥ï¼Œä»ç„¶ä½¿ç”¨æµå¼è°ƒç”¨ï¼ˆä¸å¯ç”¨ thinkingï¼‰
                # è¿™æ ·å¯ä»¥é¿å…é˜»å¡ï¼Œä¿è¯ç³»ç»Ÿæ­£å¸¸è¿è¡Œ
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    stream=True,  # å¿…é¡»ä¿æŒæµå¼è°ƒç”¨
                )
        
        full_content = ""
        try:
            for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    delta = chunk.choices[0].delta.content
                    full_content += delta
                    
                    # å®æ—¶å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if stream_callback:
                        stream_callback(delta)
                    
                    # æ¨é€åˆ°è¯·æ±‚ç‹¬ç«‹çš„é˜Ÿåˆ—ï¼ˆå¦‚æœå¯ç”¨ä¸”æä¾›äº† request_idï¼‰
                    if push_to_queue and request_id:
                        _push_to_request_queue(request_id, delta)
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
            error_type_name = type(e).__name__
            error_str = str(e).lower()
            is_timeout_error = (
                'timeout' in error_type_name.lower() or 
                'timeout' in error_str or 
                'timed out' in error_str or
                ('httpcore' in str(type(e).__module__) and 'timeout' in error_type_name.lower()) or
                ('httpx' in str(type(e).__module__) and 'timeout' in error_type_name.lower())
            )
            
            if is_timeout_error:
                # è¶…æ—¶é”™è¯¯ï¼šåªè®°å½•è­¦å‘Šï¼Œä¸è¾“å‡ºå®Œæ•´å †æ ˆ
                logger.warning(f"â° LLMè°ƒç”¨è¶…æ—¶ (request_id={request_id}): {error_type_name} - {str(e)}")
            else:
                # å…¶ä»–é”™è¯¯ï¼šæ­£å¸¸è®°å½•
                logger.error(f"âŒ LLMè°ƒç”¨å‡ºé”™ (request_id={request_id}): {e}", exc_info=True)
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise
        
        return full_content
    else:
        # éæµå¼è°ƒç”¨å·²è¢«ç¦ç”¨ï¼Œå› ä¸ºå¿…é¡»å¯ç”¨ thinking åŠŸèƒ½
        # thinking åŠŸèƒ½åªèƒ½åœ¨æµå¼è°ƒç”¨æ—¶å¯ç”¨ï¼Œéæµå¼è°ƒç”¨ä¸æ”¯æŒ
        # å¼ºåˆ¶ä½¿ç”¨æµå¼è°ƒç”¨ä»¥ç¡®ä¿ thinking åŠŸèƒ½å¯ç”¨
        raise ValueError(
            "éæµå¼è°ƒç”¨å·²è¢«ç¦ç”¨ã€‚å¿…é¡»ä½¿ç”¨æµå¼è°ƒç”¨ï¼ˆstream=Trueï¼‰ä»¥å¯ç”¨ thinking åŠŸèƒ½ã€‚"
            "è¯·ç¡®ä¿æ‰€æœ‰ call_llm è°ƒç”¨éƒ½ä½¿ç”¨ stream=True å‚æ•°ã€‚"
        )


def call_llm_stream(
    client,
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.4,
) -> Generator[str, None, str]:
    """æµå¼è°ƒç”¨ LLMï¼Œyield æ¯ä¸ª tokenï¼Œæœ€åè¿”å›å®Œæ•´å†…å®¹"""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        stream=True,
    )
    
    full_content = ""
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            delta = chunk.choices[0].delta.content
            full_content += delta
            yield delta
    
    return full_content


# ============================================================================
# ä»£ç æå–è¾…åŠ©å‡½æ•°
# ============================================================================

def extract_python_code(text: str) -> Optional[str]:
    """ä» LLM å“åº”ä¸­æå– Python ä»£ç å—"""
    # åŒ¹é… ```python ... ``` æ ¼å¼
    pattern = r"```python\s*(.*?)```"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # åŒ¹é… ``` ... ``` æ ¼å¼ï¼ˆæ— è¯­è¨€æ ‡è®°ï¼‰
    pattern2 = r"```\s*(.*?)```"
    match2 = re.search(pattern2, text, re.DOTALL)
    if match2:
        code = match2.group(1).strip()
        # ç®€å•åˆ¤æ–­æ˜¯å¦åƒ Python ä»£ç 
        if "import " in code or "print(" in code or "def " in code:
            return code
    
    return None


def has_python_code(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å« Python ä»£ç å—"""
    return extract_python_code(text) is not None


def is_execution_error(output: str) -> bool:
    """æ£€æŸ¥æ‰§è¡Œè¾“å‡ºæ˜¯å¦åŒ…å«é”™è¯¯"""
    error_indicators = [
        "[Error]",
        "[Timeout]",
        "Traceback (most recent call last)",
        "Error:",
        "Exception:",
        "SyntaxError:",
        "NameError:",
        "TypeError:",
        "ValueError:",
        "KeyError:",
        "IndexError:",
        "FileNotFoundError:",
        "ModuleNotFoundError:",
    ]
    return any(indicator in output for indicator in error_indicators)


# ============================================================================
# å·¥ä½œæµèŠ‚ç‚¹å‡½æ•°
# ============================================================================

def plan_strategy_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ç­–ç•¥åˆ¶å®šèŠ‚ç‚¹
    
    èŒè´£ï¼šåˆ¶å®šæ•°æ®åˆ†æç­–ç•¥ï¼ŒåŒ…æ‹¬åˆ†ææ–¹æ³•é€‰æ‹©ã€ä»»åŠ¡åˆ†è§£ã€ä¼˜å…ˆçº§æ’åº
    """
    logger.info("ğŸ¯ [Node] ç­–ç•¥åˆ¶å®šèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    request_id = state.get("request_id", "")
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # æ„å»ºç­–ç•¥åˆ¶å®š prompt
    messages = PromptTemplates.format_strategy_planning_prompt(
        csv_path=state["csv_path"],
        row_count=state["row_count"],
        column_names=state["column_names"],
        column_metadata=state["column_metadata"],
        data_preview=state["data_preview"],
        user_prompt=state["user_prompt"],
    )
    
    # æµå¼è°ƒç”¨ LLMï¼ˆä¸è¾“å‡º JSON åˆ°ç”¨æˆ·ï¼‰
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        push_to_queue=False,
        request_id=request_id,
    )
    
    logger.info("=" * 80)
    logger.info("ğŸ¯ [ç­–ç•¥åˆ¶å®š] LLM å®Œæ•´å“åº”:")
    logger.info(response)
    logger.info("=" * 80)
    
    # è§£æ JSON å“åº”
    import json
    try:
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        json_str = json_match.group(1) if json_match else response
        strategy_result = json.loads(json_str.strip())
    except (json.JSONDecodeError, AttributeError) as e:
        logger.warning(f"âš ï¸ [Node] æ— æ³•è§£æç­–ç•¥åˆ¶å®šç»“æœ: {e}")
        strategy_result = {
            "is_relevant": True,
            "needs_clarification": False,
            "type": "overview",
            "refined_query": state["user_prompt"],
            "tasks": ["æ•°æ®æ¦‚è§ˆåˆ†æ"],
            "first_task": state["user_prompt"],
        }
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ¾„æ¸…
    is_relevant = strategy_result.get("is_relevant", True)
    needs_clarification = strategy_result.get("needs_clarification", False)
    
    if not is_relevant or needs_clarification:
        clarification_msg = strategy_result.get(
            "clarification_message",
            "æ‚¨çš„åˆ†æéœ€æ±‚ä¸å¤Ÿæ˜ç¡®ï¼Œè¯·æä¾›æ›´å…·ä½“çš„è¦æ±‚ã€‚"
        )
        _push_to_request_queue(request_id, f"\n\nâ“ **éœ€è¦æ¾„æ¸…**\n\n{clarification_msg}\n\n")
        return {
            "phase": AnalysisPhase.USER_CLARIFICATION_NEEDED.value,
            "analysis_strategy": {
                "type": "",
                "refined_query": state["user_prompt"],
                "tasks": [],
                "current_task": "",
                "completed_tasks": [],
                "needs_clarification": True,
                "clarification_message": clarification_msg,
            },
            "stream_output": [],
        }
    
    # æ„å»ºç»Ÿä¸€ç­–ç•¥å¯¹è±¡
    analysis_type = strategy_result.get("type", "overview")
    refined_query = strategy_result.get("refined_query", state["user_prompt"])
    tasks = strategy_result.get("tasks", [refined_query])
    first_task = strategy_result.get("first_task", tasks[0] if tasks else refined_query)
    
    strategy = {
        "type": analysis_type,
        "refined_query": refined_query,
        "tasks": tasks,
        "current_task": first_task,
        "completed_tasks": [],
        "needs_clarification": False,
        "clarification_message": None,
    }
    
    logger.info(f"âœ… [Node] ç­–ç•¥åˆ¶å®šå®Œæˆ")
    logger.info(f"   - åˆ†æç±»å‹: {analysis_type}")
    logger.info(f"   - åˆ†æä»»åŠ¡: {tasks}")
    logger.info(f"   - é¦–ä¸ªä»»åŠ¡: {first_task}")
    
    # è¾“å‡ºåˆ†æç­–ç•¥
    _push_to_request_queue(request_id, f"**åˆ†æç±»å‹ï¼š** {analysis_type}\n\n")
    if tasks:
        _push_to_request_queue(request_id, "**åˆ†æç­–ç•¥ï¼š**\n")
        for i, task in enumerate(tasks, 1):
            _push_to_request_queue(request_id, f"{i}. {task}\n")
        _push_to_request_queue(request_id, "\n")
    
    return {
        "phase": AnalysisPhase.CODE_GENERATION.value,
        "analysis_strategy": strategy,
        "messages": messages + [{"role": "assistant", "content": response}],
        "stream_output": [],
    }


def generate_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç ç”ŸæˆèŠ‚ç‚¹
    
    èŒè´£ï¼šæ ¹æ®å½“å‰åˆ†æä»»åŠ¡ç”Ÿæˆ Python ä»£ç 
    æ”¯æŒå¤šè½®åˆ†æï¼Œåç»­è½®ä¼šä¼ å…¥ä¹‹å‰çš„åˆ†æç»“æœ
    """
    logger.info("ğŸ“ [Node] ä»£ç ç”ŸæˆèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    request_id = state.get("request_id", "")
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # ç¡®å®šå½“å‰ä»»åŠ¡å’Œæ˜¯å¦æ˜¯é¦–è½®
    round_count = state.get("round_count", 0)
    is_first_round = round_count == 0
    
    # ä»ç­–ç•¥å¯¹è±¡è·å–å½“å‰ä»»åŠ¡
    strategy = state.get("analysis_strategy", {})
    current_task = strategy.get("current_task") or strategy.get("refined_query") or state["user_prompt"]
    
    # è·å–ä¹‹å‰çš„åˆ†æç»“æœï¼ˆç”¨äºåç»­è½®ï¼‰
    all_outputs = state.get("all_execution_outputs", [])
    previous_results = None
    if not is_first_round and all_outputs:
        previous_results = "\n\n---\n\n".join([
            f"ã€ç¬¬ {i+1} è½®åˆ†æã€‘\n{output}"
            for i, output in enumerate(all_outputs)
        ])
    
    logger.info(f"   - è½®æ¬¡: {round_count + 1}")
    logger.info(f"   - é¦–è½®: {is_first_round}")
    logger.info(f"   - å½“å‰ä»»åŠ¡: {current_task[:100]}...")
    
    # æ„å»º promptï¼ˆåŒºåˆ†é¦–è½®å’Œåç»­è½®ï¼‰
    messages = PromptTemplates.format_code_generation_prompt(
        csv_path=state["csv_path"],
        row_count=state["row_count"],
        column_names=state["column_names"],
        column_metadata=state["column_metadata"],
        data_preview=state["data_preview"],
        user_prompt=current_task,
        previous_results=previous_results,
        is_first_round=is_first_round,
    )
    
    # è¾“å‡ºæ ‡é¢˜
    if is_first_round:
        _push_to_request_queue(request_id, "\nğŸ“ **æ­£åœ¨ç”Ÿæˆåˆ†æä»£ç ...**\n\n")
    else:
        _push_to_request_queue(request_id, f"\nğŸ“ **æ­£åœ¨ç”Ÿæˆç¬¬ {round_count + 1} è½®åˆ†æä»£ç ...**\n\n")
    
    # æµå¼è°ƒç”¨ LLM
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        request_id=request_id,
    )
    
    logger.info("=" * 80)
    logger.info(f"ğŸ“ [ä»£ç ç”Ÿæˆ] LLM å®Œæ•´å“åº” (è½®æ¬¡ {round_count + 1}):")
    logger.info(response)
    logger.info("=" * 80)
    
    # æå–ä»£ç 
    code = extract_python_code(response)
    
    if code:
        logger.info(f"âœ… [Node] æˆåŠŸç”Ÿæˆä»£ç ï¼Œé•¿åº¦: {len(code)} å­—ç¬¦")
        return {
            "phase": AnalysisPhase.CODE_EXECUTION.value,
            "current_code": code,
            "code_history": [code],
            "messages": messages + [{"role": "assistant", "content": response}],
            "stream_output": [],
        }
    else:
        logger.warning("âš ï¸ [Node] æœªèƒ½ä» LLM å“åº”ä¸­æå–ä»£ç ")
        _push_to_request_queue(request_id, f"\n\nâš ï¸ æœªç”Ÿæˆä»£ç ï¼Œç›´æ¥è¿”å›åˆ†æç»“æœ\n\n")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "current_output": response,
            "messages": messages + [{"role": "assistant", "content": response}],
            "stream_output": [],
        }


def execute_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç æ‰§è¡ŒèŠ‚ç‚¹
    
    åœ¨æœ¬åœ°å®‰å…¨ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆçš„ Python ä»£ç 
    """
    logger.info("â–¶ï¸ [Node] ä»£ç æ‰§è¡ŒèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    # å¯¼å…¥æ‰§è¡Œå‡½æ•°
    from ..utils import execute_code_safe
    
    code = state["current_code"]
    workspace_dir = state["workspace_dir"]
    
    # æ·»åŠ  matplotlib ä¸­æ–‡æ”¯æŒ
    chinese_matplot_setup = '''
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import warnings

chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC']
available_fonts = [f.name for f in fm.fontManager.ttflist]
chinese_font = next((f for f in chinese_fonts if f in available_fonts), None)

if chinese_font:
    plt.rcParams['font.sans-serif'] = [chinese_font] + plt.rcParams['font.sans-serif']
else:
    warnings.filterwarnings('ignore', category=UserWarning, message='.*Glyph.*missing.*')
plt.rcParams['axes.unicode_minus'] = False
'''
    
    full_code = chinese_matplot_setup + "\n" + code
    
    # åœ¨æ‰§è¡Œä»£ç å‰ï¼Œè®°å½•å·²æœ‰çš„CSVæ–‡ä»¶ï¼ˆç”¨äºæ£€æµ‹æ–°ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
    workspace_path = Path(workspace_dir)
    existing_csv_files = set()
    if workspace_path.exists():
        for csv_file in workspace_path.rglob("*.csv"):
            existing_csv_files.add(csv_file.resolve())
    
    # æ‰§è¡Œä»£ç 
    logger.info(f"â³ æ‰§è¡Œä»£ç ï¼Œå·¥ä½œç›®å½•: {workspace_dir}")
    output = execute_code_safe(full_code, workspace_dir)
    logger.info(f"ğŸ“Š ä»£ç æ‰§è¡Œå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
    
    # æ£€æŸ¥æ‰§è¡Œç»“æœ
    success = not is_execution_error(output)
    
    # åˆ›å»ºæ‰§è¡Œè®°å½•
    execution = CodeExecution(
        code=code,
        output=output,
        success=success,
        error_message=output if not success else None,
        attempt=state.get("retry_count", 0) + 1,
    )
    
    if success:
        logger.info("âœ… [Node] ä»£ç æ‰§è¡ŒæˆåŠŸ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç”Ÿæˆçš„CSVæ–‡ä»¶ï¼Œå¹¶å¤åˆ¶åˆ°/tmpï¼ˆç‰¹åˆ«æ˜¯ä¿®å¤åçš„ä»£ç æ‰§è¡Œï¼‰
        retry_count = state.get("retry_count", 0)
        if retry_count > 0:  # å¦‚æœæ˜¯ä¿®å¤åçš„ä»£ç æ‰§è¡Œ
            try:
                new_csv_files = []
                if workspace_path.exists():
                    for csv_file in workspace_path.rglob("*.csv"):
                        csv_resolved = csv_file.resolve()
                        if csv_resolved not in existing_csv_files:
                            new_csv_files.append(csv_resolved)
                
                if new_csv_files:
                    tmp_dir = Path("/tmp")
                    tmp_dir.mkdir(exist_ok=True)
                    logger.info(f"ğŸ“ æ£€æµ‹åˆ° {len(new_csv_files)} ä¸ªæ–°ç”Ÿæˆçš„CSVæ–‡ä»¶ï¼Œå¤åˆ¶åˆ° /tmp ç›®å½•...")
                    
                    for csv_file in new_csv_files:
                        try:
                            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’ŒåŸå§‹æ–‡ä»¶åï¼‰
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            base_name = csv_file.stem
                            dest_name = f"{base_name}_{timestamp}.csv"
                            dest_path = tmp_dir / dest_name
                            
                            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ åºå·
                            counter = 1
                            while dest_path.exists():
                                dest_name = f"{base_name}_{timestamp}_{counter}.csv"
                                dest_path = tmp_dir / dest_name
                                counter += 1
                            
                            shutil.copy2(str(csv_file), str(dest_path))
                            logger.info(f"   âœ… å·²å¤åˆ¶: {csv_file.name} â†’ /tmp/{dest_name}")
                        except Exception as e:
                            logger.warning(f"   âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {csv_file.name}: {e}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥å¹¶å¤åˆ¶CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¾“å‡ºæ‰§è¡Œç»“æœ
        debug_print = state.get("debug_print_execution_output", False)
        if debug_print:
            _push_to_request_queue(request_id, "\nâœ… **ä»£ç æ‰§è¡Œå®Œæ¯•**\n\n")
            _push_to_request_queue(request_id, "ğŸ“Š **æ‰§è¡Œç»“æœï¼š**\n\n")
            _push_to_request_queue(request_id, f"```\n{output}\n```\n\n")
            _push_to_request_queue(request_id, "æ­£åœ¨è¯„ä¼°åˆ†æå®Œæ•´æ€§...\n\n")
        else:
            # é»˜è®¤ä¸æ˜¾ç¤ºå…·ä½“æ‰§è¡Œç»“æœ
            _push_to_request_queue(request_id, "\nâœ… **ä»£ç æ‰§è¡Œå®Œæ¯•ï¼Œæ­£åœ¨è¯„ä¼°åˆ†æå®Œæ•´æ€§...**\n\n")
        
        # æˆåŠŸåè¿›å…¥åˆ†æå®Œæ•´æ€§è¯„ä¼°èŠ‚ç‚¹ï¼ˆæ–°æµç¨‹ï¼‰
        return {
            "phase": AnalysisPhase.EVALUATE_COMPLETENESS.value,
            "current_output": output,
            "execution_success": True,
            "execution_history": [execution],
            "round_count": state.get("round_count", 0) + 1,
            "stream_output": [],
        }
    else:
        logger.warning(f"âŒ [Node] ä»£ç æ‰§è¡Œå¤±è´¥: {output[:200]}...")
        return {
            "phase": AnalysisPhase.ERROR_FIXING.value,
            "current_output": output,
            "execution_success": False,
            "error_message": output,
            "execution_history": [execution],
            "stream_output": [f"\nâŒ **æ‰§è¡Œå‡ºé”™ï¼š**\n\n```\n{output}\n```\n\n"],
        }


def fix_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç ä¿®å¤èŠ‚ç‚¹
    
    å½“ä»£ç æ‰§è¡Œå¤±è´¥æ—¶ï¼Œè°ƒç”¨ LLM ä¿®å¤ä»£ç 
    """
    logger.info("ğŸ”§ [Node] ä»£ç ä¿®å¤èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    retry_count = state.get("retry_count", 0) + 1
    max_retries = 3
    
    if retry_count > max_retries:
        logger.error(f"âŒ [Node] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "retry_count": retry_count,
            "stream_output": [f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œè·³è¿‡ä»£ç æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š\n\n"],
        }
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # æ„å»ºä¿®å¤ prompt
    messages = PromptTemplates.format_code_fix_prompt(
        original_code=state["current_code"],
        error_message=state.get("error_message", "æœªçŸ¥é”™è¯¯"),
        csv_path=state["csv_path"],
        column_names=state["column_names"],
    )
    
    # æ”¶é›†æµå¼è¾“å‡ºçš„åˆ—è¡¨ï¼ˆç”¨äºåç»­æ ¼å¼åŒ–ï¼‰
    stream_chunks = []
    
    def stream_callback(chunk: str):
        """æµå¼è¾“å‡ºå›è°ƒï¼Œæ”¶é›† tokenï¼ˆåŒæ—¶ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰"""
        stream_chunks.append(chunk)
    
    # å…ˆè¾“å‡ºæ ‡é¢˜ï¼ˆå®æ—¶ä¼ é€’ï¼‰
    _push_to_request_queue(request_id, f"\nğŸ”§ **æ­£åœ¨ä¿®å¤ä»£ç ï¼ˆå°è¯• {retry_count}/{max_retries}ï¼‰...**\n\n")
    
    # æµå¼è°ƒç”¨ LLM ä¿®å¤ï¼ˆæ¯ä¸ª token ä¼šé€šè¿‡é˜Ÿåˆ—å®æ—¶ä¼ é€’ï¼‰
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        stream_callback=stream_callback,
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info(f"ğŸ”§ [ä»£ç ä¿®å¤] LLM å®Œæ•´å“åº” (å°è¯• {retry_count}/{max_retries}):")
    logger.info("=" * 80)
    logger.info(response)
    logger.info("=" * 80)
    
    # æå–ä¿®å¤åçš„ä»£ç 
    fixed_code = extract_python_code(response)
    
    if fixed_code:
        logger.info(f"âœ… [Node] æˆåŠŸè·å–ä¿®å¤ä»£ç ï¼Œé‡è¯•æ¬¡æ•°: {retry_count}")
        # æ³¨æ„ï¼šä»£ç å·²ç»åœ¨æµå¼è°ƒç”¨æ—¶å®æ—¶æ¨é€è¿‡äº†ï¼Œä¸éœ€è¦å†æ¬¡æ¨é€æ ¼å¼åŒ–ä»£ç 
        
        # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹ï¼ˆæ ‡é¢˜ã€æµå¼tokenï¼‰éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        stream_output = []
        
        return {
            "phase": AnalysisPhase.CODE_EXECUTION.value,
            "current_code": fixed_code,
            "code_history": [fixed_code],
            "retry_count": retry_count,
            "stream_output": stream_output,
        }
    else:
        logger.warning("âš ï¸ [Node] æœªèƒ½ä»ä¿®å¤å“åº”ä¸­æå–ä»£ç ")
        _push_to_request_queue(request_id, f"\n\nâš ï¸ æ— æ³•ä¿®å¤ä»£ç ï¼Œè·³è¿‡æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š\n\n")
        
        # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
        # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
        stream_output = []
        
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "retry_count": retry_count,
            "stream_output": stream_output,
        }


def evaluate_completeness_node(state: AnalysisState) -> Dict[str, Any]:
    """
    åˆ†æå®Œæ•´æ€§è¯„ä¼°èŠ‚ç‚¹ï¼ˆæ–°å¢ï¼‰
    
    åˆ¤æ–­å½“å‰åˆ†ææ˜¯å¦å·²ç»å……åˆ†å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Œæ˜¯å¦éœ€è¦è¿›è¡Œæ›´å¤šçš„æ·±å…¥åˆ†æã€‚
    
    åŠŸèƒ½ï¼š
    1. è¯„ä¼°å½“å‰åˆ†æç»“æœçš„è¦†ç›–åº¦å’Œæ·±åº¦
    2. åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ›´å¤šåˆ†æ
    3. å¦‚æœéœ€è¦ï¼Œç”Ÿæˆä¸‹ä¸€è½®åˆ†æçš„å…·ä½“æ–¹å‘
    """
    logger.info("ğŸ” [Node] åˆ†æå®Œæ•´æ€§è¯„ä¼°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # è·å–è¯·æ±‚IDï¼ˆç”¨äºå¤šçº¿ç¨‹éš”ç¦»ï¼‰
    request_id = state.get("request_id", "")
    
    # è·å–å½“å‰è½®æ¬¡å’Œæœ€å¤§è½®æ¬¡
    current_round = state.get("round_count", 1)
    max_rounds = state.get("max_analysis_rounds", 3)
    
    logger.info(f"   - å½“å‰è½®æ¬¡: {current_round}/{max_rounds}")
    
    # å¦‚æœå·²è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œç›´æ¥å»æŠ¥å‘Šç”Ÿæˆ
    if current_round >= max_rounds:
        logger.info(f"ğŸ“Š å·²è¾¾åˆ°æœ€å¤§åˆ†æè½®æ•° ({max_rounds})ï¼Œè¿›å…¥æŠ¥å‘Šç”Ÿæˆ")
        _push_to_request_queue(request_id, f"\nğŸ“Š å·²å®Œæˆ {current_round} è½®åˆ†æï¼ˆè¾¾åˆ°æœ€å¤§è½®æ•°ï¼‰ï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...\n\n")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "need_more_analysis": False,
            "stream_output": [],
        }
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # è·å–ä¹‹å‰çš„è¾“å‡º
    previous_outputs = state.get("all_execution_outputs", [])
    current_output = state.get("current_output", "")
    
    # ä»ç­–ç•¥å¯¹è±¡è·å–ä»»åŠ¡ä¿¡æ¯
    strategy = state.get("analysis_strategy", {})
    analysis_tasks = strategy.get("tasks", [])
    completed_tasks = strategy.get("completed_tasks", [])
    
    # æ„å»ºè¯„ä¼° prompt
    messages = PromptTemplates.format_evaluate_completeness_prompt(
        user_prompt=state["user_prompt"],
        analysis_tasks=analysis_tasks,
        current_output=current_output,
        previous_outputs=previous_outputs,
        completed_tasks=completed_tasks,
        current_round=current_round,
        max_rounds=max_rounds,
    )
    
    # æµå¼è°ƒç”¨ LLM è¯„ä¼°ï¼ˆä¸å®æ—¶è¾“å‡ºï¼Œé¿å…æ˜¾ç¤º JSONï¼‰
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=0.3,  # è¾ƒä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§çš„åˆ¤æ–­
        stream=True,
        push_to_queue=False,  # ä¸æ¨é€åˆ°é˜Ÿåˆ—ï¼Œé¿å…è¾“å‡ºJSON
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info(f"ğŸ” [åˆ†æå®Œæ•´æ€§è¯„ä¼°] LLM å®Œæ•´å“åº” (è½®æ¬¡ {current_round}/{max_rounds}):")
    logger.info("=" * 80)
    logger.info(response)
    logger.info("=" * 80)
    
    # è§£æ JSON å“åº”
    import json
    try:
        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç å—ï¼‰
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            json_str = response
        
        eval_result = json.loads(json_str.strip())
    except (json.JSONDecodeError, AttributeError) as e:
        logger.warning(f"âš ï¸ [Node] æ— æ³•è§£æè¯„ä¼°ç»“æœ: {e}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤ä¸éœ€è¦æ›´å¤šåˆ†æ
        eval_result = {
            "need_more_analysis": False,
            "reason": "è¯„ä¼°ç»“æœè§£æå¤±è´¥ï¼Œé»˜è®¤ç»“æŸåˆ†æ",
            "completed_aspects": [],
            "missing_aspects": [],
            "next_direction": "",
        }
    
    # è·å–è¯„ä¼°ç»“æœï¼ˆä½¿ç”¨æ–°å­—æ®µåï¼‰
    need_more = eval_result.get("need_more_analysis", False)
    reason = eval_result.get("reason", "")
    completed_tasks_new = eval_result.get("completed_tasks", [])
    next_task = eval_result.get("next_task", "")
    
    logger.info(f"   - éœ€è¦æ›´å¤šåˆ†æ: {need_more}")
    logger.info(f"   - ç†ç”±: {reason}")
    if need_more:
        logger.info(f"   - ä¸‹ä¸€æ­¥ä»»åŠ¡: {next_task}")
    
    if need_more and next_task:
        # éœ€è¦ç»§ç»­åˆ†æ
        logger.info(f"ğŸ”„ éœ€è¦æ›´å¤šåˆ†æï¼Œä¸‹ä¸€ä»»åŠ¡: {next_task}")
        _push_to_request_queue(request_id, f"\nğŸ”„ **ç»§ç»­åˆ†æï¼ˆç¬¬ {current_round + 1} è½®ï¼‰**\n\n")
        _push_to_request_queue(request_id, f"**ä»»åŠ¡ï¼š** {next_task}\n\n")
        
        # æ›´æ–°ç­–ç•¥å¯¹è±¡
        strategy = state.get("analysis_strategy", {}).copy()
        strategy["current_task"] = next_task
        strategy["completed_tasks"] = completed_tasks_new
        
        return {
            "phase": AnalysisPhase.CODE_GENERATION.value,
            "need_more_analysis": True,
            "analysis_strategy": strategy,
            "all_execution_outputs": [current_output],  # ç´¯ç§¯æ‰§è¡Œè¾“å‡º
            "stream_output": [],
        }
    else:
        # åˆ†æå·²å®Œæˆ
        logger.info("âœ… åˆ†æå·²å®Œæˆï¼Œè¿›å…¥æŠ¥å‘Šç”Ÿæˆ")
        _push_to_request_queue(request_id, f"\nâœ… **åˆ†æå®Œæˆ**ï¼ˆå…± {current_round} è½®ï¼‰ï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Š...\n\n")
        
        # æ›´æ–°ç­–ç•¥å¯¹è±¡
        strategy = state.get("analysis_strategy", {}).copy()
        strategy["completed_tasks"] = completed_tasks_new
        
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "need_more_analysis": False,
            "analysis_strategy": strategy,
            "all_execution_outputs": [current_output],  # ç´¯ç§¯æ‰§è¡Œè¾“å‡º
            "stream_output": [],
        }


def generate_report_node(state: AnalysisState) -> Dict[str, Any]:
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    
    èŒè´£ï¼šç»¼åˆæ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    """
    logger.info("ğŸ“„ [Node] æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    request_id = state.get("request_id", "")
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # ä»ç­–ç•¥å¯¹è±¡è·å–åˆ†æç±»å‹å’Œè½®æ¬¡
    strategy = state.get("analysis_strategy", {})
    analysis_type = strategy.get("type", "overview")
    round_count = state.get("round_count", 1)
    
    logger.info(f"   - åˆ†æç±»å‹: {analysis_type}")
    logger.info(f"   - å…±å®Œæˆ {round_count} è½®åˆ†æ")
    
    # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
    all_outputs = state.get("all_execution_outputs", [])
    current_output = state.get("current_output", "")
    
    # æ„å»ºç»“æœæ–‡æœ¬
    all_results_parts = []
    
    # æ·»åŠ ä¹‹å‰è½®æ¬¡çš„è¾“å‡º
    for i, out in enumerate(all_outputs, 1):
        if out and out.strip():
            all_results_parts.append(f"### ç¬¬ {i} è½®åˆ†æç»“æœ\n\n```\n{out}\n```")
    
    # æ·»åŠ å½“å‰è¾“å‡º
    if current_output and current_output not in all_outputs:
        round_num = len(all_outputs) + 1
        all_results_parts.append(f"### ç¬¬ {round_num} è½®åˆ†æç»“æœ\n\n```\n{current_output}\n```")
    
    all_results = "\n\n".join(all_results_parts) if all_results_parts else f"```\n{current_output}\n```"
    
    # å¦‚æœæ²¡æœ‰è¾“å‡ºï¼Œå°è¯•ä»æ‰§è¡Œå†å²è·å–
    if not all_results.strip() or all_results == "```\n\n```":
        if state.get("execution_history"):
            last_execution = state["execution_history"][-1]
            all_results = f"```\n{last_execution.output}\n```"
    
    # æ„å»ºæŠ¥å‘Š promptï¼ˆä½¿ç”¨æ–°æ ¼å¼ï¼‰
    messages = PromptTemplates.format_report_generation_prompt(
        user_prompt=state["user_prompt"],
        analysis_type=analysis_type,
        total_rounds=round_count,
        all_results=all_results,
        column_names=state.get("column_names", []),
        column_metadata=state.get("column_metadata", {}),
    )
    
    # è¾“å‡ºæ ‡é¢˜
    _push_to_request_queue(request_id, "\nğŸ“„ **æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...**\n\n")
    
    # æµå¼è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
    report = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
        stream=True,
        request_id=request_id,
    )
    
    # åœ¨æ§åˆ¶å°æ‰“å°LLMçš„å®Œæ•´å“åº”
    logger.info("=" * 80)
    logger.info("ğŸ“„ [æŠ¥å‘Šç”Ÿæˆ] LLM å®Œæ•´å“åº”:")
    logger.info("=" * 80)
    logger.info(report)
    logger.info("=" * 80)
    
    logger.info(f"âœ… [Node] æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼Œé•¿åº¦: {len(report)} å­—ç¬¦")
    
    # æ„å»ºæµå¼è¾“å‡ºï¼ˆç”¨äºçŠ¶æ€è®°å½•ï¼‰
    # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹ï¼ˆæ ‡é¢˜ã€æµå¼tokenï¼‰éƒ½å·²ç»åœ¨èŠ‚ç‚¹æ‰§è¡Œæ—¶å®æ—¶æ¨é€è¿‡äº†
    # stream_output ä¿ç•™ä¸ºç©ºï¼Œé¿å…é‡å¤æ¨é€
    stream_output = []
    
    return {
        "phase": AnalysisPhase.COMPLETED.value,
        "report": report,
        "stream_output": stream_output,
    }


# ============================================================================
# æ¡ä»¶è·¯ç”±å‡½æ•°
# ============================================================================

def route_after_execution(state: AnalysisState) -> Literal["fix_code", "evaluate_completeness"]:
    """
    æ‰§è¡Œåè·¯ç”±å†³ç­–
    
    æ ¹æ®æ‰§è¡Œç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - æ‰§è¡ŒæˆåŠŸ â†’ è¯„ä¼°åˆ†æå®Œæ•´æ€§ï¼ˆæ–°æµç¨‹ï¼‰
    - æ‰§è¡Œå¤±è´¥ â†’ ä¿®å¤ä»£ç 
    """
    if state.get("execution_success", False):
        return "evaluate_completeness"
    else:
        return "fix_code"


def route_after_fix(state: AnalysisState) -> Literal["execute_code", "generate_report"]:
    """
    ä¿®å¤åè·¯ç”±å†³ç­–
    
    æ ¹æ®ä¿®å¤ç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - æœ‰æ–°ä»£ç  â†’ é‡æ–°æ‰§è¡Œ
    - æ— æ³•ä¿®å¤ â†’ ç”ŸæˆæŠ¥å‘Š
    """
    if state.get("phase") == AnalysisPhase.CODE_EXECUTION.value:
        return "execute_code"
    else:
        return "generate_report"


def route_after_evaluation(state: AnalysisState) -> Literal["generate_code", "generate_report"]:
    """
    è¯„ä¼°åè·¯ç”±å†³ç­–ï¼ˆæ–°å¢ï¼‰
    
    æ ¹æ®è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - éœ€è¦æ›´å¤šåˆ†æ â†’ å›åˆ°ä»£ç ç”Ÿæˆï¼ˆå½¢æˆå¾ªç¯ï¼‰
    - åˆ†æå·²å®Œæˆ â†’ ç”ŸæˆæŠ¥å‘Š
    """
    if state.get("need_more_analysis", False):
        return "generate_code"
    else:
        return "generate_report"


# ============================================================================
# å·¥ä½œæµå›¾æ„å»º
# ============================================================================

def create_analysis_graph() -> StateGraph:
    """
    åˆ›å»ºæ•°æ®åˆ†æå·¥ä½œæµå›¾
    
    å·¥ä½œæµç»“æ„ï¼ˆæ”¯æŒå¤šè½®åˆ†æï¼‰ï¼š
    
    START â†’ plan_strategy â”€â”¬â”€(éœ€è¦æ¾„æ¸…)â”€â†’ END
                           â”‚
                           â””â”€(å¯ä»¥åˆ†æ)â”€â†’ generate_code â†’ execute_code â”€â”¬â”€(æˆåŠŸ)â”€â†’ evaluate_completeness â”€â”¬â”€(éœ€è¦æ›´å¤š)â”€â†’ generate_code (å¾ªç¯)
                                             â†‘                          â”‚                                 â”‚
                                             â”‚                          â”‚                                 â””â”€(å®Œæˆ)â”€â†’ generate_report â†’ END
                                             â”‚                          â”‚
                                             â”‚                          â””â”€(å¤±è´¥)â”€â†’ fix_code â”€â”¬â”€(æœ‰ä¿®å¤)â”€â†’ execute_code
                                             â”‚                                              â”‚
                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€(æ— æ³•ä¿®å¤)â”€â†’ generate_report
    
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AnalysisState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("plan_strategy", plan_strategy_node)
    workflow.add_node("generate_code", generate_code_node)
    workflow.add_node("execute_code", execute_code_node)
    workflow.add_node("fix_code", fix_code_node)
    workflow.add_node("evaluate_completeness", evaluate_completeness_node)  # åˆ†æå®Œæ•´æ€§è¯„ä¼°èŠ‚ç‚¹
    workflow.add_node("generate_report", generate_report_node)
    
    # æ·»åŠ è¾¹
    # START â†’ plan_strategy
    workflow.add_edge(START, "plan_strategy")
    
    # plan_strategy â†’ generate_code æˆ– ENDï¼ˆéœ€è¦æ¾„æ¸…ï¼‰
    def route_after_strategy(state: AnalysisState) -> Literal["generate_code", "end"]:
        """ç­–ç•¥åˆ¶å®šåçš„è·¯ç”±å†³ç­–"""
        phase = state.get("phase", "")
        if phase == AnalysisPhase.CODE_GENERATION.value:
            return "generate_code"
        else:
            # éœ€è¦æ¾„æ¸…æˆ–å…¶ä»–æƒ…å†µï¼Œç›´æ¥ç»“æŸ
            return "end"
    
    workflow.add_conditional_edges(
        "plan_strategy",
        route_after_strategy,
        {
            "generate_code": "generate_code",
            "end": END,
        }
    )
    
    # generate_code â†’ execute_code (å¦‚æœç”Ÿæˆäº†ä»£ç )
    workflow.add_conditional_edges(
        "generate_code",
        lambda s: "execute_code" if s.get("phase") == AnalysisPhase.CODE_EXECUTION.value else "generate_report",
        {
            "execute_code": "execute_code",
            "generate_report": "generate_report",
        }
    )
    
    # å¤„ç†éœ€è¦æ¾„æ¸…çš„æƒ…å†µï¼ˆç›´æ¥ç»“æŸï¼‰
    # æ³¨æ„ï¼šplan_strategy èŠ‚ç‚¹å¦‚æœè¿”å› USER_CLARIFICATION_NEEDEDï¼Œä¼šé€šè¿‡æ¡ä»¶è¾¹è·¯ç”±åˆ° END
    
    # execute_code â†’ fix_code æˆ– evaluate_completenessï¼ˆæ–°æµç¨‹ï¼‰
    workflow.add_conditional_edges(
        "execute_code",
        route_after_execution,
        {
            "fix_code": "fix_code",
            "evaluate_completeness": "evaluate_completeness",  # æˆåŠŸåå»è¯„ä¼°èŠ‚ç‚¹
        }
    )
    
    # fix_code â†’ execute_code æˆ– generate_report
    workflow.add_conditional_edges(
        "fix_code",
        route_after_fix,
        {
            "execute_code": "execute_code",
            "generate_report": "generate_report",
        }
    )
    
    # evaluate_completeness â†’ generate_code (éœ€è¦æ›´å¤šåˆ†æ) æˆ– generate_report (å®Œæˆ)
    workflow.add_conditional_edges(
        "evaluate_completeness",
        route_after_evaluation,
        {
            "generate_code": "generate_code",  # å¾ªç¯å›åˆ°ä»£ç ç”Ÿæˆ
            "generate_report": "generate_report",  # è¿›å…¥æŠ¥å‘Šç”Ÿæˆ
        }
    )
    
    # generate_report â†’ END
    workflow.add_edge("generate_report", END)
    
    return workflow


# ============================================================================
# é«˜çº§å°è£…ç±»
# ============================================================================

class DataAnalysisGraph:
    """
    æ•°æ®åˆ†æå›¾å°è£…ç±»
    
    æä¾›ç®€åŒ–çš„ API ç”¨äºæ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµå›¾"""
        self._workflow = create_analysis_graph()
        self._graph = self._workflow.compile()
    
    def analyze(
        self,
        workspace_dir: str,
        thread_id: str,
        csv_path: str,
        column_names: List[str],
        column_metadata: Dict[str, Any],
        row_count: int,
        data_preview: str,
        user_prompt: str,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        temperature: float = 0.4,
    ) -> AnalysisResult:
        """
        æ‰§è¡Œæ•°æ®åˆ†æï¼ˆéæµå¼ï¼‰
        
        Args:
            workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
            thread_id: ä¼šè¯ID
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            column_names: åˆ—ååˆ—è¡¨
            column_metadata: åˆ—å…ƒæ•°æ®
            row_count: æ•°æ®è¡Œæ•°
            data_preview: æ•°æ®é¢„è§ˆ
            user_prompt: ç”¨æˆ·åˆ†æéœ€æ±‚
            api_url: LLM API åœ°å€
            model: æ¨¡å‹åç§°
            api_key: LLM API å¯†é’¥
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            AnalysisResult åˆ†æç»“æœ
        """
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = create_initial_state(
            workspace_dir=workspace_dir,
            thread_id=thread_id,
            csv_path=csv_path,
            column_names=column_names,
            column_metadata=column_metadata,
            row_count=row_count,
            data_preview=data_preview,
            user_prompt=user_prompt,
            api_url=api_url,
            model=model,
            api_key=api_key,
            temperature=temperature,
        )
        
        # æ‰§è¡Œå·¥ä½œæµ
        final_state = self._graph.invoke(initial_state)
        
        # æ„å»ºç»“æœ
        return AnalysisResult(
            success=final_state.get("phase") == AnalysisPhase.COMPLETED.value,
            report=final_state.get("report", ""),
            code_history=final_state.get("code_history", []),
            execution_outputs=[
                e.output for e in final_state.get("execution_history", [])
            ],
            generated_files=final_state.get("generated_files", []),
            error_message=final_state.get("error_message"),
            total_rounds=final_state.get("round_count", 0),
        )
    
    def analyze_stream(
        self,
        workspace_dir: str,
        thread_id: str,
        csv_path: str,
        column_names: List[str],
        column_metadata: Dict[str, Any],
        row_count: int,
        data_preview: str,
        user_prompt: str,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        temperature: float = 0.4,
        analysis_timeout: Optional[int] = None,
        debug_print_execution_output: bool = False,
        max_analysis_rounds: int = 3,  # æ–°å¢ï¼šæœ€å¤§åˆ†æè½®æ•°
    ) -> Generator[str, None, AnalysisResult]:
        """
        æ‰§è¡Œæ•°æ®åˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰
        
        ä½¿ç”¨ LangGraph çš„ stream æ¨¡å¼ + çº¿ç¨‹é˜Ÿåˆ—å®ç°çœŸæ­£çš„å®æ—¶æµå¼è¾“å‡º
        åœ¨èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼ŒLLM çš„æ¯ä¸ª token éƒ½ä¼šå®æ—¶ä¼ é€’
        
        æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„é˜Ÿåˆ—ï¼Œç¡®ä¿å¤šçº¿ç¨‹å®‰å…¨ã€‚
        
        æ”¯æŒå¤šè½®åˆ†æï¼šç³»ç»Ÿä¼šè‡ªåŠ¨è¯„ä¼°åˆ†æå®Œæ•´æ€§ï¼Œå¦‚æœéœ€è¦æ›´å¤šåˆ†æï¼Œ
        ä¼šç»§ç»­ç”Ÿæˆä»£ç å¹¶æ‰§è¡Œï¼Œç›´åˆ°åˆ†æå®Œæˆæˆ–è¾¾åˆ°æœ€å¤§è½®æ•°ã€‚
        
        Args:
            max_analysis_rounds: æœ€å¤§åˆ†æè½®æ•°ï¼ˆé»˜è®¤3è½®ï¼‰ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        
        Yields:
            str: æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
            
        Returns:
            AnalysisResult åˆ†æç»“æœ
        """
        import uuid
        
        # ä¸ºæ¯ä¸ªè¯·æ±‚ç”Ÿæˆå”¯ä¸€çš„ request_idï¼ˆç”¨äºé˜Ÿåˆ—éš”ç¦»ï¼‰
        request_id = f"req-{uuid.uuid4().hex[:16]}"
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æè¯·æ±‚: {request_id}ï¼Œæœ€å¤§åˆ†æè½®æ•°: {max_analysis_rounds}")
        
        # ä¸ºè¯¥è¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„é˜Ÿåˆ—ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰
        request_queue = _create_request_queue(request_id)
        
        try:
            # åˆ›å»ºåˆå§‹çŠ¶æ€ï¼ˆåŒ…å« request_idï¼‰
            initial_state = create_initial_state(
                workspace_dir=workspace_dir,
                thread_id=thread_id,
                csv_path=csv_path,
                column_names=column_names,
                column_metadata=column_metadata,
                row_count=row_count,
                data_preview=data_preview,
                user_prompt=user_prompt,
                api_url=api_url,
                model=model,
                api_key=api_key,
                temperature=temperature,
                request_id=request_id,  # ä¼ é€’è¯·æ±‚ID
                debug_print_execution_output=debug_print_execution_output,  # ä¼ é€’è°ƒè¯•é…ç½®
                max_analysis_rounds=max_analysis_rounds,  # ä¼ é€’æœ€å¤§åˆ†æè½®æ•°
            )
            
            # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå·¥ä½œæµ
            final_state = None
            execution_done = threading.Event()
            execution_error = [None]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨çº¿ç¨‹é—´å…±äº«
            
            def run_graph():
                """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå·¥ä½œæµ"""
                nonlocal final_state
                try:
                    for state_update in self._graph.stream(initial_state):
                        # state_update æ˜¯ {node_name: node_output} çš„å­—å…¸
                        for node_name, node_output in state_update.items():
                            logger.debug(f"ğŸ“Š èŠ‚ç‚¹ {node_name} å®Œæˆï¼Œè¾“å‡ºçŠ¶æ€æ›´æ–° (request_id={request_id})")
                            
                            # è¾“å‡ºèŠ‚ç‚¹å®Œæˆåçš„æ ¼å¼åŒ–å†…å®¹
                            if "stream_output" in node_output:
                                stream_output_list = node_output["stream_output"]
                                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°†æ ¼å¼åŒ–å†…å®¹æ¨é€åˆ°é˜Ÿåˆ—
                                if isinstance(stream_output_list, list):
                                    for chunk in stream_output_list:
                                        if chunk and chunk.strip():
                                            _push_to_request_queue(request_id, chunk)
                                elif stream_output_list:
                                    _push_to_request_queue(request_id, stream_output_list)
                            
                            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
                            final_state = node_output
                except Exception as e:
                    execution_error[0] = e
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯ï¼ˆReadTimeout, ConnectTimeoutç­‰ï¼‰
                    is_timeout_error = False
                    error_type_name = type(e).__name__
                    error_str = str(e).lower()
                    
                    # æ£€æŸ¥é”™è¯¯ç±»å‹åç§°
                    if 'timeout' in error_type_name.lower():
                        is_timeout_error = True
                    # æ£€æŸ¥é”™è¯¯æ¶ˆæ¯
                    elif 'timeout' in error_str or 'timed out' in error_str:
                        is_timeout_error = True
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ httpcore æˆ– httpx çš„è¶…æ—¶é”™è¯¯
                    elif 'httpcore' in str(type(e).__module__) or 'httpx' in str(type(e).__module__):
                        if 'timeout' in error_type_name.lower() or 'timeout' in error_str:
                            is_timeout_error = True
                    
                    if is_timeout_error:
                        # è¶…æ—¶é”™è¯¯ï¼šåªè®°å½•ç®€å•ä¿¡æ¯ï¼Œä¸è¾“å‡ºå®Œæ•´å †æ ˆ
                        logger.warning(f"â° å·¥ä½œæµæ‰§è¡Œè¶…æ—¶ (request_id={request_id}): {error_type_name} - {str(e)}")
                    else:
                        # å…¶ä»–é”™è¯¯ï¼šæ­£å¸¸è®°å½•å®Œæ•´å †æ ˆ
                        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå‡ºé”™ (request_id={request_id}): {e}", exc_info=True)
                finally:
                    execution_done.set()
                    # å‘é€ç»“æŸæ ‡è®°åˆ°è¯¥è¯·æ±‚çš„é˜Ÿåˆ—
                    _push_to_request_queue(request_id, None)
            
            # å¯åŠ¨åå°çº¿ç¨‹æ‰§è¡Œå·¥ä½œæµ
            graph_thread = threading.Thread(target=run_graph, daemon=True)
            graph_thread.start()
            
            # å®æ—¶ä»è¯¥è¯·æ±‚çš„é˜Ÿåˆ—ä¸­è¯»å–å¹¶ yield token
            consumer_disconnected = False
            while True:
                try:
                    # ä»è¯¥è¯·æ±‚çš„é˜Ÿåˆ—ä¸­è·å– tokenï¼ˆè¶…æ—¶0.1ç§’ï¼Œé¿å…é˜»å¡å¤ªä¹…ï¼‰
                    chunk = request_queue.get(timeout=0.1)
                    
                    # None è¡¨ç¤ºç»“æŸ
                    if chunk is None:
                        break
                    
                    # å®æ—¶ yield tokenï¼ˆæ•è·è¿æ¥æ–­å¼€å¼‚å¸¸ï¼‰
                    try:
                        yield chunk
                    except Exception as e:
                        # æ•è· yield å¼‚å¸¸ï¼ˆé€šå¸¸æ˜¯è¿æ¥æ–­å¼€ï¼‰
                        logger.warning(f"âš ï¸ [DEBUG] yield æ—¶è¿æ¥æ–­å¼€ (request_id={request_id}): {e}")
                        consumer_disconnected = True
                        _mark_queue_disconnected(request_id)
                        break
                    
                except queue.Empty:
                    # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å·²å®Œæˆ
                    if execution_done.is_set():
                        # æ¸…ç©ºé˜Ÿåˆ—ä¸­å‰©ä½™çš„å†…å®¹
                        while True:
                            try:
                                chunk = request_queue.get_nowait()
                                if chunk is None:
                                    break
                                try:
                                    yield chunk
                                except Exception as e:
                                    # æ•è· yield å¼‚å¸¸ï¼ˆé€šå¸¸æ˜¯è¿æ¥æ–­å¼€ï¼‰
                                    logger.warning(f"âš ï¸ [DEBUG] yield æ—¶è¿æ¥æ–­å¼€ (request_id={request_id}): {e}")
                                    consumer_disconnected = True
                                    _mark_queue_disconnected(request_id)
                                    break
                            except queue.Empty:
                                break
                        break
                    # ç»§ç»­ç­‰å¾…
                    continue
            
            # ç­‰å¾…å·¥ä½œæµçº¿ç¨‹å®Œæˆ
            # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤360ç§’
            timeout_seconds = analysis_timeout if analysis_timeout is not None else 360
            graph_thread.join(timeout=timeout_seconds)
            
            # å¦‚æœçº¿ç¨‹ä»åœ¨è¿è¡Œï¼Œè¯´æ˜è¶…æ—¶äº†
            if graph_thread.is_alive():
                logger.warning(f"âš ï¸ åˆ†æè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰ï¼Œå¼ºåˆ¶ç»“æŸ (request_id={request_id})")
                yield f"\n\nâš ï¸ **åˆ†æè¶…æ—¶**\n\nåˆ†æè¿‡ç¨‹è¶…è¿‡ {timeout_seconds} ç§’ï¼Œå·²è‡ªåŠ¨ç»ˆæ­¢ã€‚\n\n"
                # æ³¨æ„ï¼šdaemon çº¿ç¨‹ä¼šåœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if execution_error[0]:
                raise execution_error[0]
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            if final_state:
                return AnalysisResult(
                    success=final_state.get("phase") == AnalysisPhase.COMPLETED.value,
                    report=final_state.get("report", ""),
                    code_history=final_state.get("code_history", []),
                    execution_outputs=[
                        e.output for e in final_state.get("execution_history", [])
                    ] if final_state.get("execution_history") else [],
                    generated_files=final_state.get("generated_files", []),
                    error_message=final_state.get("error_message"),
                    total_rounds=final_state.get("round_count", 0),
                )
            else:
                return AnalysisResult(
                    success=False,
                    error_message="å·¥ä½œæµæ‰§è¡Œå¤±è´¥",
                )
        finally:
            # æ¸…ç†è¯¥è¯·æ±‚çš„é˜Ÿåˆ—ï¼ˆä¸å½±å“å…¶ä»–è¯·æ±‚ï¼‰
            _remove_request_queue(request_id)
            logger.info(f"ğŸ åˆ†æè¯·æ±‚å®Œæˆ: {request_id}")

