"""
LangGraph Analyzer API

æä¾›ä¸ç°æœ‰ excel_analyze_api.py å…¼å®¹çš„ API æ¥å£
"""

import os
import sys
import logging
from typing import Dict, Any, List, Optional, Generator

from .graph import DataAnalysisGraph
from .state import AnalysisResult

logger = logging.getLogger(__name__)


def get_data_preview(csv_path: str, max_rows: int = 5) -> str:
    """
    è·å– CSV æ–‡ä»¶çš„æ•°æ®é¢„è§ˆ
    
    Args:
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        max_rows: æœ€å¤§é¢„è§ˆè¡Œæ•°
        
    Returns:
        æ•°æ®é¢„è§ˆå­—ç¬¦ä¸²
    """
    try:
        import pandas as pd
        df = pd.read_csv(csv_path, nrows=max_rows)
        return df.to_string(index=False)
    except Exception as e:
        logger.warning(f"æ— æ³•è¯»å–æ•°æ®é¢„è§ˆ: {e}")
        return "ï¼ˆæ— æ³•è¯»å–æ•°æ®é¢„è§ˆï¼‰"


def analyze_excel_files_with_langgraph(
    files_data: List[Dict[str, Any]],  # æ¯ä¸ªå…ƒç´ åŒ…å« file_content, filename
    analysis_api_url: str,
    analysis_model: str,
    thread_id: Optional[str] = None,
    use_llm_validate: bool = False,
    sheet_name: Optional[str] = None,
    analysis_prompt: Optional[str] = None,
    temperature: float = 0.4,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    analysis_api_key: Optional[str] = None,
    preprocessing_timeout: Optional[int] = None,
    analysis_timeout: Optional[int] = None,
    debug_print_execution_output: bool = False,
    debug_print_header_analysis: bool = False,
    max_analysis_rounds: int = 3,
    max_file_size_mb: Optional[int] = None,
    excel_processing_timeout: Optional[int] = None,
) -> Generator[str, None, None]:
    """
    ä½¿ç”¨ LangGraph åˆ†æ Excel æ–‡ä»¶ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
    
    ç»Ÿä¸€å¤„ç†å•æ–‡ä»¶å’Œå¤šæ–‡ä»¶åœºæ™¯ï¼Œä¸å†åŒºåˆ†ã€‚
    
    æµç¨‹ï¼š
    1. é¢„å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼ˆè¡¨å¤´åˆ†æ+CSVè½¬æ¢ï¼‰
    2. æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„å…ƒæ•°æ®
    3. åœ¨ç­–ç•¥åˆ¶å®šæ—¶è®©LLMé€‰æ‹©æ–‡ä»¶ï¼ˆå•æ–‡ä»¶æ—¶è‡ªåŠ¨é€‰æ‹©ï¼‰
    4. æ‰§è¡Œåˆ†æ
    
    Args:
        files_data: æ–‡ä»¶æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            - file_content: æ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰
            - filename: æ–‡ä»¶å
        analysis_api_url: åˆ†æ API åœ°å€
        analysis_model: åˆ†ææ¨¡å‹åç§°
        thread_id: ä¼šè¯ID
        use_llm_validate: æ˜¯å¦ä½¿ç”¨ LLM éªŒè¯è¡¨å¤´
        sheet_name: å·¥ä½œè¡¨åç§°
        analysis_prompt: åˆ†ææç¤ºè¯
        temperature: ç”Ÿæˆæ¸©åº¦
        llm_api_key: LLM API å¯†é’¥
        llm_base_url: LLM API åœ°å€
        llm_model: LLM æ¨¡å‹åç§°
        analysis_api_key: åˆ†æ API å¯†é’¥
        preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        analysis_timeout: åˆ†æè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        debug_print_execution_output: æ˜¯å¦æ‰“å°ä»£ç æ‰§è¡Œè¾“å‡ºï¼ˆç”¨äºè°ƒè¯•ï¼‰
        debug_print_header_analysis: æ˜¯å¦æ‰“å°è¡¨å¤´åˆ†æç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        max_analysis_rounds: æœ€å¤§åˆ†æè½®æ•°ï¼ˆé»˜è®¤3è½®ï¼‰ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        excel_processing_timeout: Excelå¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œåœ¨LLMåˆ†æä¹‹å‰
        
    Yields:
        æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
    """
    from ..storage import storage
    from ..utils import get_thread_workspace
    from ..config import DEFAULT_EXCEL_ANALYSIS_PROMPT, EXCEL_LLM_API_KEY
    from ..excel_processor import process_excel_file
    from ..excel_analyze_api import validate_excel_file
    
    import time
    import uuid
    
    total_files = len(files_data)
    
    # åˆ›å»ºæˆ–è·å–ä¼šè¯
    if thread_id:
        current_thread_id = thread_id
    else:
        current_thread_id = f"thread-{uuid.uuid4().hex[:24]}"
    
    workspace_dir = get_thread_workspace(current_thread_id)
    os.makedirs(workspace_dir, exist_ok=True)
    
    try:
        yield f"ğŸš€ **å¼€å§‹å¤„ç† {total_files} ä¸ªExcelæ–‡ä»¶...**\n\n"
        
        # === ç¬¬ä¸€é˜¶æ®µï¼šé¢„å¤„ç†æ‰€æœ‰æ–‡ä»¶ ===
        yield "ğŸ“‹ **ç¬¬ä¸€é˜¶æ®µï¼šé¢„å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼ˆè¡¨å¤´åˆ†æå’ŒCSVè½¬æ¢ï¼‰**\n\n"
        
        processed_files_info = []
        
        for file_index, file_data in enumerate(files_data, 1):
            file_content = file_data.get("file_content")
            filename = file_data.get("filename", f"file_{file_index}.xlsx")
            
            yield f"ğŸ“„ **å¤„ç†æ–‡ä»¶ {file_index}/{total_files}: {filename}**\n"
            
            # æ–‡ä»¶éªŒè¯
            file_size = len(file_content)
            try:
                validate_excel_file(filename, file_size, max_file_size_mb=max_file_size_mb)
            except ValueError as e:
                yield f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}ï¼Œè·³è¿‡æ­¤æ–‡ä»¶\n\n"
                continue
            
            # ä¿å­˜æ–‡ä»¶
            excel_path = os.path.join(workspace_dir, filename)
            with open(excel_path, "wb") as f:
                f.write(file_content)
            
            yield f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {filename}\n"
            
            # å¤„ç†è¡¨å¤´
            api_key = llm_api_key if llm_api_key else EXCEL_LLM_API_KEY
            actual_use_llm = use_llm_validate and bool(api_key)
            
            yield "ğŸ” æ­£åœ¨åˆ†æè¡¨å¤´ç»“æ„...\n"
            
            # å¤„ç†Excelæ–‡ä»¶
            process_result = process_excel_file(
                filepath=excel_path,
                output_dir=workspace_dir,
                sheet_name=sheet_name,
                use_llm_validate=actual_use_llm,
                llm_api_key=llm_api_key,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
                preprocessing_timeout=preprocessing_timeout,
                excel_processing_timeout=excel_processing_timeout,
                debug_print_header_analysis=debug_print_header_analysis,
                thinking_callback=None,
                max_file_size_mb=max_file_size_mb
            )
            
            if not process_result.success:
                yield f"âŒ Excel å¤„ç†å¤±è´¥: {process_result.error_message}ï¼Œè·³è¿‡æ­¤æ–‡ä»¶\n\n"
                continue
            
            # è·å–æ•°æ®é¢„è§ˆ
            data_preview = ""
            if process_result.processed_file_path:
                data_preview = get_data_preview(process_result.processed_file_path, max_rows=5)
            
            # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
            processed_files_info.append({
                "filename": filename,
                "csv_path": process_result.processed_file_path,
                "row_count": process_result.row_count,
                "column_names": process_result.column_names,
                "column_metadata": process_result.column_metadata,
                "data_preview": data_preview,
            })
            
            yield f"âœ… æ–‡ä»¶ {file_index} é¢„å¤„ç†å®Œæˆï¼ˆæ•°æ®è¡Œæ•°: {process_result.row_count}ï¼‰\n\n"
        
        if not processed_files_info:
            yield "âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œåˆ†æ\n"
            return
        
        yield f"âœ… **æ‰€æœ‰æ–‡ä»¶é¢„å¤„ç†å®Œæˆ**ï¼ˆå…± {len(processed_files_info)} ä¸ªæ–‡ä»¶ï¼‰\n\n"
        
        # === ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ LangGraph æ‰§è¡Œåˆ†æ ===
        yield "ğŸ§  **ç¬¬äºŒé˜¶æ®µï¼šAI æ•°æ®åˆ†æ**\n\n"
        
        prompt = analysis_prompt or DEFAULT_EXCEL_ANALYSIS_PROMPT
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºä¸»æ–‡ä»¶ï¼ˆç”¨äºåˆå§‹åŒ–çŠ¶æ€ï¼‰
        first_file = processed_files_info[0]
        
        # è°ƒç”¨ç»Ÿä¸€çš„åˆ†æå‡½æ•°
        for chunk in run_langgraph_analysis_stream(
            workspace_dir=workspace_dir,
            thread_id=current_thread_id,
            csv_path=first_file["csv_path"],  # ä¸»æ–‡ä»¶è·¯å¾„
            column_names=first_file["column_names"],
            column_metadata=first_file["column_metadata"],
            row_count=first_file["row_count"],
            data_preview=first_file["data_preview"],
            user_prompt=prompt,
            api_url=analysis_api_url,
            model=analysis_model,
            api_key=analysis_api_key,
            temperature=temperature,
            analysis_timeout=analysis_timeout,
            debug_print_execution_output=debug_print_execution_output,
            max_analysis_rounds=max_analysis_rounds,
            available_files=processed_files_info,  # ä¼ é€’æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
        ):
            yield chunk
        
    except Exception as e:
        import traceback
        yield f"\nâŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}\n{traceback.format_exc()}\n"


def run_langgraph_analysis(
    workspace_dir: str,
    thread_id: str,
    csv_path: str,
    column_names: List[str],
    column_metadata: Dict[str, Any],
    row_count: int,
    user_prompt: str,
    api_url: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.4,
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LangGraph æ‰§è¡Œæ•°æ®åˆ†æï¼ˆéæµå¼ï¼‰
    
    Args:
        workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
        thread_id: ä¼šè¯ID
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        column_names: åˆ—ååˆ—è¡¨
        column_metadata: åˆ—å…ƒæ•°æ®
        row_count: æ•°æ®è¡Œæ•°
        user_prompt: ç”¨æˆ·åˆ†æéœ€æ±‚
        api_url: LLM API åœ°å€
        model: æ¨¡å‹åç§°
        api_key: LLM API å¯†é’¥
        temperature: ç”Ÿæˆæ¸©åº¦
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    # è·å–æ•°æ®é¢„è§ˆ
    data_preview = get_data_preview(csv_path)
    
    # åˆ›å»ºåˆ†æå›¾
    graph = DataAnalysisGraph()
    
    # æ‰§è¡Œåˆ†æ
    result = graph.analyze(
        workspace_dir=workspace_dir,
        thread_id=thread_id,
        csv_path=csv_path,
        column_names=column_names,
        column_metadata=column_metadata,
        row_count=row_count,
        data_preview=data_preview,
        user_prompt=user_prompt,
        api_url=api_url,
        model=model,
        api_key=api_key,
        temperature=temperature,
    )
    
    return {
        "success": result.success,
        "report": result.report,
        "reasoning": "\n\n".join([
            f"ä»£ç  {i+1}:\n{code}" 
            for i, code in enumerate(result.code_history)
        ]),
        "generated_files": result.generated_files,
        "error_message": result.error_message,
    }


def run_langgraph_analysis_stream_legacy(
    workspace_dir: str,
    thread_id: str,
    csv_path: str,
    column_names: List[str],
    column_metadata: Dict[str, Any],
    row_count: int,
    user_prompt: str,
    api_url: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.4,
    analysis_timeout: Optional[int] = None,
    debug_print_execution_output: bool = False,
    max_analysis_rounds: int = 3,
) -> Generator[str, None, None]:
    """
    ä½¿ç”¨ LangGraph æ‰§è¡Œæ•°æ®åˆ†æï¼ˆæµå¼ï¼Œå…¼å®¹æ—§æ¥å£ï¼‰
    
    æ­¤å‡½æ•°ç”¨äºå…¼å®¹æ—§çš„å•æ–‡ä»¶æ¥å£ï¼Œå†…éƒ¨è½¬æ¢ä¸ºç»Ÿä¸€çš„å¤šæ–‡ä»¶æ ¼å¼ã€‚
    """
    # è·å–æ•°æ®é¢„è§ˆ
    data_preview = get_data_preview(csv_path)
    
    # æ„å»ºå•æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
    available_files = [{
        "filename": os.path.basename(csv_path),
        "csv_path": csv_path,
        "row_count": row_count,
        "column_names": column_names,
        "column_metadata": column_metadata,
        "data_preview": data_preview,
    }]
    
    # è°ƒç”¨ç»Ÿä¸€çš„åˆ†æå‡½æ•°
    for chunk in run_langgraph_analysis_stream(
        workspace_dir=workspace_dir,
        thread_id=thread_id,
        csv_path=csv_path,
        column_names=column_names,
        column_metadata=column_metadata,
        row_count=row_count,
        data_preview=data_preview,
        user_prompt=user_prompt,
        api_url=api_url,
        model=model,
        api_key=api_key,
        temperature=temperature,
        analysis_timeout=analysis_timeout,
        debug_print_execution_output=debug_print_execution_output,
        max_analysis_rounds=max_analysis_rounds,
        available_files=available_files,
    ):
        yield chunk


def analyze_excel_with_langgraph(
    file_content: bytes,
    filename: str,
    analysis_api_url: str,
    analysis_model: str,
    thread_id: Optional[str] = None,
    use_llm_validate: bool = False,
    sheet_name: Optional[str] = None,
    analysis_prompt: Optional[str] = None,
    temperature: float = 0.4,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    analysis_api_key: Optional[str] = None,
    preprocessing_timeout: Optional[int] = None,
    analysis_timeout: Optional[int] = None,
    debug_print_execution_output: bool = False,
    debug_print_header_analysis: bool = False,
    max_analysis_rounds: int = 3,
    max_file_size_mb: Optional[int] = None,
    excel_processing_timeout: Optional[int] = None,
) -> Generator[str, None, None]:
    """
    ä½¿ç”¨ LangGraph åˆ†æ Excel æ–‡ä»¶ï¼ˆæµå¼ç‰ˆæœ¬ï¼Œå…¼å®¹æ—§æ¥å£ï¼‰
    
    æ­¤å‡½æ•°ç”¨äºå…¼å®¹æ—§çš„å•æ–‡ä»¶æ¥å£ï¼Œå†…éƒ¨è½¬æ¢ä¸ºç»Ÿä¸€çš„å¤šæ–‡ä»¶æ ¼å¼ã€‚
    
    Args:
        file_content: Excel æ–‡ä»¶å†…å®¹
        filename: æ–‡ä»¶å
        analysis_api_url: åˆ†æ API åœ°å€
        analysis_model: åˆ†ææ¨¡å‹åç§°
        thread_id: ä¼šè¯ID
        use_llm_validate: æ˜¯å¦ä½¿ç”¨ LLM éªŒè¯è¡¨å¤´
        sheet_name: å·¥ä½œè¡¨åç§°
        analysis_prompt: åˆ†ææç¤ºè¯
        temperature: ç”Ÿæˆæ¸©åº¦
        llm_api_key: LLM API å¯†é’¥
        llm_base_url: LLM API åœ°å€
        llm_model: LLM æ¨¡å‹åç§°
        analysis_api_key: åˆ†æ API å¯†é’¥
        preprocessing_timeout: é¢„å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        analysis_timeout: åˆ†æè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        debug_print_execution_output: æ˜¯å¦æ‰“å°ä»£ç æ‰§è¡Œè¾“å‡ºï¼ˆç”¨äºè°ƒè¯•ï¼‰
        debug_print_header_analysis: æ˜¯å¦æ‰“å°è¡¨å¤´åˆ†æç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        max_analysis_rounds: æœ€å¤§åˆ†æè½®æ•°ï¼ˆé»˜è®¤3è½®ï¼‰ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        
    Yields:
        æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
    """
    # è½¬æ¢ä¸ºç»Ÿä¸€çš„å¤šæ–‡ä»¶æ ¼å¼
    files_data = [{
        "file_content": file_content,
        "filename": filename,
    }]
    
    # è°ƒç”¨ç»Ÿä¸€çš„åˆ†æå‡½æ•°
    for chunk in analyze_excel_files_with_langgraph(
        files_data=files_data,
        analysis_api_url=analysis_api_url,
        analysis_model=analysis_model,
        thread_id=thread_id,
        use_llm_validate=use_llm_validate,
        sheet_name=sheet_name,
        analysis_prompt=analysis_prompt,
        temperature=temperature,
        llm_api_key=llm_api_key,
        llm_base_url=llm_base_url,
        llm_model=llm_model,
        analysis_api_key=analysis_api_key,
        preprocessing_timeout=preprocessing_timeout,
        analysis_timeout=analysis_timeout,
        debug_print_execution_output=debug_print_execution_output,
        debug_print_header_analysis=debug_print_header_analysis,
        max_analysis_rounds=max_analysis_rounds,
        max_file_size_mb=max_file_size_mb,
        excel_processing_timeout=excel_processing_timeout,
    ):
        yield chunk
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from ..storage import storage
    from ..utils import get_thread_workspace
    from ..config import DEFAULT_EXCEL_ANALYSIS_PROMPT, EXCEL_LLM_API_KEY
    
    import time
    import uuid
    
    file_size = len(file_content)
    
    # æ–‡ä»¶éªŒè¯
    from pathlib import Path
    from ..config import EXCEL_VALID_EXTENSIONS, EXCEL_MAX_FILE_SIZE_MB
    from ..excel_analyze_api import validate_excel_file
    
    try:
        validate_excel_file(filename, file_size, max_file_size_mb=max_file_size_mb)
    except ValueError as e:
        yield f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}\n"
        return
    
    # åˆ›å»ºæˆ–è·å–ä¼šè¯
    if thread_id:
        current_thread_id = thread_id
    else:
        current_thread_id = f"thread-{uuid.uuid4().hex[:24]}"
    
    workspace_dir = get_thread_workspace(current_thread_id)
    os.makedirs(workspace_dir, exist_ok=True)
    
    try:
        # ä¿å­˜æ–‡ä»¶
        excel_path = os.path.join(workspace_dir, filename)
        with open(excel_path, "wb") as f:
            f.write(file_content)
        
        yield f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {filename}\n\n"
        
        # æ‰“å°æœ€åˆä¼ å…¥çš„ExcelåŸå§‹æ•°æ®
        logger.info(f"ğŸ“Š [DEBUG] [LangGraph] å‡†å¤‡æ‰“å°ExcelåŸå§‹æ•°æ®: {excel_path}")
        print("ğŸ” [DEBUG] [LangGraph] è°ƒç”¨ print_excel_raw_data å‰ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
        import sys
        sys.stdout.flush()
        from ..excel_processor import print_excel_raw_data
        try:
            print_excel_raw_data(excel_path, sheet_name=sheet_name)
            print("ğŸ” [DEBUG] [LangGraph] print_excel_raw_data å‡½æ•°å·²è¿”å›ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
            sys.stdout.flush()
        except Exception as e:
            print(f"âŒ [DEBUG] [LangGraph] print_excel_raw_data è°ƒç”¨å¼‚å¸¸: {e}ï¼ˆä½¿ç”¨printè¾“å‡ºï¼‰")
            sys.stdout.flush()
            raise
        logger.info(f"âœ… [DEBUG] [LangGraph] print_excel_raw_data å‡½æ•°å·²è¿”å›")
        
        # å¤„ç†è¡¨å¤´
        api_key = llm_api_key if llm_api_key else EXCEL_LLM_API_KEY
        actual_use_llm = use_llm_validate and bool(api_key)
        
        yield "ğŸ” æ­£åœ¨åˆ†æè¡¨å¤´ç»“æ„...\n"
        
        import threading
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from ..excel_processor import process_excel_file
        
        # å¤„ç†Excelæ–‡ä»¶
        process_result = process_excel_file(
            filepath=excel_path,
            output_dir=workspace_dir,
            sheet_name=sheet_name,
            use_llm_validate=actual_use_llm,
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            preprocessing_timeout=preprocessing_timeout,
            excel_processing_timeout=excel_processing_timeout,
            debug_print_header_analysis=debug_print_header_analysis,
            thinking_callback=None,  # ä¸è¾“å‡º thinking å†…å®¹
            max_file_size_mb=max_file_size_mb  # ä¼ é€’æ–‡ä»¶å¤§å°é™åˆ¶
        )
        
        if not process_result.success:
            yield f"âŒ Excel å¤„ç†å¤±è´¥: {process_result.error_message}\n"
            return
        
        yield f"âœ… è¡¨å¤´åˆ†æå®Œæˆï¼Œæ•°æ®è¡Œæ•°: {process_result.row_count}\n\n"
        
        # æ ¹æ®è°ƒè¯•å¼€å…³å†³å®šæ˜¯å¦è¾“å‡ºLLMåˆ†æå“åº”
        if debug_print_header_analysis and process_result.llm_analysis_response:
            yield "\nğŸ“‹ **LLMè¡¨å¤´åˆ†æåŸå§‹å“åº”ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰ï¼š**\n\n"
            yield "```json\n"
            yield process_result.llm_analysis_response
            yield "\n```\n\n"
        
        # ä½¿ç”¨ LangGraph æ‰§è¡Œåˆ†æ
        prompt = analysis_prompt or DEFAULT_EXCEL_ANALYSIS_PROMPT
        
        yield "ğŸ§  **å¼€å§‹ AI æ•°æ®åˆ†æ**\n\n"
        
        # æ„å»ºå•æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        available_files = [{
            "filename": filename,
            "csv_path": process_result.processed_file_path,
            "row_count": process_result.row_count,
            "column_names": process_result.column_names,
            "column_metadata": process_result.column_metadata,
            "data_preview": get_data_preview(process_result.processed_file_path, max_rows=5),
        }]
        
        for chunk in run_langgraph_analysis_stream(
            workspace_dir=workspace_dir,
            thread_id=current_thread_id,
            csv_path=process_result.processed_file_path,
            column_names=process_result.column_names,
            column_metadata=process_result.column_metadata,
            row_count=process_result.row_count,
            data_preview=get_data_preview(process_result.processed_file_path),
            user_prompt=prompt,
            api_url=analysis_api_url,
            model=analysis_model,
            api_key=analysis_api_key,
            temperature=temperature,
            analysis_timeout=analysis_timeout,
            debug_print_execution_output=debug_print_execution_output,
            max_analysis_rounds=max_analysis_rounds,
            available_files=available_files,
        ):
            yield chunk
        
    except Exception as e:
        import traceback
        yield f"\nâŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}\n{traceback.format_exc()}\n"

