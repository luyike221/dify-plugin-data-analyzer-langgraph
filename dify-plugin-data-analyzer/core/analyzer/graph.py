"""
LangGraph Data Analysis Workflow

åŸºäº LangGraph 1.0.0+ å®ç°çš„æ•°æ®åˆ†æå·¥ä½œæµå›¾
æ”¯æŒï¼šä»£ç ç”Ÿæˆ â†’ æ‰§è¡Œ â†’ é”™è¯¯ä¿®å¤ â†’ æŠ¥å‘Šç”Ÿæˆ
"""

import re
import logging
from typing import Dict, Any, List, Optional, Generator, Literal

from langgraph.graph import StateGraph, START, END

from .state import AnalysisState, AnalysisPhase, CodeExecution, create_initial_state, AnalysisResult
from .prompts import PromptTemplates

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


# ============================================================================
# LLM å®¢æˆ·ç«¯è¾…åŠ©å‡½æ•°
# ============================================================================

def extract_api_base(api_url: str) -> str:
    """ä»å®Œæ•´çš„API URLä¸­æå–base URL"""
    if api_url.endswith("/chat/completions"):
        return api_url.rsplit("/chat/completions", 1)[0]
    elif "/v1" in api_url:
        return api_url.rsplit("/v1", 1)[0] + "/v1"
    else:
        return api_url


def create_llm_client(api_url: str, api_key: Optional[str] = None):
    """åˆ›å»º OpenAI å…¼å®¹çš„ LLM å®¢æˆ·ç«¯"""
    import openai
    
    api_base = extract_api_base(api_url)
    return openai.OpenAI(
        base_url=api_base,
        api_key=api_key or "dummy",
        timeout=120.0,
    )


def call_llm(
    client,
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.4,
) -> str:
    """è°ƒç”¨ LLM å¹¶è¿”å›å“åº”å†…å®¹"""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
    )
    return response.choices[0].message.content


def call_llm_stream(
    client,
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.4,
) -> Generator[str, None, str]:
    """æµå¼è°ƒç”¨ LLMï¼Œyield æ¯ä¸ª tokenï¼Œæœ€åè¿”å›å®Œæ•´å†…å®¹"""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        stream=True,
    )
    
    full_content = ""
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            delta = chunk.choices[0].delta.content
            full_content += delta
            yield delta
    
    return full_content


# ============================================================================
# ä»£ç æå–è¾…åŠ©å‡½æ•°
# ============================================================================

def extract_python_code(text: str) -> Optional[str]:
    """ä» LLM å“åº”ä¸­æå– Python ä»£ç å—"""
    # åŒ¹é… ```python ... ``` æ ¼å¼
    pattern = r"```python\s*(.*?)```"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # åŒ¹é… ``` ... ``` æ ¼å¼ï¼ˆæ— è¯­è¨€æ ‡è®°ï¼‰
    pattern2 = r"```\s*(.*?)```"
    match2 = re.search(pattern2, text, re.DOTALL)
    if match2:
        code = match2.group(1).strip()
        # ç®€å•åˆ¤æ–­æ˜¯å¦åƒ Python ä»£ç 
        if "import " in code or "print(" in code or "def " in code:
            return code
    
    return None


def has_python_code(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å« Python ä»£ç å—"""
    return extract_python_code(text) is not None


def is_execution_error(output: str) -> bool:
    """æ£€æŸ¥æ‰§è¡Œè¾“å‡ºæ˜¯å¦åŒ…å«é”™è¯¯"""
    error_indicators = [
        "[Error]",
        "[Timeout]",
        "Traceback (most recent call last)",
        "Error:",
        "Exception:",
        "SyntaxError:",
        "NameError:",
        "TypeError:",
        "ValueError:",
        "KeyError:",
        "IndexError:",
        "FileNotFoundError:",
        "ModuleNotFoundError:",
    ]
    return any(indicator in output for indicator in error_indicators)


# ============================================================================
# å·¥ä½œæµèŠ‚ç‚¹å‡½æ•°
# ============================================================================

def generate_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç ç”ŸæˆèŠ‚ç‚¹
    
    æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œæ•°æ®ä¿¡æ¯ï¼Œè°ƒç”¨ LLM ç”Ÿæˆ Python åˆ†æä»£ç 
    """
    logger.info("ğŸ“ [Node] ä»£ç ç”ŸæˆèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # æ„å»º prompt
    messages = PromptTemplates.format_code_generation_prompt(
        csv_path=state["csv_path"],
        row_count=state["row_count"],
        column_names=state["column_names"],
        column_metadata=state["column_metadata"],
        data_preview=state["data_preview"],
        user_prompt=state["user_prompt"],
    )
    
    # è°ƒç”¨ LLM
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
    )
    
    # æå–ä»£ç 
    code = extract_python_code(response)
    
    if code:
        logger.info(f"âœ… [Node] æˆåŠŸç”Ÿæˆä»£ç ï¼Œé•¿åº¦: {len(code)} å­—ç¬¦")
        return {
            "phase": AnalysisPhase.CODE_EXECUTION.value,
            "current_code": code,
            "code_history": [code],
            "messages": messages + [{"role": "assistant", "content": response}],
            "stream_output": [f"\nğŸ“ **ç”Ÿæˆçš„åˆ†æä»£ç ï¼š**\n\n```python\n{code}\n```\n\n"],
        }
    else:
        logger.warning("âš ï¸ [Node] æœªèƒ½ä» LLM å“åº”ä¸­æå–ä»£ç ")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "current_output": response,
            "messages": messages + [{"role": "assistant", "content": response}],
            "stream_output": [f"\nâš ï¸ æœªç”Ÿæˆä»£ç ï¼ŒLLM ç›´æ¥è¿”å›ï¼š\n\n{response}\n\n"],
        }


def execute_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç æ‰§è¡ŒèŠ‚ç‚¹
    
    åœ¨æœ¬åœ°å®‰å…¨ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆçš„ Python ä»£ç 
    """
    logger.info("â–¶ï¸ [Node] ä»£ç æ‰§è¡ŒèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # å¯¼å…¥æ‰§è¡Œå‡½æ•°
    from ..utils import execute_code_safe
    
    code = state["current_code"]
    workspace_dir = state["workspace_dir"]
    
    # æ·»åŠ  matplotlib ä¸­æ–‡æ”¯æŒ
    chinese_matplot_setup = '''
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import warnings

chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC']
available_fonts = [f.name for f in fm.fontManager.ttflist]
chinese_font = next((f for f in chinese_fonts if f in available_fonts), None)

if chinese_font:
    plt.rcParams['font.sans-serif'] = [chinese_font] + plt.rcParams['font.sans-serif']
else:
    warnings.filterwarnings('ignore', category=UserWarning, message='.*Glyph.*missing.*')
plt.rcParams['axes.unicode_minus'] = False
'''
    
    full_code = chinese_matplot_setup + "\n" + code
    
    # æ‰§è¡Œä»£ç 
    logger.info(f"â³ æ‰§è¡Œä»£ç ï¼Œå·¥ä½œç›®å½•: {workspace_dir}")
    output = execute_code_safe(full_code, workspace_dir)
    logger.info(f"ğŸ“Š ä»£ç æ‰§è¡Œå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
    
    # æ£€æŸ¥æ‰§è¡Œç»“æœ
    success = not is_execution_error(output)
    
    # åˆ›å»ºæ‰§è¡Œè®°å½•
    execution = CodeExecution(
        code=code,
        output=output,
        success=success,
        error_message=output if not success else None,
        attempt=state.get("retry_count", 0) + 1,
    )
    
    if success:
        logger.info("âœ… [Node] ä»£ç æ‰§è¡ŒæˆåŠŸ")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "current_output": output,
            "execution_success": True,
            "execution_history": [execution],
            "round_count": state.get("round_count", 0) + 1,
            "stream_output": [f"\nâœ… **æ‰§è¡Œç»“æœï¼š**\n\n```\n{output}\n```\n\n"],
        }
    else:
        logger.warning(f"âŒ [Node] ä»£ç æ‰§è¡Œå¤±è´¥: {output[:200]}...")
        return {
            "phase": AnalysisPhase.ERROR_FIXING.value,
            "current_output": output,
            "execution_success": False,
            "error_message": output,
            "execution_history": [execution],
            "stream_output": [f"\nâŒ **æ‰§è¡Œå‡ºé”™ï¼š**\n\n```\n{output}\n```\n\n"],
        }


def fix_code_node(state: AnalysisState) -> Dict[str, Any]:
    """
    ä»£ç ä¿®å¤èŠ‚ç‚¹
    
    å½“ä»£ç æ‰§è¡Œå¤±è´¥æ—¶ï¼Œè°ƒç”¨ LLM ä¿®å¤ä»£ç 
    """
    logger.info("ğŸ”§ [Node] ä»£ç ä¿®å¤èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    retry_count = state.get("retry_count", 0) + 1
    max_retries = 3
    
    if retry_count > max_retries:
        logger.error(f"âŒ [Node] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "retry_count": retry_count,
            "stream_output": [f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œè·³è¿‡ä»£ç æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š\n\n"],
        }
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # æ„å»ºä¿®å¤ prompt
    messages = PromptTemplates.format_code_fix_prompt(
        original_code=state["current_code"],
        error_message=state.get("error_message", "æœªçŸ¥é”™è¯¯"),
        csv_path=state["csv_path"],
        column_names=state["column_names"],
    )
    
    # è°ƒç”¨ LLM ä¿®å¤
    response = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
    )
    
    # æå–ä¿®å¤åçš„ä»£ç 
    fixed_code = extract_python_code(response)
    
    if fixed_code:
        logger.info(f"âœ… [Node] æˆåŠŸè·å–ä¿®å¤ä»£ç ï¼Œé‡è¯•æ¬¡æ•°: {retry_count}")
        return {
            "phase": AnalysisPhase.CODE_EXECUTION.value,
            "current_code": fixed_code,
            "code_history": [fixed_code],
            "retry_count": retry_count,
            "stream_output": [f"\nğŸ”§ **ä¿®å¤åçš„ä»£ç ï¼ˆå°è¯• {retry_count}/{max_retries}ï¼‰ï¼š**\n\n```python\n{fixed_code}\n```\n\n"],
        }
    else:
        logger.warning("âš ï¸ [Node] æœªèƒ½ä»ä¿®å¤å“åº”ä¸­æå–ä»£ç ")
        return {
            "phase": AnalysisPhase.REPORT_GENERATION.value,
            "retry_count": retry_count,
            "stream_output": [f"\nâš ï¸ æ— æ³•ä¿®å¤ä»£ç ï¼Œè·³è¿‡æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š\n\n"],
        }


def generate_report_node(state: AnalysisState) -> Dict[str, Any]:
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    
    æ ¹æ®ä»£ç æ‰§è¡Œç»“æœï¼Œè°ƒç”¨ LLM ç”Ÿæˆåˆ†ææŠ¥å‘Š
    """
    logger.info("ğŸ“„ [Node] æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    client = create_llm_client(state["api_url"], state.get("api_key"))
    
    # è·å–æœ€åæ‰§è¡Œçš„ä»£ç å’Œè¾“å‡º
    code = state.get("current_code", "")
    output = state.get("current_output", "")
    
    # å¦‚æœæ²¡æœ‰æ‰§è¡Œè¾“å‡ºï¼Œä½¿ç”¨ä»£ç å†å²ä¸­çš„æœ€åä¸€ä¸ª
    if not output and state.get("execution_history"):
        last_execution = state["execution_history"][-1]
        code = last_execution.code
        output = last_execution.output
    
    # æ„å»ºæŠ¥å‘Š prompt
    messages = PromptTemplates.format_report_generation_prompt(
        user_prompt=state["user_prompt"],
        code=code,
        execution_output=output,
    )
    
    # è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
    report = call_llm(
        client=client,
        messages=messages,
        model=state["model"],
        temperature=state["temperature"],
    )
    
    logger.info(f"âœ… [Node] æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼Œé•¿åº¦: {len(report)} å­—ç¬¦")
    
    return {
        "phase": AnalysisPhase.COMPLETED.value,
        "report": report,
        "stream_output": [f"\nğŸ“Š **æ•°æ®åˆ†ææŠ¥å‘Šï¼š**\n\n{report}\n"],
    }


# ============================================================================
# æ¡ä»¶è·¯ç”±å‡½æ•°
# ============================================================================

def route_after_execution(state: AnalysisState) -> Literal["fix_code", "generate_report"]:
    """
    æ‰§è¡Œåè·¯ç”±å†³ç­–
    
    æ ¹æ®æ‰§è¡Œç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - æ‰§è¡ŒæˆåŠŸ â†’ ç”ŸæˆæŠ¥å‘Š
    - æ‰§è¡Œå¤±è´¥ â†’ ä¿®å¤ä»£ç 
    """
    if state.get("execution_success", False):
        return "generate_report"
    else:
        return "fix_code"


def route_after_fix(state: AnalysisState) -> Literal["execute_code", "generate_report"]:
    """
    ä¿®å¤åè·¯ç”±å†³ç­–
    
    æ ¹æ®ä¿®å¤ç»“æœå†³å®šä¸‹ä¸€æ­¥ï¼š
    - æœ‰æ–°ä»£ç  â†’ é‡æ–°æ‰§è¡Œ
    - æ— æ³•ä¿®å¤ â†’ ç”ŸæˆæŠ¥å‘Š
    """
    if state.get("phase") == AnalysisPhase.CODE_EXECUTION.value:
        return "execute_code"
    else:
        return "generate_report"


# ============================================================================
# å·¥ä½œæµå›¾æ„å»º
# ============================================================================

def create_analysis_graph() -> StateGraph:
    """
    åˆ›å»ºæ•°æ®åˆ†æå·¥ä½œæµå›¾
    
    å·¥ä½œæµç»“æ„ï¼š
    
    START â†’ generate_code â†’ execute_code â”€â”¬â”€(æˆåŠŸ)â”€â†’ generate_report â†’ END
                                          â”‚
                                          â””â”€(å¤±è´¥)â”€â†’ fix_code â”€â”¬â”€(æœ‰ä¿®å¤)â”€â†’ execute_code
                                                               â”‚
                                                               â””â”€(æ— æ³•ä¿®å¤)â”€â†’ generate_report
    
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AnalysisState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("generate_code", generate_code_node)
    workflow.add_node("execute_code", execute_code_node)
    workflow.add_node("fix_code", fix_code_node)
    workflow.add_node("generate_report", generate_report_node)
    
    # æ·»åŠ è¾¹
    # START â†’ generate_code
    workflow.add_edge(START, "generate_code")
    
    # generate_code â†’ execute_code (å¦‚æœç”Ÿæˆäº†ä»£ç )
    workflow.add_conditional_edges(
        "generate_code",
        lambda s: "execute_code" if s.get("phase") == AnalysisPhase.CODE_EXECUTION.value else "generate_report",
        {
            "execute_code": "execute_code",
            "generate_report": "generate_report",
        }
    )
    
    # execute_code â†’ fix_code æˆ– generate_report
    workflow.add_conditional_edges(
        "execute_code",
        route_after_execution,
        {
            "fix_code": "fix_code",
            "generate_report": "generate_report",
        }
    )
    
    # fix_code â†’ execute_code æˆ– generate_report
    workflow.add_conditional_edges(
        "fix_code",
        route_after_fix,
        {
            "execute_code": "execute_code",
            "generate_report": "generate_report",
        }
    )
    
    # generate_report â†’ END
    workflow.add_edge("generate_report", END)
    
    return workflow


# ============================================================================
# é«˜çº§å°è£…ç±»
# ============================================================================

class DataAnalysisGraph:
    """
    æ•°æ®åˆ†æå›¾å°è£…ç±»
    
    æä¾›ç®€åŒ–çš„ API ç”¨äºæ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµå›¾"""
        self._workflow = create_analysis_graph()
        self._graph = self._workflow.compile()
    
    def analyze(
        self,
        workspace_dir: str,
        thread_id: str,
        csv_path: str,
        column_names: List[str],
        column_metadata: Dict[str, Any],
        row_count: int,
        data_preview: str,
        user_prompt: str,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        temperature: float = 0.4,
    ) -> AnalysisResult:
        """
        æ‰§è¡Œæ•°æ®åˆ†æï¼ˆéæµå¼ï¼‰
        
        Args:
            workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
            thread_id: ä¼šè¯ID
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            column_names: åˆ—ååˆ—è¡¨
            column_metadata: åˆ—å…ƒæ•°æ®
            row_count: æ•°æ®è¡Œæ•°
            data_preview: æ•°æ®é¢„è§ˆ
            user_prompt: ç”¨æˆ·åˆ†æéœ€æ±‚
            api_url: LLM API åœ°å€
            model: æ¨¡å‹åç§°
            api_key: LLM API å¯†é’¥
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            AnalysisResult åˆ†æç»“æœ
        """
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = create_initial_state(
            workspace_dir=workspace_dir,
            thread_id=thread_id,
            csv_path=csv_path,
            column_names=column_names,
            column_metadata=column_metadata,
            row_count=row_count,
            data_preview=data_preview,
            user_prompt=user_prompt,
            api_url=api_url,
            model=model,
            api_key=api_key,
            temperature=temperature,
        )
        
        # æ‰§è¡Œå·¥ä½œæµ
        final_state = self._graph.invoke(initial_state)
        
        # æ„å»ºç»“æœ
        return AnalysisResult(
            success=final_state.get("phase") == AnalysisPhase.COMPLETED.value,
            report=final_state.get("report", ""),
            code_history=final_state.get("code_history", []),
            execution_outputs=[
                e.output for e in final_state.get("execution_history", [])
            ],
            generated_files=final_state.get("generated_files", []),
            error_message=final_state.get("error_message"),
            total_rounds=final_state.get("round_count", 0),
        )
    
    def analyze_stream(
        self,
        workspace_dir: str,
        thread_id: str,
        csv_path: str,
        column_names: List[str],
        column_metadata: Dict[str, Any],
        row_count: int,
        data_preview: str,
        user_prompt: str,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        temperature: float = 0.4,
    ) -> Generator[str, None, AnalysisResult]:
        """
        æ‰§è¡Œæ•°æ®åˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰
        
        Yields:
            str: æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
            
        Returns:
            AnalysisResult åˆ†æç»“æœ
        """
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = create_initial_state(
            workspace_dir=workspace_dir,
            thread_id=thread_id,
            csv_path=csv_path,
            column_names=column_names,
            column_metadata=column_metadata,
            row_count=row_count,
            data_preview=data_preview,
            user_prompt=user_prompt,
            api_url=api_url,
            model=model,
            api_key=api_key,
            temperature=temperature,
        )
        
        # ä½¿ç”¨ stream æ¨¡å¼æ‰§è¡Œ
        final_state = None
        for state in self._graph.stream(initial_state):
            # state æ˜¯ {node_name: node_output} çš„å­—å…¸
            for node_name, node_output in state.items():
                # è¾“å‡ºæµå¼å†…å®¹
                if "stream_output" in node_output:
                    for chunk in node_output["stream_output"]:
                        yield chunk
                
                # æ›´æ–°æœ€ç»ˆçŠ¶æ€
                final_state = node_output
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        if final_state:
            return AnalysisResult(
                success=final_state.get("phase") == AnalysisPhase.COMPLETED.value,
                report=final_state.get("report", ""),
                code_history=final_state.get("code_history", []),
                execution_outputs=[
                    e.output for e in final_state.get("execution_history", [])
                ] if final_state.get("execution_history") else [],
                generated_files=final_state.get("generated_files", []),
                error_message=final_state.get("error_message"),
                total_rounds=final_state.get("round_count", 0),
            )
        else:
            return AnalysisResult(
                success=False,
                error_message="å·¥ä½œæµæ‰§è¡Œå¤±è´¥",
            )

