"""
LangGraph Analyzer API

æä¾›ä¸ç°æœ‰ excel_analyze_api.py å…¼å®¹çš„ API æ¥å£
"""

import os
import logging
from typing import Dict, Any, List, Optional, Generator

from .graph import DataAnalysisGraph
from .state import AnalysisResult

logger = logging.getLogger(__name__)


def get_data_preview(csv_path: str, max_rows: int = 5) -> str:
    """
    è·å– CSV æ–‡ä»¶çš„æ•°æ®é¢„è§ˆ
    
    Args:
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        max_rows: æœ€å¤§é¢„è§ˆè¡Œæ•°
        
    Returns:
        æ•°æ®é¢„è§ˆå­—ç¬¦ä¸²
    """
    try:
        import pandas as pd
        df = pd.read_csv(csv_path, nrows=max_rows)
        return df.to_string(index=False)
    except Exception as e:
        logger.warning(f"æ— æ³•è¯»å–æ•°æ®é¢„è§ˆ: {e}")
        return "ï¼ˆæ— æ³•è¯»å–æ•°æ®é¢„è§ˆï¼‰"


def run_langgraph_analysis(
    workspace_dir: str,
    thread_id: str,
    csv_path: str,
    column_names: List[str],
    column_metadata: Dict[str, Any],
    row_count: int,
    user_prompt: str,
    api_url: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.4,
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LangGraph æ‰§è¡Œæ•°æ®åˆ†æï¼ˆéæµå¼ï¼‰
    
    Args:
        workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
        thread_id: ä¼šè¯ID
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        column_names: åˆ—ååˆ—è¡¨
        column_metadata: åˆ—å…ƒæ•°æ®
        row_count: æ•°æ®è¡Œæ•°
        user_prompt: ç”¨æˆ·åˆ†æéœ€æ±‚
        api_url: LLM API åœ°å€
        model: æ¨¡å‹åç§°
        api_key: LLM API å¯†é’¥
        temperature: ç”Ÿæˆæ¸©åº¦
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    # è·å–æ•°æ®é¢„è§ˆ
    data_preview = get_data_preview(csv_path)
    
    # åˆ›å»ºåˆ†æå›¾
    graph = DataAnalysisGraph()
    
    # æ‰§è¡Œåˆ†æ
    result = graph.analyze(
        workspace_dir=workspace_dir,
        thread_id=thread_id,
        csv_path=csv_path,
        column_names=column_names,
        column_metadata=column_metadata,
        row_count=row_count,
        data_preview=data_preview,
        user_prompt=user_prompt,
        api_url=api_url,
        model=model,
        api_key=api_key,
        temperature=temperature,
    )
    
    return {
        "success": result.success,
        "report": result.report,
        "reasoning": "\n\n".join([
            f"ä»£ç  {i+1}:\n{code}" 
            for i, code in enumerate(result.code_history)
        ]),
        "generated_files": result.generated_files,
        "error_message": result.error_message,
    }


def run_langgraph_analysis_stream(
    workspace_dir: str,
    thread_id: str,
    csv_path: str,
    column_names: List[str],
    column_metadata: Dict[str, Any],
    row_count: int,
    user_prompt: str,
    api_url: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.4,
) -> Generator[str, None, None]:
    """
    ä½¿ç”¨ LangGraph æ‰§è¡Œæ•°æ®åˆ†æï¼ˆæµå¼ï¼‰
    
    Args:
        workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
        thread_id: ä¼šè¯ID
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        column_names: åˆ—ååˆ—è¡¨
        column_metadata: åˆ—å…ƒæ•°æ®
        row_count: æ•°æ®è¡Œæ•°
        user_prompt: ç”¨æˆ·åˆ†æéœ€æ±‚
        api_url: LLM API åœ°å€
        model: æ¨¡å‹åç§°
        api_key: LLM API å¯†é’¥
        temperature: ç”Ÿæˆæ¸©åº¦
        
    Yields:
        æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
    """
    # è·å–æ•°æ®é¢„è§ˆ
    data_preview = get_data_preview(csv_path)
    
    # åˆ›å»ºåˆ†æå›¾
    graph = DataAnalysisGraph()
    
    # æµå¼æ‰§è¡Œåˆ†æ
    yield "ğŸš€ **å¼€å§‹ LangGraph æ•°æ®åˆ†æå·¥ä½œæµ**\n\n"
    
    try:
        for chunk in graph.analyze_stream(
            workspace_dir=workspace_dir,
            thread_id=thread_id,
            csv_path=csv_path,
            column_names=column_names,
            column_metadata=column_metadata,
            row_count=row_count,
            data_preview=data_preview,
            user_prompt=user_prompt,
            api_url=api_url,
            model=model,
            api_key=api_key,
            temperature=temperature,
        ):
            yield chunk
        
        yield "\n\nâœ… **åˆ†æå®Œæˆ**\n"
        
    except Exception as e:
        import traceback
        error_msg = f"\n\nâŒ **åˆ†æè¿‡ç¨‹å‡ºé”™**\n\n```\n{str(e)}\n{traceback.format_exc()}\n```\n"
        yield error_msg


def analyze_excel_with_langgraph(
    file_content: bytes,
    filename: str,
    analysis_api_url: str,
    analysis_model: str,
    thread_id: Optional[str] = None,
    use_llm_validate: bool = False,
    sheet_name: Optional[str] = None,
    analysis_prompt: Optional[str] = None,
    temperature: float = 0.4,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    analysis_api_key: Optional[str] = None,
) -> Generator[str, None, None]:
    """
    ä½¿ç”¨ LangGraph åˆ†æ Excel æ–‡ä»¶ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
    
    è¿™æ˜¯ä¸ analyze_excel_stream å…¼å®¹çš„æ¥å£ï¼Œ
    å¯ä»¥ç›´æ¥æ›¿æ¢ç°æœ‰çš„åˆ†æå‡½æ•°
    
    Args:
        file_content: Excel æ–‡ä»¶å†…å®¹
        filename: æ–‡ä»¶å
        analysis_api_url: åˆ†æ API åœ°å€
        analysis_model: åˆ†ææ¨¡å‹åç§°
        thread_id: ä¼šè¯ID
        use_llm_validate: æ˜¯å¦ä½¿ç”¨ LLM éªŒè¯è¡¨å¤´
        sheet_name: å·¥ä½œè¡¨åç§°
        analysis_prompt: åˆ†ææç¤ºè¯
        temperature: ç”Ÿæˆæ¸©åº¦
        llm_api_key: LLM API å¯†é’¥
        llm_base_url: LLM API åœ°å€
        llm_model: LLM æ¨¡å‹åç§°
        analysis_api_key: åˆ†æ API å¯†é’¥
        
    Yields:
        æµå¼è¾“å‡ºçš„å­—ç¬¦ä¸²å—
    """
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from ..excel_processor import process_excel_file, get_sheet_names
    from ..storage import storage
    from ..utils import get_thread_workspace
    from ..config import DEFAULT_EXCEL_ANALYSIS_PROMPT, EXCEL_LLM_API_KEY
    
    import time
    import uuid
    
    file_size = len(file_content)
    
    # æ–‡ä»¶éªŒè¯
    from pathlib import Path
    from ..config import EXCEL_VALID_EXTENSIONS, EXCEL_MAX_FILE_SIZE_MB
    
    ext = Path(filename).suffix.lower()
    if ext not in EXCEL_VALID_EXTENSIONS:
        yield f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}\n"
        return
    
    max_size_bytes = EXCEL_MAX_FILE_SIZE_MB * 1024 * 1024
    if file_size > max_size_bytes:
        yield f"âŒ æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.2f}MB\n"
        return
    
    # åˆ›å»ºæˆ–è·å–ä¼šè¯
    if thread_id:
        current_thread_id = thread_id
    else:
        current_thread_id = f"thread-{uuid.uuid4().hex[:24]}"
    
    workspace_dir = get_thread_workspace(current_thread_id)
    os.makedirs(workspace_dir, exist_ok=True)
    
    try:
        # ä¿å­˜æ–‡ä»¶
        excel_path = os.path.join(workspace_dir, filename)
        with open(excel_path, "wb") as f:
            f.write(file_content)
        
        yield f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {filename}\n\n"
        
        # è·å–å·¥ä½œè¡¨
        available_sheets = get_sheet_names(excel_path)
        if available_sheets:
            yield f"ğŸ“‹ å¯ç”¨å·¥ä½œè¡¨: {', '.join(available_sheets)}\n"
        
        # å¤„ç†è¡¨å¤´
        api_key = llm_api_key if llm_api_key else EXCEL_LLM_API_KEY
        actual_use_llm = use_llm_validate and bool(api_key)
        
        yield "ğŸ” æ­£åœ¨åˆ†æè¡¨å¤´ç»“æ„...\n"
        
        process_result = process_excel_file(
            filepath=excel_path,
            output_dir=workspace_dir,
            sheet_name=sheet_name,
            use_llm_validate=actual_use_llm,
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
        )
        
        if not process_result.success:
            yield f"âŒ Excel å¤„ç†å¤±è´¥: {process_result.error_message}\n"
            return
        
        yield f"âœ… è¡¨å¤´åˆ†æå®Œæˆï¼Œæ•°æ®è¡Œæ•°: {process_result.row_count}\n\n"
        
        # ä½¿ç”¨ LangGraph æ‰§è¡Œåˆ†æ
        prompt = analysis_prompt or DEFAULT_EXCEL_ANALYSIS_PROMPT
        
        yield "ğŸ§  **å¼€å§‹ AI æ•°æ®åˆ†æ**\n\n"
        
        for chunk in run_langgraph_analysis_stream(
            workspace_dir=workspace_dir,
            thread_id=current_thread_id,
            csv_path=process_result.processed_file_path,
            column_names=process_result.column_names,
            column_metadata=process_result.column_metadata,
            row_count=process_result.row_count,
            user_prompt=prompt,
            api_url=analysis_api_url,
            model=analysis_model,
            api_key=analysis_api_key,
            temperature=temperature,
        ):
            yield chunk
        
    except Exception as e:
        import traceback
        yield f"\nâŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}\n{traceback.format_exc()}\n"

